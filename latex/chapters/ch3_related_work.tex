% !TeX root = ../main.tex
\chapter{Related Work}
\label{ch:related-work}

This chapter places the thesis within existing knowledge regarding distributed systems and architectures. It evaluates established theoretical frameworks and empirical studies to provide a foundation for assessing the performance, structural complexity, and auditing capabilities of different architectural patterns. To align with the research sub-questions, the chapter is organized into three sections: performance and scalability (\acrshort{rq} 1), maintainability and flexibility (\acrshort{rq} 2), and historical traceability (\acrshort{rq} 3).

\section{RQ 1: Performance and Scalability}
\label{sec:rw-rs1}

Scalability and performance are critical considerations in modern distributed system design. \textcite{kleppmann_designing_2017} provides a foundational understanding of these concepts, defining scalability not simply as the ability to handle increased load, but as the capacity to maintain performance as load parameters grow~\cite[Chapter 1]{kleppmann_designing_2017}. To quantify scalability characteristics, \textcite{jogalekar_evaluating_2000} established a formal mathematical framework for scalability, defining a metric based on cost-effectiveness where productivity is a function of throughput and quality of service (specifically response time). Their work emphasizes that good scalability results from the system architecture and the scaling strategy.

Building on these theoretical foundations, recent studies have empirically evaluated the impact of architectural patterns like \gls{cqrs} on system performance. \textcite{jayaraman_implementing_2024} investigated the implementation of \gls{cqrs} in large-scale systems, using performance benchmarks to demonstrate that separating read and write models allows for independent optimization, reducing response times and improving throughput compared to monolithic architectures. Their research, based on simulations in a controlled environment, indicated that \gls{cqrs} systems, particularly when using read replicas and caching, maintained superior response times under high-concurrency workloads where traditional monolithic systems faced bottlenecks.

Further supporting these findings, \textcite{hruzin_migration_2024} presented a comparative analysis of a task-tracking system migrated from a traditional \gls{ddd} architecture to one using \gls{cqrs} and \acrlong{es}. The study found that while the transition increased code and infrastructure complexity, it yielded performance gains, increasing bulk read operation speeds by a factor of 6. Write operation performance varied after making the transition to \gls{cqrs}, with some operations demonstrating acceleration and others experiencing a slowdown.

Generally, there appears to be a consensus in the reviewed literature that decoupling Command processing from Query processing enables more granular scaling strategies, allowing systems to handle load more efficiently than traditional CRUD-based approaches~\cite{hruzin_migration_2024,jayaraman_implementing_2024, monagari_demystifying_2026}.

\section{RQ 2: Architectural Complexity, Maintainability, Flexibility}

Recent literature provides empirical evidence regarding the structural impact of adopting \gls{cqrs} and \acrlong{es}. The work conducted by \textcite{hruzin_migration_2024}, described in \autoref{sec:rw-rs1}, also showed specific results regarding code complexity: the migration increased the total number of classes from 47 to 213, while the overall cyclomatic complexity of the system decreased from $534$ to $522$. This suggests that while additional infrastructure classes are required, the individual modules become simpler. The authors argue that the separation of Commands and Queries simplifies debugging and extension of the application.

To objectively measure such qualities, researchers rely on established object-oriented metrics, some of which were described in \autoref{sec:static-analysis-metrics}. \textcite{singh_object_2018} employed statistical approaches, including hypothesis testing and linear regression, to correlate the \gls{ck} suite with software quality. Their analysis established that metrics such as \gls{dit} and \gls{noc} impact quality, but \gls{cbo} demonstrated the strongest negative correlation with software quality. \textcite{basili_validation_1996} present a validation of the \gls{ck} metrics, conducting experiments to test how these metrics correlate with defects in a program. Their results show that \gls{dit}, \gls{rfc} and \gls{noc} were most significant for predicting defects in a program, while \gls{lcom} was shown to be insignificant.

However, different critiques on these static analysis methods can be found in literature. \textcite[44]{richards_fundamentals_2020} offer a strong critique of the \gls{lcom} metric. They argue that the metric possesses "serious deficiencies" because it only measures the structural lack of cohesion (how methods and fields physically interact). The metric cannot determine if the pieces of code logically fit together. They also criticize complexity metrics like cyclomatic complexity, calling them "blunt". Their primary critique is that complexity metrics can not distinguish between essential complexity (complexity necessary for the domain) and accidental complexity introduced through poor coding practices~\cite[48]{richards_fundamentals_2020}. Finally, the authors express their critique of coupling metrics, mentioning that they focus too much on the low-level details of software coupling. The authors argue that developers should care more about \emph{how} components are coupled (e.g. synchronous or asynchronous)~\cite[53]{richards_fundamentals_2020}.

\textcite{drotbohm_instability-abstractness-relationship_2024}, a Spring engineer, offers his critique on the \emph{abstractness} metric, highlighting that it mistakes the existence of interfaces with actual, useful abstractions.

Kleppmann highlights the evolutionary advantages of \acrlong{es}. He argues that systems gain the flexibility to derive new read-optimized views from the same history without the need for schema migrations~\cite[461,462]{kleppmann_designing_2017}.

While Kleppmann emphasizes the architectural freedom to evolve views on existing data, \textcite{overeem_empirical_2021} discuss 19 event-sourced systems and point out that many engineers struggle with \emph{\gls{schema-evolution}} in \acrlong{es}, which is the process of changing the schema of events. The challenge is that the immutable history of the event log must remain compatible with evolving business logic, which turns the flexibility of the read-side into difficulties on the write-side. Schema evolution is not a goal of this thesis. While worth mentioning, this aspect will not be taken into consideration when providing a final comparison of the two architectures.

In terms of maintainability, while \textcite{jayaraman_implementing_2024} noted increased performance and scalability when using \gls{cqrs} with \acrlong{es}, they also found higher maintenance complexity and higher operational costs.

\section{RQ 3: Historical Traceability}
\label{sec:traceability-related-work}

\textcite[457,531]{kleppmann_designing_2017} mentions that using \gls{es} makes it easier to reproduce bugs and diagnose unexpected behaviors by replaying the event log. \textcite{monagari_demystifying_2026} cites empirical data showing that financial institutions using \acrlong{es} reduced incident resolution time from 4.2 hours to 23 minutes by replaying events to reproduce exact system states, though the primary data used in this study could not be verified during literature review.

\textcite{gantz_basics_2014} defines IT auditing as the process of validating controls to protect assets and information. Maier mentions that historically, audit logs were often viewed as "disposable" data, overwritten regularly to save space~\cite[3]{maier_audit_2006}. However, he argues that regulatory pressure and the need for accountability have made log retention critical for event reconstruction and forensic analysis~\cite[4,17]{maier_audit_2006}.

Effective auditing requires "evidence": information that auditors can verify against established criteria~\cite[155]{gantz_basics_2014}. Gantz emphasizes that the reliability of a system depends on its ability to produce accurate evidence of past states and operations~\cite[4]{gantz_basics_2014}.

The standard audit logging approach employed in traditional \acrshort{crud} systems has several downsides which can be found in the literature. \textcite[531]{kleppmann_designing_2017} notes that even if transaction logs are captured, they reveal what changed, but not necessarily \emph{why}. The application logic that decided on the mutation is transient and context may be lost. \textcite[23,24]{maier_audit_2006} points out that reconstructing a security incident in a distributed environment is time-consuming. It may require manually correlating logs from separate systems with different formats and time synchronizations.

In contrast, \acrlong{es} treats state changes as an immutable sequence of events. \textcite{helland_immutability_2015} emphasizes that the event log \emph{is the truth}, and any application state is derived from it. Because the log is append-only, history is never overwritten. This allows for deterministic replay of events, making any historic state perfectly reconstructible. \textcite[531]{kleppmann_designing_2017} also highlights that events typically capture user intent.

In terms of compliance and integrity, \textcite[531]{kleppmann_designing_2017} points out that the integrity of an event store can be verified through hashes, making event-sourced systems highly reliable.

While the reviewed literature establishes the superiority of event-sourced systems in terms of data integrity, no empirical research comparing the actual computational efficiency of state reconstruction between \acrlong{es} and traditional \acrshort{crud} audit logs could be found. Consequently, this thesis attempts to quantify the performance differences between these architectures when reconstructing historical states.
