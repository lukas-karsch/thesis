% !TeX root = ../main.tex
\chapter{Methodology}
\label{ch:methodology}

The goal of this thesis is to provide a comparison of \acrshort{crud} and \acrshort{cqrs} / \acrshort{es} architectures. At the core of this comparison, two prototypes of an identical application will be implemented and examined. These implementations will display the same interface and behavior.

To answer the primary research question based on the three defined sub-research questions, this thesis employs three phases. First, functional and non-functional requirements for the project and the applications will be described. Next, the applications can be implemented according to these requirements. The implementations can then be compared based on several criteria.

This chapter describes the method, detailing each phase and finally presenting the comparison methods used to assess performance, scalability, flexibility and traceability.

\section{Phase 1: Requirement Analysis}

Before implementing, it is necessary to conduct a requirement analysis. This requires defining functional requirements, regarding the domain and business logic of the application. Afterward, non-functional requirements are specified. These include performance goals (\acrshortpl{slo}), auditability and observability of the applications.

The defined requirements then serve as a contract which both implementations adhere to, ensuring a fair comparison.

\section{Phase 2: Implementation of prototypes}

Once requirements for the application are defined, the prototypes can be implemented. As mentioned above, these prototypes should have an identical interface and exhibit identical state transitions and error behaviors.

\section{Phase 3: Evaluation and Comparison}

After implementing the prototypes, they can be compared and evaluated based on the defined research questions. This comparison can be separated into three aspects: performance and scalability, architectural flexibility and traceability.

\subsection{Performance and Scalability}

To answer \hyperref[rs-performance-scalability]{\acrshort{rq} 1}, the performance characteristics of both architectural patterns will be evaluated and compared. To achieve this, load tests are executed on selected endpoints. The theoretical foundations of load testing, environmental constraints, and the data collection methodology are described in this subsection. Finally, perspectives on scalability are presented, which can be derived by analyzing results of load tests.

\subsubsection{Theoretical Load Testing Foundations}
\label{sec:load-test-theory}

To provide accurate performance metrics, load testing is performed on both implementations based on the methodology defined by \textcite[10-17]{kleppmann_designing_2017}. Load is generally characterized using "load parameters," which vary depending on the system's nature. For this thesis, the primary load parameter is the number of concurrent requests to the web server~\cite[11]{kleppmann_designing_2017}

The tests measure \textbf{\acrfull{rps}} and client-side response times. \textcite[15,16]{kleppmann_designing_2017} emphasizes the importance of client-side measurement to account for "queueing delays." As server-side processing is limited by hardware resources (e.g., CPU cores), requests may be stalled before processing begins. Server-side metrics typically measure latencies by executing request filters which only run once the request starts being processed. Therefore, server-side latency metrics often exclude the "waiting time" a request spent in the queue, leading to an overly optimistic view of performance. Consequently, the client-side latencies are more relevant to measure user-perceived performance.

\textcite[16]{kleppmann_designing_2017} also mentions that the load-generating client must continuously send requests without waiting for previous ones to complete, to simulate realistic concurrent user behavior. In load-testing frameworks, this is often called an "open model". If a client waits for its request to complete before sending the next one, queues on the server are kept artificially short. This is typically called "closed model".

This thesis adopts the approach of keeping system resources constant while measuring performance fluctuations under varying load intensities, as opposed increasing system resources to keep unchanged performance. These two perspectives are described by \textcite[13]{kleppmann_designing_2017}.

\textcite[264]{abbott_art_2009} mention another way of looking at load testing: In \emph{positive testing}, load is increased until an application is overwhelmed, while \emph{negative testing} takes away system resources such as memory, threads or connections, until the application degrades.

To evaluate results, the arithmetic mean is avoided as it obscures the experience of typical users and the impact of outliers. Instead, this thesis uses \glspl{percentile}. The \gls{median} (P50) serves as the metric for "typical" response time. Exactly 50\% of requests experience the median latency or less, while the other half experiences a higher latency. However, to capture the experience of users facing high delays, it is critical to measure "\glspl{tail-latency}" via a high percentile like P95 or P99. High percentiles identify the performance of outliers which, despite being a numerical minority, often represent the most valuable or complex operations. \cite[14-16]{kleppmann_designing_2017}

\subsubsection{Test Execution}
\label{sec:test-execution-method}

To obtain significant results, each \emph{test configuration} (the combination of a test script and target \acrshort{rps}) should be executed a sufficient number of times~\cite[259]{abbott_art_2009}. Between each execution, the application and its dependencies should be restarted to ensure independent results.

\subsubsection{Comparability and Environment}

\hyperref[sec:test-execution-method]{Section \ref*{sec:test-execution-method}} describes that \glspl{test-configuration} must be repeated an appropriate number of times to achieve accurate results. Additionally, the environment in which the tests are run must be controlled. It should be reproducible, identical for each test run and experience no disturbances like system updates while tests are being executed~\cite[Chapter 17]{abbott_art_2009}.

\subsubsection{Collected Metrics}
\label{sec:collected-metrics}

All metrics collected during load testing are listed in \autoref{table:collected-metrics}. The column "Metric" shows the name that will be used to refer to this metric from now on. "Location" describes where the metric is being recorded.

As described in \autoref{sec:load-test-theory}, not just the average latency is measured, but \glspl{percentile} are used to accurately report the number of users experiencing the respective latency.

While internal metrics, like CPU usage, RAM usage, the number of database connections ($hikari\_connections$) and the number of Tomcat worker threads are not measures for user-perceived performance, they can give an indicator about bottlenecks inside the applications.

Even though latencies are measured on both the client and the server, the client latency will be used primarily when visualizing results, as it is the latency users perceive when interacting with the applications. The server latency is recorded because it can help identify queueing delays by exposing differences in server and client latencies which exceed a factor of 1.1x.

$axon\_event\_size$ and $axon\_snapshot\_size$ are metrics extracted from the event store (Axon Server). They are only recorded for the ES-CQRS application. $postgres\_size$, the size of the relational database can be recorded for both applications. The CRUD implementation stores its entities and audit log in PostgreSQL, while the ES-CQRS implementation uses PostgreSQL as a secondary data store for denormalized projections and lookup tables. These metrics are relevant when assessing a system's long-term performance, scalability and maintainability.

\begin{table}[htp!]
    \small
    \centering
    \begin{tabularx}{\linewidth}{lXl}
        \toprule
        \textbf{Metric}        & \textbf{Description}                         & \textbf{Location} \\ \midrule
        $latency\_avg$         & Average (arithmetic mean) latency            & Server, Client    \\
        \addlinespace
        $latency\_p50$         & 50th \gls{percentile} Latency (\gls{median}) & Server, Client    \\
        \addlinespace
        $latency\_p95$         & 95th \gls{percentile} Latency                & Server, Client    \\
        \addlinespace
        $latency\_p99$         & 99th \gls{percentile} Latency                & Server, Client    \\
        \addlinespace
        $cpu\_usage$           & CPU usage of the Server process              & Server            \\
        \addlinespace
        $ram\_usage\_heap$     & Usage of Heap memory                         & Server            \\
        \addlinespace
        $ram\_usage\_total$    & Usage of total memory                        & Server            \\
        \addlinespace
        $hikari\_connections$  & Number of Data Source connections            & Server            \\
        \addlinespace
        $tomcat\_threads$      & Number of Tomcat worker threads              & Server            \\
        \addlinespace
        $postgres\_size$       & Size of PostgreSQL database                  & Server            \\
        \addlinespace
        $axon\_event\_size$    & Size of Axon's Event store                   & Axon Server       \\
        \addlinespace
        $axon\_snapshot\_size$ & Size of Axon's Snapshot store                & Axon Server       \\
        \addlinespace
        $axon\_storage\_size$  & $axon\_event\_size + axon\_storage\_size$    & Axon Server       \\
        \bottomrule
    \end{tabularx}
    \caption{All metrics collected during load testing}
    \label{table:collected-metrics}
\end{table}

\subsubsection{Visualizing Results}

Data is visualized using box plots and line graphs. Box plots are used to show the distribution of latencies, while line graphs illustrate performance changes as \gls{rps} increases. Per scientific standards, error bars are used to represent the variability of the measurements across the test runs.

\subsubsection{Scalability}

As outline above, load tests are used to collect latencies and system metrics. From these results, \autoref{sec:scalability-dimensions} serves as a basis to define a scalability function specific to this thesis. Results from the scalability function can then be used to assess the scalability of each application.

\subsection{Flexibility}

\autoref{sec:static-analysis-metrics} described a set of static analysis metrics. These serve as a basis to answer \hyperref[rs-architecture]{\acrshort{rq} 2}, using the results to compare the applications' architectures and their potential for flexibility and evolution over time.

\subsection{Traceability}
\label{sec:traceability-method}

This section describes how traceability will be compared; it serves as a basis to answer \hyperref[rs-traceability]{\acrshort{rq} 3}. The traceability aspect will be compared via two perspectives. First, the \emph{accuracy} of historic reconstruction is compared. Then, \emph{efficiency} of reconstructions is evaluated and compared.

\subsubsection{Accuracy of reconstruction}

Several qualitative criteria for the accuracy of an historic reconstruction can be defined. \emph{Source of truth integrity} defines how the system guarantees that the historic records match the current state. When a system needs to write state changes to two separate data stores, this creates a problem called "dual-write" problem in distributed systems.

\emph{Intent preservation} assesses the ability of a system to distinguish between different business reasons for the same data change.

\emph{Schema resilience} examines how historical data survives the evolution of the application's data structure. As business rules change, entities and database schemas evolve. The history must remain readable without being corrupted by modifications to schemas. TODO: remove this?? -> then also in discussion

\subsubsection{Efficiency}

The primary objective is to measure how each architecture handles "time-travel" queries. These are requests that require the system to reconstruct a specific state from the past. To ensure a fair comparison, both applications will be tested under identical load testing scenarios.

Two distinct scenarios of varying complexity will be evaluated: A "simple state reconstruction" scenario will query the historic state of one entity in the system. A more complex state reconstruction scenario will require a broader view of the system's state to be reconstructed. This requires the system to cross-reference and reconstruct multiple entity relationships as they existed at that moment.

The project's existing load-testing infrastructure will be used for these tests. This allows to capture the same data points, such as latencies and resource consumption, which can then serve as a basis to compare the efficiency of historic reconstruction in both applications.

\section{Limitations of the method}
\label{sec:limitations}

While the research design aims for a rigorous comparison, several limitations must be acknowledged. These constraints arise from the controlled environment and the scope of the prototype implementations.

The evaluation is conducted on a single machine running virtual machines, which consequently live on the same network. Therefore, the prototypes are not tested on distributed clusters, which would be necessary to observe the real effects of horizontal scaling. This setup also eliminates common distributed system challenges, such as network partitions, varying latency, and partial failures. By running all components in a controlled environment, the results may not fully reflect the resilience or overhead of the architectures when deployed across a distributed system. These challenges, which are bypassed by this study's setup, are known as the eight fallacies of distributed systems~\cite{rotem-gal-oz_fallacies_2008}, a term first coined by P. Deutsch in 1994.

In the \acrshort{crud} implementation, the audit log resides within the same relational database as the operational data. In a production-grade system, these might be separated to prevent the audit log's growth from impacting query performance, with the audit log being an additional data store, creating a "dual-write" problem~\cite[Chapter 11]{kleppmann_designing_2017}. Furthermore, because all components share the same underlying hardware resources during testing, the performance of one architectural layer (e.g., the event store) may influence another in ways that would not occur if they were isolated on dedicated hardware.

The load testing scenarios are artificial and focused on specific endpoints rather than complete user journeys. \textcite[267]{abbott_art_2009} suggest that the most accurate load profiles are derived from real-world application or load-balancer logs. Without such data, the tests rely on the estimation of relevant scenarios. Additionally, the prototypes lack complex access control patterns and multi-tenancy logic, which in a real-world scenario could introduce additional, non-negligible overhead.

The comparison focuses on the initial implementation and short-term performance, which may obscure long-term operational complexities. While data volume is simulated, the actual challenges of managing a growing event store, specifically the evolution of the schema over time, are evaluated theoretically rather than through years of operational data. As \textcite{young_versioning_2017} notes, the permanence of events in an immutable log necessitates complex strategies like upcasting or transformation to handle changing business requirements. Without observing these "versioning" cycles over a multi-year lifecycle, an incorrect view of the maintainability of event-sourced architectures may be presented.
