% !TeX root = ../main.tex
\chapter{Methodology}
\label{ch:methodology}

The goal of this thesis is to provide a comparison of \acrshort{crud} and \acrshort{cqrs} / \acrshort{es} architectures. At the core of this comparison, two prototypes of an identical application will be implemented and examined. These implementations will display the same interface and behavior.

To answer the primary research question based on the three defined sub-research questions, this thesis employs three phases. First, functional and non-functional requirements for the project and the applications will be described. Next, the applications can be implemented according to these requirements. The implementations can then be compared based on several criteria.

This chapter describes the method, detailing each phase and finally presenting the comparison methods used to assess performance, scalability, flexibility and traceability.

\section{Phase 1: Requirement Analysis}

Before implementing, it is necessary to conduct a requirement analysis. This requires defining functional requirements, regarding the domain and business logic of the application. Afterward, non-functional requirements are specified. These include performance goals (\acrshortpl{slo}), auditability and observability of the applications.

The defined requirements then serve as a contract which both implementations adhere to, ensuring a fair comparison.

\section{Phase 2: Implementation of prototypes}

Once requirements for the application are defined, the prototypes can be implemented. As mentioned above, these prototypes should have an identical interface and exhibit identical state transitions and error behaviors.

\section{Phase 3: Evaluation and Comparison}

After implementing the prototypes, they can be compared and evaluated based on the defined research questions. This comparison can be separated into three aspects: performance and scalability, architectural flexibility and traceability.

\subsection{Performance and Scalability}

To answer \hyperref[rs-performance-scalability]{\acrshort{rq} 1}, the performance characteristics of both architectural patterns will be evaluated and compared. To achieve this, load tests are executed on selected endpoints. The theoretical foundations of load testing, environmental constraints, and the data collection methodology are described in this subsection. Finally, perspectives on scalability are presented, which can be derived by analyzing results of load tests.

\subsubsection{Theoretical Load Testing Foundations}
\label{sec:load-test-theory}

To provide accurate performance metrics, load testing is performed on both implementations based on the methodology defined by \textcite[10-17]{kleppmann_designing_2017}. Load is generally characterized using "load parameters," which vary depending on the system's nature. For this thesis, the primary load parameter is the number of concurrent requests to the web server. \cite[11]{kleppmann_designing_2017}

The tests measure \textbf{\acrfull{rps}} and client-side response times. \textcite[15,16]{kleppmann_designing_2017} emphasizes the importance of client-side measurement to account for "queueing delays." As server-side processing is limited by hardware resources (e.g., CPU cores), requests may be stalled before processing begins. Server-side metrics typically measure latencies by executing request filters which only run once the request starts being processed. Therefore, server-side latency metrics often exclude the "waiting time" a request spent in the queue, leading to an overly optimistic view of performance. Consequently, the client-side latencies are more relevant to measure user-perceived performance.

\textcite[16]{kleppmann_designing_2017} also mentions that the load-generating client must continuously send requests without waiting for previous ones to complete, to simulate realistic concurrent user behavior. In load-testing frameworks, this is often called an "open model". If a client waits for its request to complete before sending the next one, queues on the server are kept artificially short. This is typically called "closed model".

This thesis adopts the approach of keeping system resources constant while measuring performance fluctuations under varying load intensities, as opposed increasing system resources to keep unchanged performance. These two perspectives are described by \textcite[13]{kleppmann_designing_2017}

\textcite[264]{abbott_art_2009} mention another way of looking at load testing: In \emph{positive testing}, load is increased until an application is overwhelmed, while \emph{negative testing} takes away system resources such as memory, threads or connections, until the application degrades.

To evaluate results, the arithmetic mean is avoided as it obscures the experience of typical users and the impact of outliers. Instead, this thesis uses \glspl{percentile}. The \gls{median} (P50) serves as the metric for "typical" response time. Exactly 50\% of requests experience the median latency or less, while the other half experiences a higher latency. However, to capture the experience of users facing high delays, it is critical to measure "\glspl{tail-latency}" via a high percentile like P95 or P99. High percentiles identify the performance of outliers which, despite being a numerical minority, often represent the most valuable or complex operations. \cite[14-16]{kleppmann_designing_2017}

\subsubsection{Test Execution}
\label{sec:test-execution-method}

To obtain significant results, each \emph{test configuration} (the combination of a test script and target \acrshort{rps}) should be executed a sufficient number of times. \cite[259]{abbott_art_2009} Between each execution, the application and its dependencies should be restarted to ensure independent results.

\subsubsection{Comparability and Environment}

\hyperref[sec:test-execution-method]{Section \ref*{sec:test-execution-method}} describes that \glspl{test-configuration} must be repeated an appropriate number of times to achieve accurate results. Additionally, the environment in which the tests are run must be controlled. It should be reproducible, identical for each test run and experience no disturbances like system updates while tests are being executed. \cite[Chapter 17]{abbott_art_2009}

\subsubsection{Collected Metrics}
\label{sec:collected-metrics}

All metrics collected during load testing are listed in \autoref{table:collected-metrics}. The column "Metric" shows the name that will be used to refer to this metric from now on. "Location" describes where the metric is being recorded.

As described in \autoref{sec:load-test-theory}, not just the average latency is measured, but \glspl{percentile} are used to accurately report the number of users experiencing the respective latency.

While internal metrics, like CPU usage, RAM usage, the number of database connections ($hikari\_connections$) and the number of Tomcat worker threads are not measures for user-perceived performance, they can give an indicator about bottlenecks inside the applications.

Even though latencies are measured on both the client and the server, the client latency will be used primarily when visualizing results, as it is the latency users perceive when interacting with the applications. The server latency is recorded because it can help identify queueing delays by exposing differences in server and client latencies which exceed a factor of 1.1x.

$axon\_event\_size$ and $axon\_snapshot\_size$ are metrics extracted from the event store (Axon Server). It is only recorded for the ES-CQRS application. $postgres\_size$, the size of the relational database can be recorded for both applications. The CRUD implementation stores its entities and audit log in PostgreSQL, while the ES-CQRS implementation uses PostgreSQL as a secondary data store for denormalized projections and lookup tables. These metrics are relevant when assessing a system's long-term performance, scalability and maintainability.

\begin{table}[htp!]
    \small
    \centering
    \begin{tabularx}{\linewidth}{lXl}
        \toprule
        \textbf{Metric}        & \textbf{Description}                         & \textbf{Location} \\ \midrule
        $latency\_avg$         & Average (arithmetic mean) latency            & Server, Client    \\
        \addlinespace
        $latency\_p50$         & 50th \gls{percentile} Latency (\gls{median}) & Server, Client    \\
        \addlinespace
        $latency\_p95$         & 95th \gls{percentile} Latency                & Server, Client    \\
        \addlinespace
        $latency\_p99$         & 99th \gls{percentile} Latency                & Server, Client    \\
        \addlinespace
        $cpu\_usage$           & CPU usage of the Server process              & Server            \\
        \addlinespace
        $ram\_usage\_heap$     & Usage of Heap memory                         & Server            \\
        \addlinespace
        $ram\_usage\_total$    & Usage of total memory                        & Server            \\
        \addlinespace
        $hikari\_connections$  & Number of Data Source connections            & Server            \\
        \addlinespace
        $tomcat\_threads$      & Number of Tomcat worker threads              & Server            \\
        \addlinespace
        $postgres\_size$       & Size of PostgreSQL database                  & Server            \\
        \addlinespace
        $axon\_event\_size$    & Size of Axon's Event store                   & Axon Server       \\
        \addlinespace
        $axon\_snapshot\_size$ & Size of Axon's Snapshot store                & Axon Server       \\
        \addlinespace
        $axon\_storage\_size$  & $axon\_event\_size + axon\_storage\_size$    & Axon Server       \\
        \bottomrule
    \end{tabularx}
    \caption{All metrics collected during load testing}
    \label{table:collected-metrics}
\end{table}

\subsubsection{Visualizing Results}

Data is visualized using box plots and line graphs. Box plots are used to show the distribution of latencies, while line graphs illustrate performance changes as \acrshort{rps} increases. Per scientific standards, error bars are used to represent the variability of the measurements across the test runs.

\subsection{Scalability Dimensions}
\label{sec:scalability-dimensions}

This section outlines the dimensions of scalability, ranging from mathematical productivity metrics to architectural design choices, that will be used to benchmark and compare the systems under investigation.

\subsubsection{Quantifying Scalability through a Productivity Metric}
\label{sec:quantifying-scalability-metric}

\textcite{jogalekar_evaluating_2000} propose measuring scalability by looking at a system's "productivity" $F(k)$ at different scale factors $k_i$. They argue that productivity is a measure of value versus cost.

This is their proposed productivity metric:
\[F(k)=\lambda(k) \cdot f(k) / C(k)\]

Here, $k$ is the scale factor. This may be  the number of processors or number of users. $\lambda(k)$ is the system's throughput, measured in responses per second. $f(k)$ is the average value of each response. The value of a response must be determined per system. For this thesis, it will be based on \acrshortpl{slo} defined in \autoref{sec:slo}. $C(k)$ is the system's running cost per second at scale $k$. The running cost can be defined through resource consumption, e.g. CPU and RAM usage. The authors mention that several factors can be used for the cost function, they also propose the actual cost of hardware, e.g. when running on the cloud.

From the given productivity metric, a scalability metric can be calculated. To determine if a system is scalable, its productivity at two different scales is compared.

\[ \psi(k_1, k_2) = \frac{F(k_2)}{F(k_1)} \]

When $\psi$ is close to or greater than 1, the system is considered to be scalable. \cite[4-6]{jogalekar_evaluating_2000}

\subsubsection{Headroom and Resource Saturation}

\textcite[Chapter 11]{abbott_art_2009} defines headroom as the amount of free capacity existing within a system before it begins to suffer from performance degradation or outages. To accurately calculate headroom, one must subtract the current usage from the "ideal usage" of a component's maximum capacity, while also factoring in expected future growth and any planned optimization projects. This metric is used to ensure a system can handle future increases in load without failure.

Resource saturation occurs when a system approaches its maximum capacity, a state that should be avoided because system behavior becomes unpredictable near 100\% utilization. As resources saturate, issues such as "thrashing" (excessive swapping of data between memory and disk) emerge, causing performance to plummet dramatically rather than degrade linearly. To prevent saturation and maintain stability, Abbott suggests adhering to an "ideal usage percentage," generally recommending that systems run at only 50\% to 75\% of their total capacity to account for demand variability and estimation errors.

\subsubsection{Architecture that scales}

Scalability is influenced by the alignment between the technology and the organization that builds it. \textcite[2]{abbott_art_2009} argues that scalability issues often originate with people and management rather than technology alone, as the organizational structure dictates communication flows and decision-making efficiency. Consequently, an architecture's ability to scale depends on whether the organization is structured to support growth without creating bottlenecks.

A primary factor in architectural decisions is the choice to "scale out" (horizontal scaling) rather than "scale up" (vertical scaling). \emph{Scaling out} relies on adding more units of commodity hardware, which is cheaper, standard equipment, to handle increased load, whereas \emph{scaling up} depends on purchasing larger, faster, and more expensive individual systems. According to \textcite[203, 207]{abbott_art_2009}, scaling out using commodity hardware should be preferred, because it demonstrates that a system is not viable through faster, more expensive hardware. Instead, a system designed to be horizontally split is more resilient as it does not rely on third-party technologies or technological progress.

Code architecture further impacts scalability through the implementation of fault isolation, asynchronous design, and statelessness. Fault isolative architectures, often described as "swim lanes," ensure that a failure in one component does not propagate to others, preserving the availability of the broader system. \cite[Chapter 21]{abbott_art_2009} Additionally, asynchronous designs are more tolerant of slowdowns, as the speed of the entire system is not determined by its slowest component. This makes asynchronous systems easier to scale. \cite[205]{abbott_art_2009} Finally, stateless systems allow requests to be distributed freely across any available server, making them more scalable. \textcite[206]{abbott_art_2009} argues that stateful applications should be avoided, when possible.

\subsubsection{Load Testing Scenarios}

To accurately assess the performance of the architectures, a diverse suite of load tests should be employed. The selection of specific endpoints for testing follows the methodology outlined by \textcite[260, 265]{abbott_art_2009}, prioritizing scenarios based on usage volume, business criticality, and resource intensity.

When choosing which endpoints are relevant to test, Abbott advocates for a "Pareto Distribution" (80/20 rule) and argues that 20\% of tests will provide developers with 80\% of information. Endpoints can be ordered by criticality, a process which identifies the endpoints that are most likely to affect performance. Endpoints can also be ranked by usage of \gls{io} necessary, locking or synchronous calls. After ordering the endpoints as described, the 80/20 rule can be used to select one read and write per "complexity level", eliminating the need to load test every single endpoint.

\subsection{Flexibility - Architectural Metrics}
\label{sec:flexibility-architectural-metrics}

This section describes various architectural metrics established in literature which are used to assess the flexibility, quality and evolutionary potential of a software architecture. These metrics serve as a basis to answer \hyperref[rs-architecture]{\acrshort{rq} 2}, using static analysis to compare the two architectures.

\subsubsection{Coupling Metrics}
\label{sec:coupling-metrics}

Coupling metrics are based on graph theory. They are used to measure how dependent components are on each other. \emph{Afferent coupling} ($C_a$) measures the number of incoming connections to a code artifact, indicating how many other components depend on it. \emph{Efferent coupling} ($C_e$) calculates the number of outgoing connections to other artifacts, reflecting the component's dependency on external code. The concepts of afferent and efferent coupling were first introduced by \textcite{yourdon_structured_1978} in 1978.

\subsubsection{Instability and Abstractness}
\label{sec:instability}

\emph{Instability} assesses the volatility of a code base by calculating the ratio of efferent coupling to the sum of all coupling, indicating how easily a component breaks when changed. \emph{Abstractness} determines the ratio of abstract artifacts, such as interfaces and abstract classes, to concrete implementations within a module. Finally, the \emph{Distance from the Main Sequence} combines abstractness and instability to determine if a component is optimally balanced or falls into problematic "Zones of Uselessness or Pain"~\cite[261-268]{martin_agile_2003}. Martin considers a stable, concrete package to fall into the "Zone of Pain", while instable, highly abstract packages fall into the "Zone of Uselessness". The Main Sequence is illustrated in \autoref{fig:method-main-sequence}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Abstractness-Instability-Main_Sequence.png}
    \caption[Main Sequence, Zones of Pain and Uselessness]{Main Sequence, Zones of Pain and Uselessness. Adapted from \cite[266]{martin_agile_2003}}
    \label{fig:method-main-sequence}
\end{figure}

\subsubsection{MOOD metrics}
\label{sec:mood}

\emph{\gls{mood}}, introduced by \textcite{brito_e_abreu_object-oriented_1994}, provide a summary of the overall quality of an object-oriented project. The \emph{\gls{mhf}} represents the ratio of hidden methods to total methods, indicating the level of encapsulation. The \emph{\gls{ahf}} measures the ratio of hidden attributes to total attributes, serving as indicator for the system's encapsulation. The \emph{\gls{mif}} calculates the ratio of inherited methods to the total available methods. The \emph{\gls{aif}} determines the ratio of inherited attributes to the total attributes available in the classes. The \emph{\gls{pf}} measures the actual number of different polymorphic situations relative to the maximum possible distinct situations. The \emph{\gls{cf}} evaluates the ratio of actual couplings not related to inheritance against the maximum possible number of couplings in the system. The \emph{\gls{clf}} measures the ratio of independent class groups, or "Class Clusters," to the total number of classes, identifying disjoint sub-graphs within the system. Finally, the \emph{\gls{rf}} measures the ratio of code reused from library code or inheritance to the total code written. All \gls{mood} metrics are expressed in percentages, meaning they take a value between $0$ and $1$.

% \subsubsection{Complexity Metrics}
% \label{sec:complexity-metrics}
% ACHTUNG: da die Ergebnisse sehr nichtssagend sind, ist das auskommentiert. 
% A common metric for code complexity is cyclomatic complexity. Cyclomatic complexity can give a measure for code complexity at the function, class or application level by measuring the number of decision paths through a given component. \cite[79,80]{richards_fundamentals_2020} It is distinguished between essential complexity (complexity necessary for the domain) and accidental complexity introduced through poor coding practices.

\subsubsection{Chidamber and Kemerer (CK) Metrics}
\label{sec:ck-metrics}

\emph{\gls{ck}} are the most well-known object-oriented suite of metrics, according to \textcite[162]{chawla_comparative_2013}. They are described in a work from 1994 by \textcite{chidamber_metrics_1994}. \emph{\gls{wmc}} calculates the sum of the complexities of all methods within a class to determine how much effort is required to maintain it. The authors deliberately do not give a specific complexity function to make the metric as general as possible. \emph{\gls{dit}} measures the maximum length from a specific node to the root of the tree, where deeper trees imply higher complexity. \emph{\gls{noc}} counts the immediate subclasses of a class, indicating the potential influence on the design and level of reuse. \emph{\gls{cbo}} counts the number of other classes to which a specific class is coupled, serving as an indicator of sensitivity to changes in other parts of the design. \emph{\gls{rfc}} counts the number of methods that can be executed in response to a message received by an object, reflecting the class's complexity. \emph{\gls{lcom}} measures the degree to which methods within a class reference different instance variables, where high values indicate a class does too many unrelated things.

\subsubsection{Usefulness of Static Analysis}

So, how can these static analysis metrics help to answer \acrshort{rq} 2? They offer a quantitative method to identify the fundamental structural differences between architectures by measuring how components are organized and connected. Metrics such as \emph{\gls{cbo}} and \emph{Afferent/Efferent coupling} reveal the degree of dependence between modules. High coupling suggests a structure where changes in one area may have affects on large parts of the system, while low coupling indicates better modularity and isolation, leading to easier maintenance. \cite{chawla_comparative_2013} Additionally, cohesion metrics like \emph{\gls{lcom}} and complexity measures such as \emph{\gls{wmc}} help determine if an architecture is composed of focused, manageable components or large, complex classes that attempt to do too much. \cite{chawla_comparative_2013}

Regarding long-term flexibility and evolution, metrics that evaluate stability and abstraction provide insights into how easily a codebase may change over time. For example, calculating \emph{Instability} and \emph{Abstractness} allows architects to visualize their \emph{"Distance from the Main Sequence,"} highlighting which parts of the system may be becoming too rigid to change or too abstract to be useful. \cite[45-47]{richards_fundamentals_2020}

\subsubsection{Critiques of Static Analysis methods}

\textcite[44]{richards_fundamentals_2020} offer a strong critique of the \gls{lcom} metric. They argue that the metric possesses "serious deficiencies" because it only measures the structural lack of cohesion (how methods and fields physically interact). The metric cannot determine if the pieces of code logically fit together. They also criticize complexity metrics like cyclomatic complexity, calling them "blunt". Their primary critique is that complexity metrics can not distinguish between essential complexity (complexity necessary for the domain) and accidental complexity introduced through poor coding practices. \cite[48]{richards_fundamentals_2020} Finally, the authors express their critique of connascence, mentioning that it focuses too much on the low-level details of software coupling. They argue that developers should care more about \emph{how} components are coupled (e.g. synchronous or asynchronous). \cite[53]{richards_fundamentals_2020}

\textcite{drotbohm_instability-abstractness-relationship_2024}, a Spring engineer, offers his critique on the \emph{abstractness} metric, highlighting that it mistakes the existence of interfaces with actual, useful abstractions.

\subsubsection{Schema Evolution TODO}

TODO: Finally, talk about schema evolution a little bit. Can we determine methods to decide which architecture is more flexible regarding schemas?

\subsection{Traceability}

This section describes how traceability will be compared; it serves as a basis to answer \hyperref[rs-traceability]{\acrshort{rq} 3}. The traceability aspect will be compared via two perspectives. First, the \emph{accuracy} of historic reconstruction is compared. Then, \emph{efficiency} of reconstructions is evaluated and compared.

\subsubsection{Accuracy of reconstruction}

Several qualitative criteria for the accuracy of an historic reconstruction can be defined. \emph{Source of truth integrity} defines how the system guarantees that the historic records match the current state. When a system needs to write state changes to two separate data stores, this creates a problem called "dual-write" problem in distributed systems.

\emph{Intent preservation} assesses the ability of a system to distinguish between different business reasons for the same data change.

\emph{Schema resilience} examines how historical data survives the evolution of the application's data structure. As business rules change, entities and database schemas evolve. The history must remain readable without being corrupted by modifications to schemas.

\subsubsection{Efficiency}

The primary objective is to measure how each architecture handles "time-travel" queries. These are requests that require the system to reconstruct a specific state from the past. To ensure a fair comparison, both applications will be tested under identical load testing scenarios.

Two distinct scenarios of varying complexity will be evaluated: A "simple state reconstruction" scenario will query the historic state of one entity in the system. A more complex state reconstruction scenario will require a broader view of the system's state to be reconstructed. This requires the system to cross-reference and reconstruct multiple entity relationships as they existed at that moment.

The project's existing load-testing infrastructure will be used for these tests. This allows to capture the same data points, such as latencies and resource consumption, which can then serve as a basis to compare the efficiency of historic reconstruction in both applications.

\section{Limitations of the method}
\label{sec:limitations}

While the research design aims for a rigorous comparison, several limitations must be acknowledged. These constraints arise from the controlled environment and the scope of the prototype implementations.

The evaluation is conducted on a single machine running virtual machines, which consequently live on the same network. Therefore, the prototypes are not tested on distributed clusters, which would be necessary to observe the real effects of horizontal scaling. This setup also eliminates common distributed system challenges, such as network partitions, varying latency, and partial failures. By running all components in a controlled environment, the results may not fully reflect the resilience or overhead of the architectures when deployed across a distributed system. These challenges, which are bypassed by this study's setup, are known as the eight fallacies of distributed systems~\cite{rotem-gal-oz_fallacies_2008}, a term first coined by P. Deutsch in 1994.

In the \acrshort{crud} implementation, the audit log resides within the same relational database as the operational data. In a production-grade system, these might be separated to prevent the audit log's growth from impacting query performance, with the audit log being an additional data store, creating a "dual-write" problem~\cite[Chapter 11]{kleppmann_designing_2017}. Furthermore, because all components share the same underlying hardware resources during testing, the performance of one architectural layer (e.g., the event store) may influence another in ways that would not occur if they were isolated on dedicated hardware.

The load testing scenarios are artificial and focused on specific endpoints rather than complete user journeys. \textcite[267]{abbott_art_2009} suggest that the most accurate load profiles are derived from real-world application or load-balancer logs. Without such data, the tests rely on the estimation of relevant scenarios. Additionally, the prototypes lack complex access control patterns and multi-tenancy logic, which in a real-world scenario could introduce additional, non-negligible overhead.

The comparison focuses on the initial implementation and short-term performance, which may obscure long-term operational complexities. While data volume is simulated, the actual challenges of managing a growing event store, specifically the evolution of the schema over time, are evaluated theoretically rather than through years of operational data. As \textcite{young_versioning_2017} notes, the permanence of events in an immutable log necessitates complex strategies like upcasting or transformation to handle changing business requirements. Without observing these "versioning" cycles over a multi-year lifecycle, an incorrect view of the maintainability of event-sourced architectures may be presented.
