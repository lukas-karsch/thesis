% !TeX root = ../main.tex
\chapter{Results (TODO)}

\section{Performance}
\label{sec:performance-results}

This section describes results of load testing. Each \gls{l} is presented in order, describing the kind of endpoint and test that was employed.

For all load tests, the primary results are visualized through diagrams like latency-vs-load line plots. The raw numerical data, including confidence intervals and significance levels for all tested load levels, are provided in \autoref{appendix:results}.

\subsection{Significance}

The significance levels indicated in the tables inside the appendix are calculated using the \emph{Mann-Whitney U} significance test. In load testing, response times are usually skewed by outliers (tail latencies) that would incorrectly bias a standard average-based test. The Mann-Whitney U test is used because it doesn't require normally distributed samples and compares the \emph{rankings} of results rather than the averages, making it more resistant to these outliers\cite[Chapter 13.3]{triola_elementary_2012}. TODO: cite on "latencies are skewed"

\subsection{Warmup}

Median CPU Usage: warmup times omitted (first few second spikes)

\subsection{Speedup / Ratios}

When comparing the CRUD and ES-CQRS applications' results, sometimes a "speedup" factor or ratio is given. For example, a 5x speedup may be stated. These precise numbers supporting these statements are also available in the tables inside the appendix. The term "speedup" is used when comparing latencies, while the term "ratio" is used for resource metrics like $cpu\_usage$. The given speedups / ratios are always calculated like this: $\frac{metric_{crud}}{metric_{es\_cqrs}}$.

\subsection{Dropped Iterations}

In some tests, iterations were dropped. This occurs if an iteration takes longer than 1 second and no more \glspl{VU} --- which act as request processors --- are available to k6. The fact that iterations were dropped during testing is mentioned when describing results, and the rate of dropped iterations per second ($dropped\_iterations\_rate$) is available in the results tables inside the appendix.

The consequence of dropped iterations on test results will be outlined in \autoref{ch:discussion}.

\subsection{Data Store Size}

The size of the data store is calculated differently for both applications. The CRUD application uses PostgreSQL as its only data store. The $postgres\_size$ metric equals the application's data store size.

The total data store size for the ES-CQRS application is defined as the sum of the PostgreSQL projection size and the allocated Axon storage. It is important to note that Axon Server allocates storage in fixed-size segments (or "pages") of 4MB. Because these segments are allocated eagerly, the total storage includes a minimum overhead of 8MB (one 4MB segment each for events and snapshots), regardless of the actual data density within those blocks. Consequently, the measured size represents the allocated capacity rather than the literal byte-count of the stored records.

\subsection{Graphs}

In graphs that show latencies, the shaded areas in the line graphs represent the \emph{standard deviation} of the latency measurements, indicating the statistical spread or variance around the median performance at each load level. Wider areas of standard deviation indicate that the applications experience a wider range of response times.

Graphs showing resource consumption typically show the area between the 25th and 75th percentile in the shaded area, also called \gls{iqr}.

TODO: Explain boxplots.

\subsection{Write Performance}

\subsubsection{L1: Create Courses Simple}
\label{sec:l1}

The endpoint \texttt{POST} \texttt{/courses} is be used by professors to create courses. The "simple" test case tests creation of courses \emph{without prerequisites}. This means that no validation is necessary to create a new entity, making this test the raw insertion performance.

\autoref{fig:l1_combined} presents line graphs using a logarithmic y-axis. The graphs describe the observed client-side and server-side latencies under varying loads of both applications by showing their $latency\_p50$ (median) and $latency\_p95$ (tail latency). The Latency \gls{slo} defined the threshold at 100ms, meaning the ES-CQRS application failed to satisfy the \gls{slo} between 500 and 1000 \gls{rps}. Meanwhile, the \gls{crud} application achieves a sub 10ms $latency\_p95$ until at least 1000 \gls{rps}.

It can be noted that the latency curve follows the same pattern in both the measurements made on the client and on the server, except for the ES-CQRS $latency\_p95$ which exhibits a value of over 1000ms on the client, but only around 300ms on the server.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/create-course-simple/create_course_simple_latency-vs-load__client.png}
        \caption{Latency vs. Load measured on the client}
        \label{fig:l1_latency_vs_load_client}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/create-course-simple/create_course_simple_latency-vs-load__server.png}
        \caption{Latency vs. Load measured on the server}
        \label{fig:l1_latency_vs_load_server}
    \end{subfigure}
    \caption[L1 Performance Metrics, load measured in \gls{rps}]{L1 Performance Metrics: Load measured in \gls{rps}. Detailed results in \ref{results:l1}.}
    \label{fig:l1_combined}
\end{figure}

\autoref{fig:l1_cpu_usage_vs_load} presents median CPU usage of the endpoint under increasing load. The ES-CQRS application generally has a higher CPU usage, exhibiting a value of $\approx$45\% at 1000 \gls{rps}, while the CRUD application uses $\approx$10\%. The CRUD application's CPU usage rises linearly, while the ES-CQRS application's CPU usage rises slower past 500 \gls{rps}.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/create-course-simple/CPU_Usage_vs_Load.png}
        \caption{CPU Usage (\%) vs. Load}
        \label{fig:l1_cpu_usage_vs_load}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/create-course-simple/create-course-threadpool-usage.png}
        \caption{Threadpool Usage vs. Load}
        \label{fig:l1_data_store_vs_load}
    \end{subfigure}
    \caption[L1 Resource Usage, load measured in \gls{rps}]{L1 Resource Usage: Load measured in \gls{rps}. Detailed results in \ref{results:l1}.}
    \label{fig:l1_resource_vs_load}
\end{figure}

In \autoref{fig:l1_data_store_vs_load}, the size of the data store under increasing load is presented. The CRUD application's data store, consisting only of the PostgreSQL database, exhibits a linear growth, reaching a size of $\approx$70MB at 1000 \gls{rps}. With $10 000$ requests sent at 1000 \gls{rps}, this comes out to around 7kB per persisted course. The ES-CQRS graph also grows linearly. At 500 \gls{rps}, the size of the data store is about 60MB, a 1.5x higher storage consumption than the CRUD application. This equals around 12kB per persisted course. However, at 1000 \gls{rps}, the storage size decreases. This can be attributed to a projection lag caused by the eventual consistency present in the application and is explained further in \autoref{ch:discussion}. The true storage consumption of the ES-CQRS application can be assumed to be around 120MB, once all projections are updated.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/create-course-simple/create-course-simple_db-connections.png}
        \caption{Database Connections vs. Load}
        \label{fig:l1_db_connections}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/create-course-simple/data_store_size.png}
        \caption{Data Store Size vs. Load}
        \label{fig:l1_data_store_vs_load}
    \end{subfigure}
    \caption[L1 Database Usage, load measured in \gls{rps}]{L1 Database Usage: Load measured in \gls{rps}. Detailed results in \ref{results:l1}.}
    \label{fig:l1_database}
\end{figure}

\subsubsection{L2: Create Courses Prerequisites}
\label{sec:l2}

This test also evaluates the performance of the endpoint described in \autoref{sec:l1}. However, before executing the load generation, a set of "prerequisite" courses are generated, which are then referenced during load generation. This creates the necessity to do additional checks on existing data, verifying whether the referenced courses actually exist.

The performance, presented in \autoref{fig:l2_combined} using a logarithmic y-axis, is similar to the observations made in \hyperref[sec:l1]{L1}. After exceeding 500 \gls{rps}, the ES-CQRS application fails to satisfy \ref{slo-latency} with a $latency\_p95$ exceeding the threshold of 100ms. The CRUD application, on the other hand, is able to fulfill the latency threshold up to at least 1000 \gls{rps}.

It can be noted that the latency curve follows the same pattern in both the measurements made on the client and on the server, except for the ES-CQRS $latency\_p95$ which exhibits a value of over 1000ms on the client, but only $\approx$300ms on the server.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/create-course-prerequisites/create-course-prerequisites__latency-vs-load__client.png}
        \caption{Latency vs. Load measured on the client}
        \label{fig:l2_latency_vs_load_client}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/create-course-prerequisites/create-course-prerequisites__latency-vs-load__server.png}
        \caption{Latency vs. Load measured on the server}
        \label{fig:l2_latency_vs_load_server}
    \end{subfigure}
    \caption[L2 Performance Metrics, load measured in \gls{rps}]{L2 Performance Metrics: Load measured in \gls{rps}. Detailed results in \ref{results:l2}.}
    \label{fig:l2_combined}
\end{figure}

\autoref{fig:l2_cpu_usage_vs_load} presents the median $cpu\_usage$ of the endpoint under increasing load. The ES-CQRS application has a higher CPU usage, exhibiting a value of $\approx45\%$ at 1000 \gls{rps}, while the CRUD application uses $\approx10\%$. The CRUD application's CPU usage rises linearly, while the ES-CQRS application's CPU usage rises slower past 500 \gls{rps}.

TODO insert data store size here after re-running the test!

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/create-course-prerequisites/CPU_usage_vs_load.png}
        \caption{CPU Usage (\%) vs. load. Detailed results in \ref{table:run-create-course-prerequisites-aggregated-cpu-usage}}
        \label{fig:l2_cpu_usage_vs_load}
    \end{subfigure}
    \caption{L2: Resource usage graphs}
    \label{fig:l2_resource_vs_load}
\end{figure}

\subsubsection{L3: Enrollment (TODO)}

The endpoint \texttt{POST /lectures/{lectureId}/enroll} enrolls students to a lecture. Its performance under load is visualized in \autoref{fig:l3_combined} using logarithmic x- and y-axes.

The ES-CQRS application's $latency\_p95$ rises above 100ms between 50 and 100 \gls{rps}, violating \autoref{slo-latency}. The CRUD application's $latency\_p95$ and $latency\_p50$ remain below 10ms until 200 \gls{rps}, representing more than a 20x slowdown for the ES-CQRS application. Between 200 and 500 \gls{rps}, CRUD's P95 latency rises to over 1000ms, violating \ref{slo-latency}. At 500 \gls{rps}, ES-CQRS' $latency\_p95$ exceeds 10,000ms.

It is worth noting that starting from 200 \gls{rps}, the ES-CQRS's load tests started dropping iterations. $dropped\_iterations\_rate$ reached a value of around 240 at 500 \gls{rps}, at which point the CRUD application also exhibited dropped iterations with a rate of around 36.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/enrollment/enrollment_latency-vs-load_client.png}
        \caption{Latency vs. Load measured on the client}
        \label{fig:l3_latency_vs_load_client}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/enrollment/enrollment_latency-vs-load_server.png}
        \caption{Latency vs. Load measured on the server}
        \label{fig:l3_latency_vs_load_server}
    \end{subfigure}
    \caption[L3 Performance Metrics, load measured in \gls{rps}]{L3 Performance Metrics: Load measured in \gls{rps}. Detailed results in \ref{results:l3}.}
    \label{fig:l3_combined}
\end{figure}

Now I show graphs for resource consumption during load tests.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/enrollment/enrollment-cpu-usage-vs-load.png}
        \caption{Resource Usage (\%) vs. Load}
        \label{fig:l3_cpu_usage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/enrollment/enrollment-threadpool_usage.png}
        \caption{Threadpool Usage vs. Load}
        \label{fig:l3_threadpool_usage}
    \end{subfigure}
    \caption[L3 Resource Usage, load measured in \gls{rps}]{L3 Resource Usage: Load measured in \gls{rps}. Detailed results in \ref{results:l3}.}
    \label{fig:l3_resource_usage_cpu}
\end{figure}

This is usage of database.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/enrollment/enrollment-database-connections.png}
        \caption{Database Connections vs. Load}
        \label{fig:l3_db_connections}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/enrollment/enrollment-data-size.png}
        \caption{Data Store Size vs. Load}
        \label{fig:l3_data_size}
    \end{subfigure}
    \caption[L3 Database Usage, load measured in \gls{rps}]{L3 Database Usage: Load measured in \gls{rps}. Detailed results in \ref{results:l3}.}
    \label{fig:l3_database}
\end{figure}

\subsection{Read Performance}

\subsubsection{L4: Read Lectures}
\label{sec:l4}

\texttt{GET} \texttt{/lectures} returns all lectures a student is enrolled or waitlisted in. \autoref{fig:l4_combined} presents client-side and server-side latencies for this endpoint using a logarithmic y-axis.

In the client-side graph (\autoref{fig:l4_latency_vs_load_client}), both applications maintain $latency\_p50$ and $latency\_p95$ of below 10ms up to 3000 \gls{rps}. At 3000 \gls{rps}, the ES-CQRS application exhibits an around 5x higher $latency\_p95$ than the CRUD app. Beyond 3000 \gls{rps}, though, the CRUD latencies overtake the ES-CQRS latencies. At 4000 \gls{rps}, the ES-CQRS application exhibits a $latency\_p95$ of around 370ms, which is around 1.7x faster than  the CRUD application at 620ms.

Both applications violate \ref{slo-latency} beyond 3000 \gls{rps}. It can also be noted that the standard deviation, indicated by the shaded areas, grows wider, especially in the CRUD application.

The server-side graph, presented in \autoref{fig:l5_latency_vs_load_server}, initially shows a similar pattern. At 3000 \gls{rps}, the latencies also start increasing, however, the observed increase is not as strong as in the client-side latencies. At 4000 \gls{rps}, the ES-CQRS application shows a $latency\_p95$ of around 60ms, the CRUD application has a $latency\_p95$ of around 130ms. The standard deviation does not grow as wide as in the client-side graph.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/read-lectures/read-lectures_latency-vs-load_client__to4000RPS.png}
        \caption{Latency vs. Load measured on the client}
        \label{fig:l4_latency_vs_load_client}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/read-lectures/read-lectures_latency-vs-load_server__to4000RPS.png}
        \caption{Latency vs. Load measured on the server}
        \label{fig:l4_latency_vs_load_server}
    \end{subfigure}
    \caption[L4 Performance Metrics, load measured in \gls{rps}]{L4 Performance Metrics: Load measured in \gls{rps}. Detailed results in \ref{results:l4}.}
    \label{fig:l4_combined}
\end{figure}

\subsubsection{L5: Get credits}
\label{sec:l5}

The GET /stats/credits endpoint retrieves a student's total collected credits. \autoref{fig:l5_combined} presents the endpoint's client-side and server-side latencies using a logarithmic y-axis.

\autoref{fig:l5_latency_vs_load_client} presents the client-side latency under increasing load. It can be seen that the $latency\_p50$ remains below 5ms for both applications until around 2000 \gls{rps}.

Between 2000 \gls{rps} and 3000 \gls{rps}, a performance divergence occurs: the CRUD application's $latency\_p50$ and $latency\_p95$ increase to more than 1000ms, violating \ref{slo-latency} beyond 2000 \gls{rps}. On the other hand, the ES-CQRS application still exhibits a $latency\_p95$ of less than 10ms at 3000 \gls{rps}. This represents a speedup of around 200x. At 4000 \gls{rps}, the ES-CQRS application exhibits a $latency\_p95$ of around 80ms, a 20x speedup. Beyond 4000 \gls{rps}, the ES-CQRS app's latencies also begin to violate \ref{slo-latency}, reaching latencies around 1000ms.

It can be noted that starting at 3000 \gls{rps}, the CRUD application's latencies seem to reach a plateau, with identical observed latencies at 3000, 4000 and 5000 \gls{rps}. Meanwhile, the ES-CQRS application's latencies keep increasing up to a load of 5000 \gls{rps}. At this load, the ES-CQRS application's $latency\_p95$ is about 2.4x lower than the CRUD application's.

The server latencies, displayed in \autoref{fig:l5_latency_vs_load_server}, differ from client latencies at higher loads. Starting at 3000 \gls{rps}, the measured latencies do not increase as strongly as observed on the client. At 5000 \gls{rps}, the $latency\_p95$ of the CRUD app resides around 260ms, while the ES-CQRS app has a $latency\_p95$ of around 50ms.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/get-credits/get-credits-latency-vs-load__client.png}
        \caption{Latency vs. Load measured on the client}
        \label{fig:l5_latency_vs_load_client}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/get-credits/get-credits-latency-vs-load__server.png}
        \caption{Latency vs. Load measured on the server}
        \label{fig:l5_latency_vs_load_server}
    \end{subfigure}
    \caption[L5 Performance Metrics, load measured in \gls{rps}]{L5 Performance Metrics: Load measured in \gls{rps}. Detailed results in \ref{results:l5}.}
    \label{fig:l5_combined}
\end{figure}

\subsection{Time to Consistency}
\label{sec:l6}

\ref{slo-freshness} defined a threshold of 100ms inside which all writes shall be reflected on the read-side. This "freshness" is measured in the following test.

\subsubsection{L6: Create Lecture, then Read}

This load test differs from others in the fact that each iteration executes 2 \gls{http} requests. The first request creates a lecture. After sleeping for 100ms --- the consistency threshold defined in \ref{slo-freshness} --- the script executes a request to \texttt{GET} the created lecture. If status code $404$ is returned, the write was not reflected in the read model in time. The rate of successful reads is recorded as a metric called $read\_visible\_rate$, presented in \autoref{fig:l6_read_visible_rate}. It can be seen that once exceeding 200 \gls{ips}, the ES-CQRS application failed to synchronize the read-side fast enough. \autoref{fig:l6_latency_vs_load} shows the latencies under varying loads. The $latency\_p50$ remains similar for both applications, however the ES-CQRS application's $latency\_p95$ increases with rising \gls{ips}, finally violating the threshold of 100ms defined in \ref{slo-latency} once exceeding 400 \gls{ips}.

The CRUD application constantly exhibits a 100\% $read\_visible\_rate$.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/time-to-consistency/get_lecture_simple_latency-vs-load__client.png}
        \caption{Latency vs. Load, visualized for the subsequent GET request}
        \label{fig:l6_latency_vs_load}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/time-to-consistency/get_lecture-read-visible-rate.png}
        \caption{Rate of visible reads}
        \label{fig:l6_read_visible_rate}
    \end{subfigure}
    \caption{L6 Performance Metrics, load measured in \gls{ips} with 2 requests per iteration. Detailed results in \ref{results:l6}}
    \label{fig:l6_combined}
\end{figure}

\subsection{Historic Reconstruction (TODO)}
\label{sec:results-historic-reconstruction}

Blabla.

\subsubsection{L7: Grade History (TODO)}

Describe endpoint. Then, show latency graphs and describe.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/grade-history/grade-history-latency-vs-load-CLIENT.png}
        \caption{Latency vs. Load measured on the client}
        \label{fig:l7_latency_vs_load_client}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/results/grade-history/grade-history-latency-vs-load-SERVER.png}
        \caption{Latency vs. Load measured on the server}
        \label{fig:l7_latency_vs_load_server}
    \end{subfigure}
    \caption{L7 Performance Metrics, load measured in \gls{rps}. Detailed results in \ref{results:l7}}
    \label{fig:l7_combined}
\end{figure}

Below that, show CPU usage in \% and threadpool usage.

Finally, show database connections.

\section{Static Analysis}
\label{sec:statc-analysis-results}

\acrshort{rq} 2 attempts to evaluate the architectural flexibility of the architectural styles \gls{crud} and \gls{es}-\gls{cqrs}. In \autoref{sec:flexibility-architectural-metrics}, several static analysis methods were established. This section presents results and visualizations for these metrics.

\subsection{Coupling Metrics}
\label{sec:results-coupling}

Afferent coupling ($C_a$, describing incoming connections) and Efferent coupling ($C_e$, describing outgoing connections), which were described in \autoref{sec:coupling-metrics}, are calculated by MetricsReloaded on a package-basis. The results are visualized in a boxplot. Package size influences the value of $C_a$ and $C_e$, which is why the results were normalized by class count. Therefore, the plots show this data:

\[C_a\_norm = \frac{C_a}{Class\_count}\]
\[C_e\_norm = \frac{C_e}{Class\_count}\]

\autoref{fig:combined_ca} presents the normalized $C_a$ per application. The CRUD application generally has higher Afferent coupling across its packages (a 33\% higher median and higher 75th \gls{percentile}). Most packages in the ES-CQRS architecture have low Afferent coupling, but some packages areas are more coupled than any package found in the CRUD app.

\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/Ca_normalized_Afferent_Coupling_per_package.png}
        \caption{Boxplot distribution}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \small
        \resizebox{\textwidth}{!}{
            \input{tables/ca_norm.tex}
        }
        \caption{Descriptive statistics}
    \end{subfigure}
    \caption{Comparison of $C_a$ (Afferent coupling) by Application.}
    \label{fig:combined_ca}
\end{figure}

Regarding Efferent coupling, the two architectures show more similarity. The medians are almost equal at values of 2.1 (CRUD) and 2.5 (ES-CQRS). CRUD's \gls{iqr} is wider, ranging from 0 to around 13, while the ES-CQRS \gls{iqr} ranges from 0 to around 9. However, the ES-CQRS architecture displays more outliers (7, versus 2 in CRUD) reaching a value of 148, while the maximum value of the CRUD architecture is 92.

\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/Ce_normalized_Efferent_Coupling_per_package.png}
        \caption{Boxplot distribution}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \small
        \resizebox{\textwidth}{!}{
            \input{tables/ce_norm.tex}
        }
        \caption{Descriptive statistics}
    \end{subfigure}
    \caption{Comparison of $C_e$ (Efferent coupling) by Application.}
    \label{fig:combined_ce}
\end{figure}

\subsection{Dependency Metrics}
\label{sec:results-dependency}

\autoref{table:dependency-metrics} outlined all dependency metrics recorded in the applications. In this section, these metrics are visualized using boxplots and descriptive statistics. Per-class results for the described dependency metrics are available in \autoref{appendix:dependency}.

\autoref{fig:combined_dpt} illustrates the distribution of $Dpt$, the number of classes depending directly on a class (\emph{dependents}), for both architectures. Both architectures have an equal median at 2. This indicates that in both cases, a typical class has two dependents. The range between the 25th to 75th \gls{percentile}, also called \gls{iqr} is indicated by the shaded areas. It differs for the two architectures. The CRUD architecture shows that 50\% of classes have 1 to 3 dependents, while the ES-CQRS architecture exhibits higher variability. Its IQR spans from 0 to 4. Notably, since the 25th percentile aligns with the minimum value of 0, at least 75\% of the classes in this architecture have 4 or fewer dependents.

\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/class_dependencies_DPT.png}
        \caption{Boxplot distribution}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \small
        \resizebox{\textwidth}{!}{ % Resizes table to fit subfigure width
            \input{tables/dpt.tex}
        }
        \caption{Descriptive statistics}
    \end{subfigure}
    \caption{Comparison of $Dpt$ (direct dependants) by Application.}
    \label{fig:combined_dpt}
\end{figure}

$Dpt^*$, the transitive dependent count, is presented in \autoref{fig:combined_dpt_transitive}. Compared to $Dpt$, a larger difference between the architectures is visible. While the median values of both applications are still similar, with values between 2 and 4, the CRUD architecture's \gls{iqr} has a much wider range than the ES-CQRS architecture. 50\% of classes in the CRUD architecture have between 2 and 33 transitive dependents, while at least 75\% of the ES-CQRS architecture's classes have between 0 and 7 transitive dependents.

\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/class_dependencies_DPT-transitive.png}
        \caption{Boxplot distribution}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \small
        \resizebox{\textwidth}{!}{ % Resizes table to fit subfigure width
            \input{tables/dpt_transitive.tex}
        }
        \caption{Descriptive statistics}
    \end{subfigure}
    \caption{Comparison of $Dpt^*$ (transitive dependants) by Application.}
    \label{fig:combined_dpt_transitive}
\end{figure}

\autoref{fig:combined_dcy} illustrates the distribution of $Dcy$, a metric representing the number of classes a given class directly depends on (\emph{dependencies}). Similar to the results for $Dpt$, the direct dependencies across both architectures exhibit similar values. CRUD and ES-CQRS architecture exhibit a median $Dcy$ of 2, respectively 1, with both \glspl{iqr} situated between 0 and 4. Both architectures feature several outliers, with individual classes reaching direct dependency counts of approximately 40.

\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/class_dependencies_DCY.png}
        \caption{Boxplot distribution}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \small
        \resizebox{\textwidth}{!}{
            \input{tables/dcy.tex}
        }
        \caption{Descriptive statistics}
    \end{subfigure}
    \caption{Comparison of $Dcy$ (direct dependencies) by Application.}
    \label{fig:combined_dcy}
\end{figure}

A more pronounced divergence is visible in the transitive dependencies, $Dcy^*$, presented in \autoref{fig:combined_dcy_transitive}. In the CRUD architecture, this distribution shows a much wider range, with the \gls{iqr} sitting between 1 and 26 transitive dependencies.

The ES-CQRS architecture exhibits a more concentrated distribution. Its \gls{iqr} is narrower, with at least 75\% of classes having between 0 and 3 transitive dependencies. While there are numerous outliers reaching more than 40 transitive dependencies, the primary distribution remains lower than that of the CRUD application.

\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/class_dependencies_DCY-transitive.png}
        \caption{Boxplot distribution}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \small
        \resizebox{\textwidth}{!}{
            \input{tables/dcy_transitive.tex}
        }
        \caption{Descriptive statistics}
    \end{subfigure}
    \caption{Comparison of $Dcy^*$ (transitive dependencies) by Application.}
    \label{fig:combined_dcy_transitive}
\end{figure}

$PDpt$, presented in \autoref{fig:combined_PDpt}, is a metric describing the number of packages transitively depending on a class. Both architectures have a median value of 1 and a P75 of 2, indicating that at least 75\% of classes have less than 2 transitive package dependents.

\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/class_dependencies_PDpt.png}
        \caption{Boxplot distribution}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \small
        \resizebox{\textwidth}{!}{
            \input{tables/PDpt.tex}
        }
        \caption{Descriptive statistics}
    \end{subfigure}
    \caption{Comparison of $PDpt$ (transitive package dependents) by Application.}
    \label{fig:combined_PDpt}
\end{figure}

The $PDcy$ metric describes the number of packages a class transitively depends on. It is presented in \autoref{fig:combined_PDcy}. Again, both architectures show similar values with a median of 1.5 for the CRUD architecture and 1 for ES-CQRS. The \gls{iqr} ranges from $\approx1$ to $\approx3$ for CRUD, and from 0 to 2 for ES-CQRS. The ES-CQRS architecture exhibits more outliers at 7 with a maximum value of 13, while the CRUD application shows one outlier depending on 8 packages.

\begin{figure}[H]
    \centering
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/class_dependencies_PDcy.png}
        \caption{Boxplot distribution}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.48\textwidth}
        \centering
        \small
        \resizebox{\textwidth}{!}{
            \input{tables/PDcy.tex}
        }
        \caption{Descriptive statistics}
    \end{subfigure}
    \caption{Comparison of $PDcy$ (transitive package dependencies) by Application.}
    \label{fig:combined_PDcy}
\end{figure}

\subsection{Instability and Abstractness (TODO: move below Coupling)}

Instability $I$ is defined as the ratio of Efferent coupling to the total coupling of a package: $I = \frac{C_e}{C_e + C_a}$. Therefore, it can take values between 0 and 1. It is visualized using a boxplot in \autoref{fig:instability}. The plot highlights a wide \gls{iqr} and a median value around 0.5 to 0.6 for both architectures.

Abstractness $A$ measures the ratio of abstract classes and interfaces to the total number of classes in a package: $A = \frac{Abstract Classes + Interfaces}{Total Classes}$. Therefore, it can take values between 0 and 1. Its visualization in \autoref{fig:abstractness} shows that the typical package in both applications has an Abstractness of 0. The \gls{iqr} of the CRUD architecture reaches from 0 to 0.2; the \gls{iqr} of the ES-CQRS architecture reaches from 0 to 0.4.

\autoref{fig:main-sequence} shows a scatter plot which visualizes $A$ and $I$, as well as the "Main Sequence" (gray diagonal line). The concept of the "Main Sequence" was explained in \autoref{sec:instability}. The distribution shows two prominent large clusters of the ES-CQRS architecture located at the corners of (0, 0) and (1, 0), while the remaining smaller points are scattered primarily in the lower half of the graph below the diagonal line. Generally, it can be seen that ES-CQRS has more packages than CRUD and is more abstract. The ES-CQRS architecture exhibits a slightly lower median Distance from the Main Sequence $D$ (\autoref{fig:distance-main-sequence-per-package}) with a value of around 0.4, opposed to around 0.5 for the CRUD architecture. However, both \glspl{iqr} for $D$ have a wide spread, with the ES-CQRS architecture spanning from 0.1 to almost~1.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/Instability_per_package.png}
        \caption{Instability per package. Descriptive statistics in \autoref{table:instability}.}
        \label{fig:instability}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/Abstractness_per_package.png}
        \caption{Abstractness per package. Descriptive statistics in \autoref{table:abstractness}.}
        \label{fig:abstractness}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/Distance_main_sequene_per_package.png}
        \caption{Distance from the main sequence per package. Descriptive statistics in \autoref{table:distance-from-the-main-sequence}.}
        \label{fig:distance-main-sequence-per-package}
    \end{subfigure}
    \par\medskip
    \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/static-analysis/Martin_main_sequence_graph.png}
        \caption{Visualization of the Main Sequence}
        \label{fig:main-sequence}
    \end{subfigure}
    \caption[Comparison of Instability $I$ and Abstractness $A$.]{Comparison of Instability $I$ and Abstractness $A$. Detailed results in \autoref{appendix:instability-abstractness}.}
    \label{fig:combined_instability_abstractness}
\end{figure}

\subsection{MOOD Metrics}
\label{sec:results-mood}

Results of the \acrfull{mood} suite, outlined in \autoref{sec:mood}, are presented in \autoref{fig:mood-results}. \gls{ahf} is the highest value for both architectures. Both architectures sit at around 95\%.

The CRUD architecture exhibits a \gls{mif} of around 16\%, higher than the ES-CQRS architecture at 1\%. With a value of 45\%, the ES-CQRS architecture's \gls{mhf} is higher than the 29\% for the CRUD architecture.

The CRUD architecture's \gls{cf} sits at around 12\%, while the ES-CQRS's architecture's \gls{cf} has a value of around 3\%.

The \gls{aif} of the CRUD architecture is at around 28\%, the ES-CQRS application reaches 8\%.

Generally, the CRUD Architecture covers a larger total surface area on the diagram, specifically showing higher values on the \gls{aif} and \gls{cf} axes compared to the ES-CQRS Architecture.

It is worth noting that the \gls{pf} of both architectures is not present in the diagram. This is due to the fact that the values exceed 100\%, with the CRUD architecture having a \gls{pf} of 360\%, and the ES-CQRS architecture having a \gls{pf} of 185\%. The \gls{pf} calculates the ratio of polymorphic situations to the maximum possible number of polymorphic situation, but the static analysis tool used does not take classes from libraries or external modules into consideration when computing the total possible number, which is why the values exceed 100\%.

TODO: What about CLF and RF?

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{images/static-analysis/MOOD_spider_diagram.png}
    \caption{MOOD metrics presented in a spider diagram. Results in \ref{table:mood}}
    \label{fig:mood-results}
\end{figure}

\subsection{Complexity Metrics (TODO)}
\label{sec:results-complexity}

TODO

\subsection{Chidamber Kemerer Metrics}

In \autoref{sec:ck-metrics}, the \acrlong{ck} suite was described. Its results are presented in \autoref{fig:ck-results} using a spider diagram. As the metrics are calculated on a per-class basis, the values were normalized and aggregated using a median for visualization purposes.

The CRUD architecture's plot (blue) forms the larger shape. It reaches the highest point on the \gls{cbo} axis, with a value around 0.1, and the \gls{dit} axis with a value around 0.13. It also shows a distinct outward point on the \gls{rfc} axis.

The ES-CQRS architecture's plot forms a smaller shape nested mostly inside the blue area. It shows lower values than the CRUD architecture on the \gls{cbo} (0.07), \gls{wmc} (0), and \gls{rfc} (< 0.05) axes. It sits close to the CRUD architecture on the \gls{lcom} and \gls{dit} axes.

Both architectures exhibit a median \gls{noc} of 0. TODO: also show average here. some values are kinda misleading when only using the median. Polymorphism exists in the CRUD app.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/static-analysis/Median-c-k-metrics.png}
    \caption{Median CK-Metrics presented in a spider diagram. Detailed results in \ref{appendix:ck-results}}
    \label{fig:ck-results}
\end{figure}
