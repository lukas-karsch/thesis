% !TeX root = ../main.tex
\chapter{Discussion}
\label{ch:discussion}

\section{Interpretation of Results}

TODO: explain this section. First, bottlenecks and anomalies are described, then interpret results per research question. Finally give "recommendations" --- when may which architecture better suited?

\subsection{Identifying Bottlenecks}
\label{sec:identifying-bottlenecks}

In general, the bottlenecks of either application lie in threadpool saturation or database contention. These two metrics are intrinsically linked, with database connection contention acting as a back-pressure mechanism that blocks threads from being freed up. Each request is processed in a thread ($tomcat\_threads$), and most requests make at least one database round-trip. When the database is heavily used ($hikari\_connections$ close to 10), each thread has to wait longer to get access to a connection. This in turn fills up the threadpool. As soon as the threadpool is saturated ($tomcat\_threads=200$), any further requests are queued before they can be processed. This can be seen in most load tests, as the two metrics often reach their ceiling at the same load (\hyperref[sec:l1]{L1}, \hyperref[sec:l2]{L2}, \hyperref[sec:l4]{L4}, \hyperref[sec:l5]{L5}, \hyperref[sec:l6]{L6}, \hyperref[sec:l8]{L8}). As soon as either of these resources is \emph{saturated}, meaning no headroom is left, the latencies typically increase sharply, due to the queueing which occurs before a request can be processed. Before that point, enough resources are available to concurrently handle most incoming requests.

A clear correlation exists between the Scalability Metric ($\psi$) and resource headroom. For example, in L1 and L2, $\psi$ for ES-CQRS approaches 0.01 as the threadpool reaches saturation (zero headroom), indicating that the system can no longer effectively convert additional resource expenditure into performance gains.

During enrollment (\hyperref[sec:l3]{L3}), the bottleneck is not visible immediately from the resource consumption metrics. The ES-CQRS application shows a sharp increase in latency at 100 \gls{rps}. At this point, neither the threadpool nor the database connections are saturated. The issue here most likely stems from the subscription query which is used in the implementation. It blocks until the read-side projector updated the student's enrollment status. The point at which the latencies start increasing sharply can be assumed to be the moment in which the projector can not handle incoming events quickly enough. While enough worker threads would be available, the existing requests need to wait for a longer time until the request finishes processing.

\subsection{Anomalies in Test Results}
\label{sec:anomalies}

\autoref{sec:l5} revealed inconsistencies in the collected resource consumption metrics for the CRUD application. Beyond 250 RPS, Prometheus could not detect any CPU usage, and the threadpool metrics exhibited a sharp decline in usage at 200 RPS and 250 RPS. As previously mentioned, these results were confirmed through repeated test executions, suggesting a consistent failure at this load level. A definitive root cause is difficult to pinpoint. One possible cause is resource contention within the shared infrastructure, as both the Spring Boot application and the PostgreSQL instance compete for CPU cycles on the same \gls{vm}. The computational intensity of the complex joins required for this endpoint may saturate the database, consequently starving the SpringBoot server of CPU time. Because a system call is necessary to determine the CPU usage of a process, metrics may not have been recorded accurately (or at all) due to high resource contention. However, this does not explain why the threadpool usage increases again at even higher loads. Ultimately, this shared-resource setup limits both performance of the applications and the effectiveness of the testing setup, making it difficult to identify the reason for such anomalies.

Furthermore, it is worth noting that in some cases, the metrics for data store size showed an unexpected confidence interval (CI) in the ES-CQRS application. Several read-only tests show a non-zero CI, which is surprising as the seed data is identical for each test and no further data is written to the system during load generation. This observation can be made in \gls{l}4, \gls{l}5, \gls{l}6, \gls{l}8. Identifying a reason for this effect would require further investigation.

\subsection{Projection Lags}
\label{sec:projection-lag}

Projection lag is the delay between the moment an event is published by the write-side and the moment that change is reflected in the read-side (projections). In a \gls{cqrs} architecture, the read-side is updated asynchronously by event processors (projectors). When the system is under high load, the event processor may become a bottleneck, when it is unable to keep up with the new events being published. This results in eventual consistency, where a user might successfully save data but receive an "old" or "not found" response when querying the read-side immediately afterward, as the background synchronization has not yet caught up.

Several tests showed a decrease in storage size for the ES-CQRS application under increased load, which was attributed to projection lag. The projection lag is a very likely explanation, however no metrics were taken during the load tests which can definitely confirm this as the root cause. Furthermore, even projection lag does not fully explain why the storage taken at higher load is \emph{lower} than at a previous load. One explanation for this could be that the higher load on the system takes up more system resources to handle the incoming requests, taking away CPU cycles from the event processor. However, this is only a theoretical conclusion that would require detailed profiling to confirm.

\subsection{Performance and Scalability}
\label{sec:interpretation-performance}

The results obtained during load testing reveal substantial tradeoffs between the two architectural patterns. The CRUD application shows better write performance in both throughput and resource efficiency, while the ES-CQRS application offers specialized advantages for complex read queries at the cost of higher overhead. In most write-heavy scenarios like \hyperref[sec:l1]{L1}, \hyperref[sec:l2]{L2}, and \hyperref[sec:l2]{L3}, the CRUD system maintains significantly lower latency and consumes fewer resources. In contrast, the ES-CQRS system frequently hits resource saturation --- reaching 200 threads and 10 database connections --- and violates latency thresholds as load increases. This divergence is especially visible in the enrollment test (L3), where the CRUD application manages much higher request volumes before performance degrades. Across nearly all tests, the ES-CQRS architecture requires more CPU and storage, often using double the disk space due to the overhead of storing both events and multiple denormalized projections.

These differences are reflected in the calculated scalability metric $\psi$. While the CRUD application often exhibits values above 1.0, indicating efficient scaling where throughput increases outpace cost, the ES-CQRS application frequently shows values near zero under high load (e.g., $\psi \approx 0.01$ in L1). This metric serves as a useful tool for evaluating "productivity" by balancing throughput against resource costs like CPU and thread saturation. However, its usability is sensitive to how the "cost" function is weighted. For instance, the inclusion of a high penalty for projection lag (weighted at 3.0 in the model) heavily penalizes the ES-CQRS application's scalability score when consistency thresholds are missed.

However, the ES-CQRS application demonstrates its strength in specific read-heavy scenarios such as retrieving student credits (L6), where it provides a significant speedup compared to the CRUD application. While the CRUD application struggles with complex aggregations at high loads, reaching a scalability of $\psi \approx 0.00$ as its threadpool saturates, ES-CQRS benefits from pre-calculated read models that keep latencies low even at 3000 \gls{rps}. A similar advantage can be observed in L5 (reading a list of all lectures), because \gls{cqrs} is specifically designed to optimize these types of data retrievals. In these read-intensive contexts, the ES-CQRS application maintains a higher scalability factor (e.g., $\psi \approx 1.1$ at 2000 RPS) because the low latency and lack of complex DB joins compensate for its higher idle resource usage.

A critical drawback of \acrlong{es} and \gls{cqrs} is the projection lag observed in the freshness test (L7), where the application fails to reflect new writes on the read side within the required 100ms window (\ref{slo-freshness}) under heavy load. This eventual consistency means that even if reads have low latencies, they are not guaranteed to be up-to-date. Represented by the $read\_visible\_rate$ metric, this \emph{freshness} drops as low as $0.02$ for ES-CQRS at 400 \gls{ips}. This highlights a tradeoff for systems that require immediate data freshness: the mechanism that enables fast and scalable reads in ES-CQRS can also lead to stale data being served when the system is under pressure.

It should be noted that the results of L8 --- the historic reconstruction load test presented in \autoref{sec:l8} --- yielded a rather surprising result in terms of active database connections. Despite only using the database for two indexed ID lookups before streaming events from Axon's Event Store, the ES-CQRS application saturated the connection pool with a median of 10 active connections. This behavior is counter-intuitive compared to the CRUD implementation, which consistently maintains fewer active connections even though it performs more complex data fetching directly from the relational database through Envers.

Beyond the specific resource metrics, the ES-CQRS implementation adds an inherent architectural overhead due to the abstraction layers provided by Axon Framework and Axon Server. While these tools offer benefits like location transparency and automated message routing, they introduce a more complex data flow compared to the direct function calls inside a standard Layered Architecture. Theoretically, the process of wrapping, routing, and handling command messages and events through the framework adds a latency penalty and increases CPU demand. However, as no profiling was conducted to isolate these specific costs during program execution, this point remains theoretical.

\subsection{Architectural Flexibility}
\label{sec:architectural-flexibility}

The static analysis results reveal distinct characteristics of the \gls{crud} and \gls{es}-\gls{cqrs} implementations. While metric suites like \gls{mood} and \acrlong{ck} provide a broad overview of code quality, the most insightful data for evaluating architectural flexibility results from the coupling and dependency metrics. These metrics provide a direct representation of how components interact and the extent to which a change in one area of the system propagates through others.

\subsubsection{Evaluation of Metric Utility}

The analysis indicates that transitive dependency metrics ($Dpt^*$ and $Dcy^*$), described in \autoref{sec:results-dependency}, are more useful than their direct counterparts ($Dpt$ and $Dcy$) for assessing the quality and coupling of an architecture. While the median values for direct dependencies are similar across both architectures, the \gls{es}-\gls{cqrs} approach shows a significantly narrower \gls{iqr} for transitive dependencies. In the \gls{crud} application, 50\% of classes have between 2 and 33 transitive dependents, whereas at least 75\% of \gls{es}-\gls{cqrs} classes have 7 or fewer. This suggests that the \gls{es}-\gls{cqrs} architecture more effectively isolates components, preventing a "rippling effect" of changes common in highly coupled architectures.

In contrast, the Distance from the Main Sequence ($D$), composed of Abstractness ($A$) and Instability ($I$), appears to be less accurate in this specific context. It is especially worth noting that a high Abstractness score does not inherently equate to a high level of \emph{functional abstraction}. For instance, the \gls{es}-\gls{cqrs} architecture utilizes many \gls{jpa} repositories on its Query side. These components are technically interfaces. This inflates the Abstractness score without necessarily having an architectural impact. Furthermore, these metrics are highly sensitive to the chosen package layout rather than the logic itself. A prime example is the \texttt{api} package in the \gls{es}-\gls{cqrs} application. It serves as a vital decoupling point between the Command and Query side, but because the classes are not abstract and highly depended upon, they appear "poor" according to traditional instability metrics despite their arguably high architectural value. Similarly, colocating Controller, Service and Repository classes in the same package in the CRUD application results in improved Instability values, even though the dependencies between classes are still present in the same way.

\subsubsection{Architectural Impact on Evolution and Scalability}

The structural differences between the two approaches have an impact regarding long-term flexibility and evolution. The \gls{crud} architecture displays higher \gls{cbo}, higher coupling ($C_a$, $C_e$) and more dependencies ($Dcy^*$, $Dpt^*$), and a larger "surface area" in the \gls{mood} metrics. This increased coupling implies that as the codebase grows, the complexity of making changes increases non-linearly, as each class is transitively linked to a larger portion of the system.

In contrast, the \gls{es}-\gls{cqrs} architecture demonstrates a structural advantage for scalability and evolution. The primary difference lies in the separation of concerns between writes and reads. By decoupling the Command and Query sides, the architecture maintains lower transitive coupling across the majority of the system.

Consequently, the \gls{es}-\gls{cqrs} approach likely provides a more seamless transition to horizontal scaling. Because the dependencies are already logically and physically partitioned (shown by the lower $Dcy^*$ and $PDcy$ values), the effort required to split the monolith into independent microservices is significantly reduced. This allows for specific parts of the system, such as high-traffic read models, to be scaled horizontally on separate infrastructure without requiring the entire application to be replicated. Additionally, Axon Framework provides location transparency, reducing the need to write additional boilerplate code when attempting to split the system into separate services. Therefore, while the \gls{es}-\gls{cqrs} architecture may have more outliers in package coupling, its fundamental structure provides the necessary isolation for the long-term scalability and independent evolution of system components.

It can be noted that the ES-CQRS pattern introduces unique challenges regarding developer experience and code clarity. The indirection introduced by Axon's message buses leads to transparency and decoupling, which is reflected by the dependency and coupling metrics. However, this makes the execution flow less explicit than the direct method calls found in a standard Layered Architecture. The application logic becomes more difficult to follow through static analysis alone. This fragmented data flow may increase the cognitive load on developers and make debugging harder as stack traces become less explicit.

\subsection{Traceability}

The findings presented in this section are primarily derived from a synthesis of existing literature regarding system architecture and data integrity previously presented in \autoref{sec:traceability-related-work}, alongside empirical performance data collected in L8 (\autoref{sec:l8}).

The comparison between \gls{crud} and \gls{es}-\gls{cqrs} systems reveals a trade-off between the reliability of historical data and the speed of accessing it. In traditional \gls{crud} systems, the audit log acts as a secondary observer. This architecture is susceptible to the "dual-write" problem, where a database update succeeds but the log entry fails. As noted by \textcite[155]{gantz_basics_2014}, the reliability of an audit depends on the system's ability to produce accurate evidence. If the audit trail diverges from the application state, it fails to meet the criteria for strict legal compliance. In contrast, Event-sourced systems address these accuracy concerns by adhering to the principle that the event log \emph{is} the truth \cite{helland_immutability_2015}. Because every state change is recorded as an immutable event, the system preserves the exact domain context and "user intent" \cite[531]{kleppmann_designing_2017}. This makes the reconstruction process deterministic and eliminates the risk of silent data divergence. Even if read-side projections become desynchronized, they can be corrected and rebuilt from the log. \textcite{kleppmann_designing_2017} suggests that this process additionally makes diagnosing unexpected behaviors significantly easier through event replay.

Furthermore, \gls{crud} logs often capture \emph{what} changed without preserving the \emph{why}. As \textcite[531]{kleppmann_designing_2017} argues, the application logic in \gls{crud} is transient, meaning business context is often lost over time. Capturing and reconstructing intent is possible in \gls{crud} systems. It was also implemented in this study, though it requires additional effort to attach the context to changes. In contrast, events in \gls{es} carry inherent metadata which contain intent.

However, the efficiency of this reconstruction remains a challenge when using \acrlong{es}. While \textcite{monagari_demystifying_2026} cites a dramatic reduction in incident resolution time for financial institutions using \gls{es}, this study's load testing results (L8) provide contrasting empirical results. The results show that \gls{crud} architectures are significantly faster in reconstructing historic state, at least for simple history queries. Reconstructing a student's grade history, for example, proved more efficient in the \gls{crud} model.

Ultimately, while \gls{es}-\gls{cqrs} provides the "superior evidence" required for forensic or legal auditing \cite[17]{maier_audit_2006}, the \gls{crud} approach remains more performant for frequent, high-volume historical lookups. As historical state lookups are likely uncommon in typical applications, the choice of architecture should be guided by whether the priority lies in the speed of retrieval or the absolute integrity of the audit trail.

It should also be mentioned that frequent lookups of historic state are likely uncommon in typical applications. If a high volume of time-travel queries was a real use-case in an application, the architects may want to adopt an entirely different approach in storing current and historic state, such as time-series databases.

\section{Limitations of the Study}
\label{sec:limitations-of-the-study}

This research provides an empirical, quantifiable comparison between \gls{crud} and \gls{es}-\gls{cqrs} architectures. However, several technical and methodological limitations must be acknowledged when interpreting the results. One of these limitations is the infrastructure environment used for testing. Because both the load generation and the server-side components were hosted on \glspl{vm} running on the same physical machine, the observed network latency does not accurately reflect a distributed production environment. In a real-world scenario, services typically communicate across a physical network rather than on a single host. Particularly the ES-CQRS architecture involves a more complex chain of communication, including the command and query bus, aggregate handling, projections and event storage. In a distributed environment, the cumulative network round trips required for these steps would likely result in higher latencies than those recorded in this study's test setup.

Except for Axon Server's storage size, the server-side metrics were only collected from the SpringBoot application. Because the \gls{vm} also hosted PostgreSQL and Axon Server, the collected CPU metric ($process\_cpu\_usage$) does not capture the total system load. It is likely that the overall system CPU was fully saturated even when the server's CPU usage suggested remaining overhead. This is a threat to the validity of the performance data.

The accuracy of the benchmarking results was further affected by the testing configuration. In some load tests, a significant number of iterations was dropped at high RPS because the maximum number of \glspl{VU} in the k6 configuration was insufficient for the load. This means the results do not reflect the real performance under the respective load, but instead the load of $RPS - dropped\_iterations\_rate$.

Additionally, a more granular performance analysis would require measuring garbage collection pauses, memory allocation rates, database time, and other system metrics. Furthermore, without profiling, the "root causes" of specific bottlenecks remain theoretical, as the collected data shows the \emph{results} of system stress rather than a detailed breakdown of internal execution delays. The performance analysis is also limited because the tests only varied the number of requests per second. Other factors, such as the size of the data being sent in POST requests or the volume of data fetched in queries, remained constant per test and were not evaluated.

Next, the server-side metrics collected through Actuator and Prometheus may have a reduced accuracy under very high load because the metric collection in itself is bound to system resources and may start slowing down under high load. Also, the actuator endpoints themselves experience the same queueing delay as other requests.

Furthermore, although both applications were developed following industry standards and common best practices, it cannot be formally guaranteed that the implementations are entirely free of defects or suboptimal coding practices. Consequently, the observed performance metrics may be influenced by unidentified implementation errors rather than being representative of the underlying architecture.

One such implementation error which likely influenced the load testing measurements is a specific oversight in the ES-CQRS implementation regarding resource contention. In the current setup, both the synchronous lookup projectors --- which belong solely to the write-side and are used to ensure immediate consistency --- and the asynchronous query-side projections share the same underlying database instance and connection pool. This creates a bottleneck where the lookup projectors, which should ideally be very low-latency, must compete for database connections and \gls{io} cycles with the query-side event processors. This shared resource dependency undermines the theoretical decoupling of the command and query sides, as heavy query-side projection activity directly introduces latency into the command-handling pipeline.

Beyond the technical measurements, the given architectural evaluation is limited as it is simply a synthesis of static analysis. This study did not investigate the topic of \gls{schema-evolution}, which is the process of managing how data structures, such as events or database tables, change over time. In a long-running production system, the difficulty of evolving an event store's schema is an additional factor in the total cost of ownership and flexibility of an ES-CQRS system. Similarly, managing complex database migrations in coupled \gls{crud} architectures without data-loss may introduce additional challenges.

Because the use case and the load patterns used in this study were artificially generated, they may not capture the unpredictable nature of real-world user behavior or specific business requirements.

While a load test was developed and executed for a simple history query (L8), the potential use-cases for time-travel queries are far wider. More complex historical queries involving several entities would have been a different challenge, and could have shown different results for the two architectures, both in implementation complexity and latencies.

Finally, the emphasis placed on certain architectural benefits, such as traceability or scalability, is inherently subjective. Different organizations or developers might prioritize \glspl{slo} or compliance requirements differently, which would alter the perceived value of one architecture over the other.

\section{Answering the Research Questions}

This section will provide a conclusive answer to the three sub-research questions, before providing a holistic answer to the main \glslink{rq}{research question}.

\subsection{RQ 1: Performance and Scalability}
\label{sec:answering-rq-1}

\begin{quote}
    \textit{How do CRUD and ES-CQRS implementations perform under increasing load, and what are the resulting implications for system scalability and resource efficiency?}
\end{quote}

The CRUD architecture generally outperforms ES-CQRS during writes, maintaining lower latency and higher resource efficiency. The CRUD system has minimal architectural overhead, which could be one reason for this observation. Further, while writes in ES-CQRS \emph{technically} run on an append-only event stream, which should be very fast in theory, realistic business requirements, such as the ones chosen for this study, still require synchronous writing to lookup tables, resulting in longer blocking times during command handling. While CRUD systems scale efficiently until reaching threadpool or database connection limits, ES-CQRS frequently hits these saturation points sooner.

However, ES-CQRS demonstrates improved scalability for intensive read operations, as its pre-calculated, denormalized projections allow it to handle high request volumes that cause the CRUD implementation to fail. Generally, it can be observed that the more intensive a read-scenario is, the higher are the performance benefits of the ES-CQRS implementation.

In summary, ES-CQRS offers a specialized performance trade-off: it provides high-speed data retrieval at the cost of significantly higher CPU, storage, and a risk of stale data due to projection lag. It should be noted that the business requirement of immediately consistent writes most likely substantially decreased the ES-CQRS implementation's performance, as the synchronous lookup tables added substantial overhead.

In conclusion, this study demonstrates that architectural choices dictates a system's performance and scalability limits. CRUD remains resource efficient and performant under write-heavy scenarios. Conversely, ES-CQRS effectively "pre-pays" for performance through higher storage taken up by projections, enabling a substantially higher scalability ceiling for intensive read operations. These findings suggest that the value of ES-CQRS is not universal, but rather highly dependent on a domain's specific ratio of reads to writes and its tolerance for eventual consistency.

\subsection{RQ 2: Architectural Complexity and Flexibility}
\label{sec:answering-rq-2}

\begin{quote}
    \textit{What are the fundamental structural differences between the two approaches, and how do these impact the long-term flexibility and evolution of the codebase?}
\end{quote}

The fundamental structural difference between the two approaches lies in the degree of component isolation, with the ES-CQRS architecture maintaining lower transitive coupling than the CRUD model. The CRUD application suffers from high dependency counts, which could cause "ripple effects" where changes propagate across the system. This confirms the concerns raised by \textcite{singh_object_2018} regarding \gls{cbo} as a primary detractor of quality, as the CRUD model's higher coupling creates a non-linear increase in change complexity. In contrast, ES-CQRS effectively separates read and write concerns, leading to a reduced dependency count and high separation of concerns. This structural decoupling suggests enhanced long-term flexibility by allowing for independent evolution of features and modules, and a more seamless transition towards horizontal scaling or microservices. Conversely, the CRUD application's higher coupling suggests that as the codebase grows, the complexity of making changes will increase non-linearly. Ultimately, the architectural isolation in ES-CQRS provides a more robust foundation for system scalability and maintenance.

Schema evolution is a topic that was not evaluated in the thesis, but it plays a substantial role in how a system can evolve over time. Literature, such as \cite{overeem_empirical_2021}, suggest that schema evolution is challenging, particularly in ES-CQRS systems.

\subsection{RQ 3: Historical Traceability}
\label{sec:answering-rq-3}

\begin{quote}
    \textit{To what extent can CRUD and ES-CQRS systems accurately and efficiently reconstruct historical states to satisfy business intent and compliance requirements?}
\end{quote}

CRUD systems offer higher performance for high-volume historical lookups. However, they are susceptible to the "dual-write" problem, which can lead to a divergence between the application state and the audit trail. This divergence undermines the reliability of the system's "evidence," which \textcite[155]{gantz_basics_2014} identifies as the critical factor for successful IT auditing and compliance.

In contrast, \acrlong{es} guarantees data integrity by treating the immutable event log as the singular "truth" of the system~\cite{helland_immutability_2015}, capturing both system state and business intent. While this event-driven approach facilitates deterministic reconstruction for forensic auditing, empirical results from this study (\gls{l}8) indicate significantly lower performance in historic reconstruction compared to traditional audit logs stored in SQL tables.

Ultimately, the CRUD application provides higher efficiency for simple historic state retrieval, whereas ES-CQRS offers the "superior evidence"~\cite[17]{maier_audit_2006} required for rigorous legal requirements. Furthermore, as suggested by \cites[531]{kleppmann_designing_2017}{monagari_demystifying_2026}, the event log simplifies bug reproduction and incident diagnosis by allowing developers to replay exact system states. Therefore, the choice between these architectures requires developers to make a trade-off between the required speed of access and the guaranteed accuracy of the historical record.

\subsection{Conclusion}

\begin{quote}
    \textit{How does an Event Sourcing architecture compare to CRUD systems with an independent audit log regarding performance, scalability, flexibility and traceability?}
\end{quote}

While the CRUD implementation demonstrates superior write performance and lower CPU and storage overhead, it suffers from higher transitive coupling that may hinder the system's architectural evolution. In contrast, the combination of \gls{es} and \gls{cqrs} provides improved latencies for read-heavy scenarios and appears to be more flexible in terms of architectural evolution. However, it introduces additional challenges such as eventual consistency, added infrastructure complexity and increased storage requirements.

Regarding traceability, the CRUD application offers faster retrieval of historical states but remains vulnerable to data divergence, whereas \acrlong{es} ensures data integrity by treating the event log as the immutable source of truth.

Ultimately, the choice between these architectures should be driven by the specific domain requirements: CRUD is better suited for write-heavy applications with simple data relations, while ES-CQRS excels in complex, read-intensive environments, and domains requiring valuing high traceability. Implementing \acrlong{es} and \gls{cqrs} can therefore be seen as a strategic investment in flexibility and data reliability at the potential expense of raw write throughput.

\section{Optimizations and Further Work}
\label{sec:optimizations-and-further-work}

This section outlines potential technical refinements for the existing implementations and identifies opportunities for future work. While the current results establish a baseline for both architectures, several optimization strategies could be applied to reduce the performance bottlenecks in both applications.

Firstly, as described in \autoref{sec:limitations-of-the-study}, several issues can be identified in the testing setup. To collect more meaningful results during load testing, these should be eliminated.

Next, the performance of both applications could be optimized by investigating their source code and accurately identifying bottlenecks through profiling. As previously mentioned in \autoref{sec:limitations-of-the-study}, several implementation details likely influenced measurements. Improving on these aspects would be the first step towards an increased performance.

The underlying infrastructure and framework configurations offer room for improvement. While no tuning was done for this thesis on purpose to make "out-of-the-box" performance comparable, \autoref{tab:system-tuning} summarizes parameters that could be tuned to resolve the resource saturation and latency bottlenecks observed in the ES-CQRS and CRUD applications. Before doing any tuning, further profiling would be necessary to identify bottlenecks in function calls or queries.

\begin{table}[h!]
    \centering
    \small
    \begin{tabularx}{\linewidth}{lXX}
        \toprule
        \textbf{Keyword}          & \textbf{Action}                                                                                                                                                                    & \textbf{Goal}                                                                                               \\ \midrule
        \textbf{Pool Tuning}      & Adjust Tomcat thread counts and database connection limits.                                                                                                                        & Reduce queueing delays.                                                                                     \\ \addlinespace
        \textbf{Event Store}      & Tune Axon page sizes and set different snapshot thresholds.                                                                                                                        & Reduce the number of expensive \gls{io} operations when reading long event streams.                         \\ \addlinespace
        \textbf{Reactivity}       & Use reactive programming paradigms by implementing Spring WebFlux\footnotemark[1] and reactive JPA repositories\footnotemark[2].                                                   & Free up worker threads while waiting for database or network responses.                                     \\ \addlinespace
        \textbf{Query Refinement} & Replace auto-generated Hibernate queries with custom JPQL queries, optimize further using \texttt{@BatchSize} and EAGER fetching options.                                          & Eliminate inefficient queries and reduce the total number of database round-trips to minimize database time \\ \addlinespace
        \textbf{Serialization}    & Tune Jackson's serialization options                                                                                                                                               & Minimize CPU overhead.                                                                                      \\ \addlinespace
        \textbf{Projections}      & Transition from JSON-based projections to flat, denormalized SQL-native tables, or attempt to replace JSON by binary formats like Protobuf\footnotemark[3] or Kryo\footnotemark[4] & Eliminate serialization overhead, reduce projection storage size and enable high-performance JDBC mapping.  \\
        \bottomrule
    \end{tabularx}
    \caption{Proposed System Tuning and Optimization Strategies}
    \label{tab:system-tuning}
\end{table}

\footnotetext[1]{\url{https://docs.spring.io/spring-framework/reference/web/webflux.html}}
\footnotetext[2]{\url{https://docs.spring.io/spring-data/jpa/reference/data-commons/api/java/org/springframework/data/repository/reactive/ReactiveCrudRepository.html}}
\footnotetext[3]{\url{https://protobuf.dev/}}
\footnotetext[4]{\url{https://github.com/EsotericSoftware/kryo}}

Additional performance gains could be achieved through the implementation of caching. Introducing a cache for frequently accessed student data could provide the CRUD application with read speeds comparable to \gls{cqrs} without the structural complexity of \acrlong{es}, though this introduces its own challenges regarding cache invalidation.

The need for synchronous lookup tables to enable consistent writes in ES-CQRS likely added substantial overhead, drastically reducing write performance. This need emerged from the non-functional requirements defined for this study. However, in high-traffic environments where immediate consistency during writes may not be valued as highly, the system could also implement eventual consistency on the write-side, avoiding the overhead of global set-based validation. Instead, the read-side could be queried as a heuristic before issuing a Command, and process managers (Sagas) could be applied to later correct any illegal states. This would likely increase write performance, while taking the risk of temporary inconsistencies. Therefore, future research should explore how different domain requirements influence technical performance, specifically evaluating how the prioritization of immediate versus eventual consistency changes the scalability and resource efficiency of each architectural style.

Furthermore, as this study evaluated both applications on a single machine, the assessment of scalability and architectural flexibility remains primarily theoretical. To provide empirical evidence for these claims, a follow-up study would be appropriate, which physically transitions the systems into microservices and performs horizontal scaling. By implementing these structural changes, the practical performance limits and the true decoupling of the ES-CQRS pattern could be validated further. Additionally, the effort required to transform the monolithic structure into microservices would serve as a practical benchmark to validate the accuracy of the static analysis results.

Another point that could be evaluated in future work is schema evolution. This topic was only briefly mentioned in this study, but an empirical comparison of schema evolution methods in both CRUD and ES-CQRS systems could be worth researching.
