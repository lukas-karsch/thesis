%LTeX: enabled=false
\documentclass[12pt,a4paper]{article}
\usepackage{array}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[automake]{glossaries}
\usepackage{xcolor}
\usepackage[export]{adjustbox}

% TODO
% microtype is supposed to help with text breaking (overflow), but it gives an error:  pdfTeX error (font expansion): auto expansion is only possible with scalable fonts. <argument> ...shipout:D \box_use:N \l_shipout_box
% \usepackage{microtype}

\usepackage{color}

\definecolor{pblue}{rgb}{0.13,0.13,1}
\definecolor{pgreen}{rgb}{0,0.5,0}
\definecolor{pred}{rgb}{0.9,0,0}
\definecolor{pgrey}{rgb}{0.46,0.45,0.48}
\definecolor{porange}{rgb}{1,0.5,0.1}

\usepackage{listings}
\lstset{language=Java,
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  columns=flexible,
  commentstyle=\color{pgrey},
  keywordstyle=\color{porange},
  stringstyle=\color{pgreen},
  basicstyle={\ttfamily\small},
  moredelim=[il][\textcolor{pgrey}]{\$\$},
  moredelim=[is][\textcolor{pgrey}]{\%\%}{\%\%}
}

\usepackage[
  backend=biber,
  style=authoryear, % or numeric, ieee, apa, ...
]{biblatex}
\DefineBibliographyStrings{english}{
  and = {\&}
}

\usepackage{hyperref} % must be loaded last! 

% Adapted from Source - https://tex.stackexchange.com/a
% Posted by Don Hosek
% Retrieved 2026-01-08, License - CC BY-SA 4.0
\ExplSyntaxOn
\NewDocumentCommand{\javaname}{ m }
  {
    \group_begin:
    \tl_set:Nn \l_tmpa_tl { #1 }
    % Replace dots with dot + allowbreak
    \tl_replace_all:Nnn \l_tmpa_tl { . } { .\allowbreak }
    % Replace dashes with dash + allowbreak
    \tl_replace_all:Nnn \l_tmpa_tl { - } { -\allowbreak }
    
    \texttt{ \tl_use:N \l_tmpa_tl }
    \group_end:
  }
\ExplSyntaxOff

\addbibresource{references.bib}

\makeglossaries

%LTeX: enabled=true
\newglossaryentry{api}
{
    name=API, 
    description={API stands for \emph{Application Programming Interface}. It describes the public interface of a module or service, often exposed over a network}
}

\newglossaryentry{rest}
{
    name=REST,
    description={REST stands for \emph{Representational State Transfer}. It is an architectural style for distributed hypermedia systems}
}

\newglossaryentry{http}
{
    name=HTTP,
    description={HTTP stands for \emph{Hypertext Transfer Protocol}. It is a protocol used in internet communication and was defined in RFC 2616 \parencite{rfc2616}}
}

\newglossaryentry{adm}
{
    name={Anemic Domain Model},
    description={The objects describing the domain only hold data, no logic}
}

\newglossaryentry{rdm}
{
    name={Rich Domain Model},
    description={Objects incorporate both data and the behavior or rules that govern that data}
}

\newglossaryentry{atomicity}
{
    name=Atomicity,
    description={Atomicity means that an action is either fully executed or not at all. Atomic operations make sure the application is not left in an invalid state \parencite[10]{bernstein_principles_2009}}
}

\newglossaryentry{cap}
{
    name={CAP Theorem},
    description={The CAP theorem by Eric Brewer states that a distributed data store can only display at most two of the following three guarantees at the same time: consistency, availability and partition tolerance \parencite{gilbert_brewers_2002}}
}

\newglossaryentry{contract-test}
{
    name={Contract Test},
    text={contract test},
    plural={contract tests},
    description={A contract test verifies that services implement a shared interface by testing their interactions against an explicitly defined contract}
}

\newglossaryentry{groovy}
{
    name=Groovy,
    description={A dynamic JVM language that extends Java with concise syntax and powerful features such as closures, making it well suited for scripting, DSLs, and test code \parencite{groovy-homepage}}
}

\newglossaryentry{gpath}
{
    name={GPath Expression},
    text={GPath expression},
    plural={GPath expressions},
    description={A Groovy-based path language for navigating and querying nested object graphs (such as JSON or XML) using concise, expressive selectors and closures  \parencite{groovy-gpath}}
}

\newglossaryentry{restassured}
{
    name={REST Assured},
    description={A library for testing \acrshort{http} servers}
}

\newglossaryentry{dockerfile}
{
    name=Dockerfile,
    description={A text document containing a series of instructions used to assemble a Docker image}
}

\newglossaryentry{docker}
{
    name=Docker,
    description={Open platform for developing, shipping and running containerized applications}
}

\newglossaryentry{testcontainer}
{
    name=Testcontainer,
    description={Testcontainers are a way to declare infrastructure dependencies as code using \gls{docker} \parencite{testcontainers-homepage}}
}

\newacronym{www}{WWW}{World Wide Web}

\newacronym{hateoas}{HATEOAS}{Hypermedia as the engine of application state}

\newacronym{html}{HTML}{HyperText Markup Language}

\newacronym{json}{JSON}{JavaScript Object Notation}

\newacronym{xml}{XML}{Extensible Markup Language}

\newacronym{dao}{DAO}{Data Access Object}

\newacronym{ddd}{DDD}{Domain Driven Design}

\newacronym[plural=URIs]{uri}{URI}{Uniform Resource Identifier}

\newacronym{crud}{CRUD}{Create Read Update Delete}

\newacronym{dto}{DTO}{Data Transfer Object}

\newacronym{acid}{ACID}{Atomicity, Consistency, Isolation, Durability}

\newacronym{cqrs}{CQRS}{Command Query Responsibility Segregation}

\newacronym{cqs}{CQS}{Command And Query Separation}

\newacronym{base}{BASE}{Basically Available, Soft State, Eventual Consistency}

\newacronym{es}{ES}{Event Sourcing}

\newacronym{dsl}{DSL}{Data Specific Language}

\newacronym{jpa}{JPA}{Jakarta Persistence API}

\newacronym{orm}{ORM}{Object-relational Mapper}

\newacronym{jpql}{JPQL}{Java Persistence Query Language}

\newacronym{pojo}{POJO}{Plain Old Java Object}

\newacronym{jmx}{JMX}{Java Management Extensions}

\begin{document}

\title{How does an Event Sourcing architecture compare to CRUD systems with an independent audit log, when it comes to scalability, performance and traceability?}
\author{Lukas Karsch}

\begin{titlepage}
    \centering

    \includegraphics[width=0.3\textwidth]{images/HdM_Logo.svg.png}
    \vspace{1cm}

    {\large Bachelor's Thesis in Computer Science and Media}

    \vspace{1.5cm}

    % Title
    {\LARGE\bfseries How does an Event Sourcing architecture compare to CRUD systems with an independent audit log, when it comes to scalability, performance and traceability?}

    \vspace{0.5cm}
    \rule{\linewidth}{0.5pt}
    \vspace{0.5cm}

    {\large\bfseries Lukas Karsch}

    \vspace{0.3cm}

    45259

    \vspace{0.8cm}

    %LTeX: language=de-DE
    {\bfseries Hochschule der Medien Stuttgart}
    %LTeX: language=en-US

    \vspace{0.8cm}

    Submitted on 2026/03/02

    \vspace{0.3cm}

    to obtain the degree of Bachelor of Science

    \vfill

    \begin{flushleft}
        \begin{tabular}{ll}
            \textbf{Main Supervisor:}      & Prof. Dr. Tobias Jordine \\[0.3cm]
            \textbf{Secondary Supervisor:} & Felix Messner            \\[0.3cm]
        \end{tabular}
    \end{flushleft}

\end{titlepage}

\newpage

\pagenumbering{roman}
%LTeX: language=de-DE
\section*{Ehrenwörtliche Erklärung}
%LTeX: language=en-US

\newpage

\tableofcontents

\newpage

\listoffigures

\newpage

\pagenumbering{arabic}

\section{Introduction}

\subsection{Motivation}

\subsection{Research question(s)}

\subsection{Goals and non goals}

\subsection{Structure of the paper}

\section{Basics}

\subsection{WWW, Web APIs, REST}

The \acrfull{www} is a connected information network used to exchange data. Resources are can be accessed via \glspl{uri} which are transferred using formats like JSON or HTML via protocols like \gls{http}. \gls{http} is a stateless protocol based on a request-response structure. It supports standardized request types, such as \texttt{GET} and \texttt{POST}, which convey a semantic meaning \parencite{jacobs_architecture_2004}.

Web APIs are interfaces that enable applications to communicate. They use \gls{http} as a network-based API \parencite[138]{fielding_architectural_2000}. Modern APIs typically follow \gls{rest} principles. REST stands for "Representational State Transfer" and describes an architectural style for distributed hypermedia systems \parencite[76]{fielding_architectural_2000}.

REST APIs adhere to principles derived from a set of constraints imposed by the \gls{http} protocol, for example. One such constraint is "stateless communication": Communication between clients and the server must be \emph{stateless}, meaning the client must provide all the necessary information for the server to fully understand the request.

Furthermore, every resource in REST applications must be addressable via a unique ID, which can then be used to derive a \acrshort{uri} to access the resource. Below are some examples for resources and \glspl{uri} which could be derived from them:

\begin{itemize}
    \item Book; ID=1; URI=\texttt{http://example.com/books/1}
    \item Book; ID=2; URI=\texttt{http://example.com/books/2}
    \item Author; ID=100; URI=\texttt{http://example.com/authors/100}
\end{itemize}

The "\acrfull{hateoas}" principle states that resources should be linked to each other. Clients should be able to control the application by following a series of links provided by the server \parencite{tilkov_brief_2007}.

Every resource must support the same interface, usually HTTP methods (GET, POST, PUT, etc.) where operations on the resource correspond to one method of the interface. For example, a POST operation on a customer might map to the \texttt{createCustomer()} operation on a service.

Resources are decoupled from their representations. Clients can request different representations of a resource, depending on their needs \parencite{tilkov_brief_2007}: a web browser might request \acrshort{html}, while another server or application might request \acrshort{xml} or \acrshort{json}.

%s TODO explain CRUD here or somewhere else? 

\subsection{Layered Architecture Foundations}
\label{sec:layered}

Layered Architecture is the most common architecture pattern in enterprise applications. Applications following a layered architecture are divided into \emph{horizontal layers}, with each layer performing a specific role. A standard implementation consists of the following layers:

\begin{itemize}
    \item Presentation: Handles requests and displays data in a user interface or by turning it into representations (e.g. JSON)
    \item Business: Encapsulates business logic
    \item Persistence: Persists data by interacting with the underlying persistence technologies (e.g. SQL databases)
    \item Database
\end{itemize}

A key concept in this design is layers of isolation, where layers are "closed", meaning a request must pass through the layer directly below it to reach the next, ensuring that changes in one layer do not affect others.

In a layered application, data flows downwards during request handling and upwards during the response: a request arrives in the presentation layer, which delegates to the business layer. The business layer fetches data from the persistence layer which holds logic to retrieve data, e.g. by encapsulating SQL statements.

The database responds with raw data, which is turned into a \acrlong{dao} (\acrshort{dao}) by the persistence layer. The business layer uses this data to execute rules and make decisions. The result will be returned to the presentation layer which can then wrap the response and return it to the caller. \parencite{richards_software_2015}

The data in layered applications is often times modeled in an \emph{anemic} way. In an \gls{adm}, business entities are treated as only data. They are objects which contain no business logic, only getters and setters. Business logic is entirely contained in the business (or "service") layer. \textcite{anemic-fowler-2003} describes this as an object-oriented \emph{antipattern}. % TODO explain why (this is procedural style, not actually object oriented)

% TODO figure

\subsection{Domain Driven Design}
\label{sec:ddd}

\acrfull{ddd} is a different architectural approach for applications.  It differs from layered architecture primarily in the way the domain is modelled and the responsibilities of application services.

The core idea of \acrshort{ddd} is that the primary focus of a software project should not be the underlying technologies, but the domain. The domain is the topic with which a software concerns itself. The software design should be based on a model that closely matches the domain and reflects a deep understanding of business requirements. \parencite[8, 12]{evans_domain-driven_2004}

This domain model is built from a \emph{ubiquitous language} which is a language shared between domain experts and software experts. This ubiquitous language is built directly from the real domain and must be used in all communications regarding the software. \parencite[24-26]{evans_domain-driven_2004}

%s TODO here, talk about model driven design -> way from the language to the code 

The software must always reflect the way that the domain is talked about. Changes to the domain and the ubiquitous language must result in an immediate change to the domain model.

When modeling the domain model, the aim should not be to create a perfect replica of the real world. While it should carefully be chosen, the domain model artificial and forms a selective abstraction which should be chosen for its utility. \parencite[12, 13]{evans_domain-driven_2004}

While \hyperref[sec:layered]{Layered Architecture} organizes code into technical tiers and is typically built on \glspl{adm}, often resulting in the \emph{big ball of mud} antipattern \parencite[V]{richards_software_2015}, \acrshort{ddd} demands a \gls{rdm} where objects incorporate both data and the behavior or rules that govern that data. The code is structured semantically into bounded context and modules which are chosen to tell the "story" of a system rather than its technicalities. \parencite[80]{evans_domain-driven_2004}

Entities (also known as reference objects) are domain elements fundamentally defined by a thread of continuity and identity rather than their specific attributes. Entities must be distinguishable from other entities, even if they share the same characteristics. To ensure consistency and identity, a unique identifier is assigned to entities. This identifier is immutable throughout the object's life. \parencite[65-69]{evans_domain-driven_2004}

Value Objects are elements that describe the nature or state of something and have no conceptual identity of their own. They are interesting only for their characteristics. While two entities with the same characteristics are considered as different from each other, the system does not care about "identity" of a value object, since only its characteristics are relevant. Value objects should be used to encapsulate concepts, such as using an "Address" object instead of distinct "Street" and "City" attributes. Value objects should be immutable. They are never modified, instead they are replaced entirely when a new value is required. \parencite[70-72]{evans_domain-driven_2004}

Using a \gls{rdm} does not mean that there should be no layers, the opposite is the case. \textcite{evans_domain-driven_2004} advocates for using layers in domain driven designs. He proposes the following layers: \parencite[53]{evans_domain-driven_2004}

\begin{itemize}
    \item Presentation: Presents information and handles commands
    \item Application Layer: Coordinates app activity. Does not hold business logic, but delegate tasks and hold information about their progress
    \item Domain Layer: Holds information about the domain. Stateful objects (rich domain model) that hold business logic and rules
    \item Infrastructure layer: Supports other layers. Handles concerns like communication and persistence
\end{itemize}

\textcite[75]{evans_domain-driven_2004} points out that in some cases, operations in the domain can not be mapped to one object. For example, transferring money does conceptually not belong to one bank account. In those cases, where operations are important domain concepts, domain services can be introduced as part of model-driven design. To keep the domain model rich and not fall back into procedural style programming like with an \gls{adm}, it is important to use services only when necessary. Services are not allowed to strip the entities and value objects in the domain of behavior. According to Evans, a good domain service has the following characteristics:

\begin{itemize}
    \item The operation relates to a domain concept which would be misplaced on an entity or a value object
    \item The operation performed refers to other objects in the domain
    \item The operation is stateless
\end{itemize}

\subsection{CRUD architecture}
\label{sec:crud-architecture}

% TODO refine 

\hyperref[sec:layered]{Layered architectures} are the standard for data-oriented enterprise applications. These applications mostly follow a \acrshort{crud} architecture. \acrshort{crud} is an acronym coined by \textcite{martin_managing_1983} that stands for "Create, Read, Update, Delete". These four actions can be applied to any record of data.

The state of domain objects in a \acrshort{crud} architecture is often mapped to normalized tables on a relational database, though other storage mechanisms maybe used. The application acts on the current state of the data, with all actions (reads and writes) acting on the same data. % TODO cite 

\acrshort{acid} (\acrlong{acid}) are an important feature of \acrshort{crud} applications. They can be guaranteed using transactions, ensuring that data stays consistent and operations are \glslink{atomicity}{atomic}. \parencite[10,11]{bernstein_principles_2009} % TODO more explanation: what is a transaction, commit, rollback, etc 

Databases in CRUD systems are typically normalized. Normalization is a process of organizing data into separate tables, removing redundancies and creating relationships through "foreign keys". It is the best practice for relational databases. There are several normal forms that can be achieved, each form building on the previous one: to achieve the second normal form, the first normal form has to be achieved first. \parencite[203]{martin_managing_1983}

\begin{itemize}
    \item 1NF (First Normal Form): Each table cell contains a single (atomic) value, every record is unique
    \item 2NF (Second Normal Form): Remove partial dependencies by requiring that all \emph{non-key} columns are fully dependent on the primary key
    \item 3NF (Third Normal Form): Removes transitive dependencies by requiring that non-key columns depend \emph{only} on the primary key
    \item Further Normal Forms (4NF, 5NF): Require a table can not be broken down into smaller tables without losing data
\end{itemize}

\subsection{CQRS Architecture}
\label{sec:cqrs}

\acrfull{cqrs} is an architectural pattern based on the fundamental idea that the models used to update information should be separate from the models used to read information. This approach originated as an extension of Bertrand Meyer’s \acrfull{cqs} principle, which states that a method should either perform an action (a command) or return data (a query), but never both. \parencite[148]{meyer_standard_2006}

\acrshort{cqrs} is different from \acrshort{cqs} in the fact that in \acrshort{cqrs}, objects are split into two objects, one containing commands, one containing queries. \parencite[17]{young_cqrs_2010}

\acrshort{cqrs} applications are typically structured by splitting the application into two paths:

\begin{itemize}
    \item Command Side: Deals with data changes and captures user intent. Commands tell the system what needs to be done rather than overwriting previous state. Commands are validated by the system before execution and can be rejected. \parencite[11,12]{young_cqrs_2010}
    \item Read Side: Strictly for reading data. The read side is not allowed to modify anything in the primary data store. The read side typically stores \glspl{dto} in its own data store that can directly be returned to the presentation layer. \parencite[20]{young_cqrs_2010}
\end{itemize}

In a CQRS architecture, the read side typically updates its data asynchronously by consuming notifications or events generated by the write side. Because the models for updating and reading information are strictly separated, a synchronization mechanism is required to ensure the read store eventually reflects the changes made by commands. This usually leads to stale data on the read side.

Each read service independently updates its model by consuming notifications or events published by the write side, allowing the read model to store optimized, denormalized views on the data. \parencite[23]{young_cqrs_2010}

%s TODO figure 

\subsection{(Eventual) Consistency}

% TODO have to talk about "read your writes"? 
% TODO maybe move above CRUD and CQRS 

\textcite{gray_dangers_1996} explain that large-scale systems become unstable if they are held consistent at all times according to \acrshort{acid} principles. This is mostly due to the large amount of communication necessary to handle atomic transactions in distributed systems. To address these issues, modern distributed systems often adopt the \acrshort{base} (\acrlong{base}) model which explicitly trades off isolation and strong consistency for availability. Eventually consistent systems are allowed to exist in a so-called "soft state" which eventually converges through the use of synchronization mechanisms over time rather than being strongly consistent at all times. \parencite{braun_tackling_2021, vogels_eventually_2009} This creates an inconsistency window in which data is not consistent across the system. During this window, stale data may be read. \parencite{vogels_eventually_2009}

\subsection{Event Sourcing and event-driven architectures}
\label{sec:event-sourcing}

Event driven architecture is a design paradigm where systems communicate via the production and consumption of events. Events are records of changes in the system's domain. \parencite{michelson_event-driven_2006} This approach allows for a high degree of loose coupling, as the system publishing an event does not need to know about the recipient(s) or how they will react. These architectures offer excellent horizontal scalability and resilience, as individual system components can fail or be updated without bringing down the entire network. \parencite{fowler_event_2005}

Event Sourcing is an architectural pattern within the landscape of event driven architectures. Event-sourced systems ensure that \emph{all} changes to a system's state are captured and stored as an ordered sequence of domain events. \parencite{fowler_event_2005} Unlike traditional persistence models that overwrite data and store only the most recent state, event sourcing maintains an immutable record of every action taken over time. These events are persisted in an append-only event store, which serves as the principal source of truth from which the current system state can be derived. \parencite{fowler_what_2017,lima_improving_2021} % TODO bessere quelle finden

The current state of any entity in such a system can be rebuilt by replaying the history of events from the log, starting from an initial blank state. \parencite{fowler_event_2005} To address the performance costs of replaying thousands of events for every request, developers implement projections or materialized views, which are read-only, often denormalized versions of the data optimized for specific queries. \parencite{malyi_developing_2024} This separation of concerns is frequently managed by pairing event sourcing with the \hyperref[sec:cqrs]{\acrlong{cqrs} (\acrshort{cqrs})} pattern, which physically divides the data structures used for reading from those used for writing state changes. \parencite[50]{young_cqrs_2010} % TODO Snapshots erklären; Projection = "ephemeral", kann leicht neu gebaut werden 

Because every action taken on the system is stored, a number of facilities can be built on top of the event log: Temporal queries can be made, which determine the exact state of the application at any point in time. The event log acts as an immutable audit trail, making Event Sourcing architectures highly valuable for systems like accounting applications. \parencite{fowler_event_2005}

\subsection{Traceability and auditing in IT systems}

\subsubsection{Audit Logs}

An audit log (often called audit trail) is a chronological record which provides evidence of a sequence of activities on an entity. \parencite{committee_on_national_security_systems_national_2010} In information security, the audit log stores a record of system activities, enabling the reconstruction of events. \parencite{atis_committee_atis_2013} A trustworthy audit log in a system can guarantee the principle of traceability which states that actions can be tracked and traced back to the entity who is responsible for them. \parencite[266]{joint_task_force_interagency_working_group_security_2020}

Traceability and auditing are legal requirements across various sectors, as they are derived from federal laws and regulations intended to protect the integrity and confidentiality of sensitive data. Organizations implement these mechanisms to stay compliant with mandates that require a verifiable, time-sequenced history of system activities to support oversight and forensic reviews. In the financial sector, for example, 17 CFR § 242.613 requires the establishment of a consolidated audit trail to track the complete lifecycle of securities orders, documenting every stage from origination and routing to final execution. \parencite{us_securities_and_exchange_commission_17_2012}

\textcite{fowler_audit_2004} describes an audit log as simple and effective way of storing temporal information. Changes are tracked by writing a record indicating \emph{what} changed \emph{when}. A basic implementation of an audit log can have many forms, for example a text file, database tables or \acrshort{xml} documents. Fowler also mentions that while the audit log is easy to write, it is harder to read and process. While occasional reads can be done by eye, complex processing and reconstruction of historical state can be resource-intensive.

\subsubsection{Event Streams}

\subsubsection{Rebuilding state from an audit log and an event stream}

\subsection{Scalability of systems}

\section{Related Work}

\section{Proposed Method}

This thesis aims to provide a fair, quantitative comparison of \acrshort{crud} and \acrshort{cqrs} / \acrshort{es} architectures. To achieve this, the architectures should be applied not only to the same domain, but to the exact same requirements. The implementations can then be tested against the same \glspl{contract-test}.

This chapter will first present the requirements for the actual application, then outline metrics and comparison methods.

\subsection{Project requirements}

The applications will implement a course enrollment and grading system which might for example be used in universities. Core features include:

\begin{itemize}
    \item Professors can create courses and lectures
    \item Students can enroll and disenroll from lectures
    \item Professors can enter grades
    \item Students can view their current and past lectures
    \item Students can view their credits
\end{itemize}

\subsubsection{Entities}
\label{sec:entities}

Two types of users exist in the domain: professors and students. Their personal information is not relevant for this thesis, which is why only their first and last name are stored for presentation reasons. The student additionally has a semester.

Professors can create courses. Courses have a name, a description, an amount of credits they yield, a minimum amount of credits required to enroll and can have a set of courses as prerequisites.

Courses are the "blueprints" for lectures. Lectures are the "implementation" of a course for a semester. Each lecture created from a course yields the course's amount of credits and has the requirements specified by the course. Lectures have a lifecycle: they can be in draft state, open for enrollment, in progress, finished or archived. A lecture has a list of time slots and a maximum amount of students that can enroll.

A lecture can have several assessments. Each assessment has a type. The professor can enter grades for a student and an assessment. Grades are integers in the range of 0 to 100. Credits are awarded to a student as soon as they completed all assessments for a lecture with a passing grade (grade higher than 50).

\subsubsection{Business rules}

Relationships and business rules in this system are deliberately chosen complex, involving many relationships between \hyperref[sec:entities]{entities} and intricate validation rules. This approach was adopted in order to be able to make realistic assumptions about the research question by evaluating a project that closely resembles complex, real-world scenarios.

\begin{itemize}
    \item Existence checks: any requests including references to entities will fail if the references entities do not exist.
    \item Requests leading to conflicts, for example creating a lecture with overlapping time slots, will fail
    \item When a student tries enrolling to a lecture which is already full, they will be put on a waitlist
    \item When a student disenrolls from a lecture, the next eligible student (higher semesters are preferred) will be enrolled
    \item Actions on a lecture can only be done during the appropriate lifecycle state (enrolling only when the lifecycle is "open for enrollment", grades can only be assigned when the lecture is "finished")
\end{itemize}

\subsubsection{Contract Tests}
\label{sec:contract-tests}

To ensure both implementations adhere to the business rules, an extensive test suite was set up. While the internals of the implementations are vastly different architecturally and conceptually, they both have the same public \gls{api}. This makes it possible to run the same test suite on both apps by sending \gls{http} requests and verifying their responses. The test suite includes integration tests for all \gls{api} endpoint covering both regular and edge-case (error) scenarios to ensure that the \acrshort{crud} and \acrshort{es}-\acrshort{cqrs} application exhibit identical state transitions and error behaviors. \hyperref[sec:contract-test-implementation]{Section \ref{sec:contract-test-implementation}} outlines the implementation of those tests in detail.

\subsection{Performance}

\subsection{Scalability or flexibility (TODO)}

\subsection{Traceability}

\subsection{Tech Stack}

\subsubsection{SpringBoot}

SpringBoot is an open-source, opinionated framework for developing enterprise Java applications. It is based on Spring Framework, which is a platform aiming to make Java development "quicker, easier, and safer for everybody" \parencite{noauthor_why_nodate}. Spring provides an Inversion of Control (IoC) container which can be used for dependency injection. \parencite[Chapter~1]{deinum_spring_2023} It offers support for several programming paradigms: reactive, event-driven, microservices and serverless. \parencite{noauthor_why_nodate}

SpringBoot builds on top of the Spring platform by applying a "convention-over-configuration" approach, intended to minimize the need for configuration. In a 2023 survey by JetBrains, SpringBoot was the most popular choice of web framework. \parencite{noauthor_java_nodate}

Spring Boot starters are specialized dependency descriptors designed to simplify dependency management by aggregating commonly used libraries into feature-defined packages. Rather than requiring developers to manually identify and maintain a list of individual group IDs, artifact IDs, and compatible version numbers for every necessary library, starters use transitive dependency resolution to pull in all required components under a single entry. To quickly bootstrap a web application, a developer can simply add the \javaname{spring-boot-starter-web} dependency to their Maven or Gradle build file. By requesting this specific functionality, Spring Boot automatically includes essential dependencies such as Spring MVC, Jackson for JSON processing, and an embedded Tomcat server, ensuring that all included libraries have been tested together for compatibility. This approach shifts the developer's focus from managing individual JAR files to simply defining the high-level capabilities the application requires, minimizing configuration overhead and reducing risk of version mismatches. \parencite[Chapter~1.1.2]{walls_spring_2016}
% TODO code example for IoC container / dependency injection 
\subsubsection{JPA}

\acrfull{jpa}, formerly Java Persistence \gls{api} is a Java specification which provides a mechanism for managing persistence and object-relational mapping (\acrshort{orm}). \glspl{orm} act as a brige between the relational world of SQL databases and the object-oriented world of Java.

Instead of writing SQL to create the database schema, entities can be described using special Java classes (defined by annotations or \acrshort{xml} configurations) which can be mapped to an SQL schema. \acrshort{jpa} allows querying the database for these entities in a type-safe way by providing a range of helpful query methods on JPA repositories, for example \texttt{findAll()} or \texttt{findById(UUID id)}. This removes the need to write "low-level", database-specific SQL for basic \acrshort{crud} operations. Complex data retrieval is also possible with \acrshort{jpa} using the \acrfull{jpql}, which is an object-oriented, database-agnostic query language.

When using \acrshort{jpa} with SpringBoot by including the \javaname{spring-boot-starter-data-jpa} dependency, \emph{Hibernate} is used as implementation of the \acrshort{jpa} standard. \parencite[Chapter~1]{bauer_java_2016}

\subsubsection{PostgreSQL}

PostgresQL is an open-source relational database which has been in active development for over 35 years. It is designed for a wide range of workloads and can handle many tasks thanks to its extensibility and large suite of extensions, such as the popular PostGIS extension for storing and querying geospatial data. \parencite{noauthor_postgis_nodate} As of January 2026, PostgreSQL 18 is the latest version. \parencite{postgresql_global_development_group_postgresql_2026} % TODO write more; book source

\subsubsection{Jackson}

Jackson is a high-performance, feature-rich \acrshort{json} processing library for Java. It is the default \acrshort{json} library used within the Spring Boot ecosystem. Its primary purpose is to provide a seamless bridge between Java objects and JSON data through three main processing models: the Streaming API for incremental parsing, the Tree Model for a flexible node-based representation, and the most commonly used Data Binding module. This data binding capability allows developers to automatically convert (\emph{marshal}) Java \glspl{pojo} into JSON and vice versa (\emph{unmarshal}) with minimal configuration. Beyond its speed and efficiency, Jackson is highly extensible, offering modules to handle complex Java types like Java 8 Date/Time and Optional classes. Jackson also supports various other data formats such as XML, YAML and CSV. \parencite{noauthor_jackson_nodate,fasterxml_jackson_2025}

\subsubsection{Axon}
\label{sec:axon}

Axon Framework is an open-source Java framework for building event-driven applications. Following the \acrshort{cqrs} and event-sourcing pattern, Commands, Events and Queries are the three core message types any Axon application is centered around. Commands are used to describe an intent to change the application's state. Events communicate a change that happened in the application. Queries are used to request information from the application.

Axon also supports \acrlong{ddd} by providing tools to manage entities and domain logic. \parencite{axoniq_introduction_2025,axoniq_messaging_2025}

Axon Server is a platform designed specifically for event-driven systems. It functions as both a high-performance Event Store and a dedicated Message Router for commands, queries, and events. By bundling these responsibilities into a single service, Axon Server replaces the need for separate infrastructures such as a relational database for events and a message broker like Kafka or RabbitMQ for communication. Axon Server is designed to seamlessly integrate with Axon Framework. When using the Axon Server Connector, the application automatically finds and connects to the Axon Server. It is then possible to use the Axon server without further configuration. \parencite{axoniq_introduction_2025-1,axoniq_axon_2025} % TODO book source 

\subsubsection{Testing}

To ensure functionality of the applications, unit and integration tests were implemented using various testing libraries like JUnit as the testing platform, \gls{restassured} for making and asserting \gls{http} calls, Mockito for unit testing and ArchUnit for architecture tests. This chapter describes all mentioned technologies.

JUnit is an open-source testing framework for Java. It offers a structured way of writing tests, driven by lifecycle methods like \texttt{beforeEach} or \texttt{afterAll}. Tests are annotated with \texttt{@Test}. They can also be parametrized and run repeatedly. Results can be asserted using assertion methods like \texttt{assertTrue()}. \parencite{noauthor_junit_nodate}

REST Assured is a Java library that provides a highly fluent \acrshort{dsl} for testing and validating REST APIs in a readable, chainable style. It allows complex assertions to be written inline using \gls{groovy} expressions, making it easy to deeply verify JSON responses beyond simple field checks. \parencite{restassured-documentation}

The below code example shows how one might use a \gls{groovy} expression to find and validate a path in the returned JSON object:

\begin{lstlisting}
RestAssured.when()
    // omitted request 
    .then()
    .body(
        "data.grades.find { it.combinedGrade == 0 }.credits", 
        equalTo(0)
    );
\end{lstlisting}

Here, the path \texttt{data.grades} of the returned JSON object is expected to be an array. The array is filtered using a \gls{gpath} with a closure to find the first entry where \texttt{combinedGrade} equals 0. Then, this entry's \texttt{credits} field is extracted and validated using the \texttt{equalTo(0)} matcher.

% TODO Mockito, ArchUnit 

\subsubsection{SpringBoot Actuator}
\label{sec:actuator}

Spring Boot Actuator is a tool designed to help monitor and manage Spring Boot applications running in a production environment. It provides several built-in features that allow developers to check on the application's status, gather performance data, and track \gls{http} requests. Developers can interact with these features using either web-based \gls{http} links or \acrshort{jmx}, which is a standard Java management technology. By using Actuator, developers can quickly see if an application is running correctly without the need to write custom monitoring code.

The most common way to use Actuator is through its "endpoints", which are specific web addresses that provide different types of information. For example, the health endpoint shows if the application and its connected services, like databases, are working, while the metrics endpoint displays detailed data on memory and CPU usage. Actuator also integrates with cloud platforms like Kubernetes to provide status updates for automated systems. Beyond the standard options, you can also create your own custom endpoints or connect the data to external monitoring software to visualize how your application is performing over time.

Actuator can be enabled in a Spring Boot project by including the \javaname{spring-boot-starter-actuator} dependency. \parencite{noauthor_production-ready_nodate}

\subsubsection{Prometheus}

Prometheus is an open-source systems monitoring toolkit originally built at SoundCloud that is now a project under the Cloud Native Computing Foundation. It is primarily used for collecting and storing multi-dimensional metrics as time-series data, meaning information is recorded with a timestamp and optional key-value pairs called labels. The system is designed for reliability and is capable of scraping data from instrumented jobs and web servers, storing it in a local time-series database, and triggering alerts based on predefined rules when specific thresholds are met. Through its powerful functional query language called "PromQL", developers can aggregate and visualize performance data. \parencite{noauthor_prometheus_2026,prometheus-overview-2026}

To collect and export \hyperref[sec:actuator]{Actuator} metrics specifically for Prometheus, the \javaname{micrometer-registry-prometheus} dependency must be included in the classpath. \parencite{noauthor_micrometer_nodate} Access to the metrics is granted by including "prometheus" in the list of exposed web endpoints within the application's configuration properties. Once these components are in place, metrics are automatically formatted for consumption and can be scraped by a Prometheus server. \parencite{noauthor_metrics_nodate}

\subsubsection{Docker}

\gls{docker} is a platform utilized for the development and deployment of applications. It is designed to separate software from the underlying infrastructure, which allows for faster delivery and consistent environments.

The capabilities of \gls{docker} are centered around the use of containers, which are lightweight and isolated environments. Each container is packaged with all necessary dependencies required for an application to run, ensuring it operates independently of the host system. High portability is maintained, as these workloads can be executed across different environments, such as local computers, data centers, or cloud providers. \parencite{what-is-docker}

A \gls{dockerfile} is a text-based document that contains a series of instructions used to assemble a Docker image. Each command within this file results in the creation of a layer in the image, making the final template efficient and fast to rebuild. These images serve as read-only blueprints from which runnable instances, or containers, are created. \parencite{writing-a-dockerfile}

Docker Compose is a tool employed for the definition and management of applications that consist of multiple containers. A single configuration file is used to specify the services, networks, and volumes required for the entire application stack. Through this tool, the lifecycle of complex applications is streamlined, enabling all associated services to be started, stopped, and coordinated with simplified commands. \parencite{what-is-docker-compose}

\subsubsection{k6}

\section{Implementation}

\subsection{Contract Test Implementation}
\label{sec:contract-test-implementation}

The \hyperref[sec:contract-tests]{contract tests} are implemented in a separate maven module called \texttt{test-{\allowbreak}suite}\footnote{\javaname{test-suite/src/test/java/karsch.lukas}}. The test classes use the \texttt{JUnit 5} testing framework and \texttt{REST Assured} to send and assert \gls{http} requests. A basic test might look like this:

\begin{lstlisting}
@Test
void getLectureDetails_shouldReturn200_returnTwoDates() {
    // First, create seed data
    var lectureSeedData = createLectureSeedData();

    RestAssured.given()
            .when() 
            .get("/lectures/{lectureId}", lectureSeedData.lectureId())
            .then()
            .statusCode(200)
            .body("data.dates", hasSize(2));
}
\end{lstlisting}
\footnote{Adapted from \javaname{test-suite/src/test/karsch.lukas.lectures.AbstractLecturesE2ETest}}

All contract tests follow a consistent pattern. First, they are annotated with \javaname{@DisplayName} to provide a descriptive, human-readable name. The test method itself is precisely named after the behavior it asserts. In the example above, the test verifies that the response status code is \javaname{200} and that the response body contains a field called \javaname{dates} consisting of an array of size two.

Before making these assertions, each test creates "seed data". Seed data is prerequisite data that must exist on the system under test for the execution to be valid. For instance, a professor, a course, and a lecture must be created before the endpoint to \javaname{GET} that specific lecture can be tested. Tests that assert invariants, such as the business rule preventing lecture from having overlapping timeslots, typically set the system time via a Spring Boot Actuator endpoint first.

Once the prerequisites are met, the request is executed and assertions are made using \gls{restassured}. The \texttt{given()} block sets up the request requirements like headers, parameters, or body content; the \texttt{when()} block defines the action, such as the \gls{http} method (GET, POST) and the endpoint URL. Finally, the \texttt{then()} block is used to verify the response, allowing the developer to assert status codes and validate the data returned in the response body.

The test classes in \texttt{test-suite} are all \texttt{abstract}, meaning they can not be run directly. Instead, they are intended to be subclassed by the modules implementing the concrete applications (\texttt{impl-crud} \& \texttt{impl-es-cqrs}). The subclasses must implement a set of abstract methods which are implementation specific, for example a method to reset the database in between each test, a method to set the application's time and methods to create seed data for tests.

Necessary infrastructure is spun up by the subclasses using \glspl{testcontainer}. \glspl{testcontainer} is a way to declare infrastructure dependencies as code and is an open-source library available for many programming languages. \parencite{testcontainers-homepage}

\begin{lstlisting}
%%@TestConfiguration%%
public class PostgresTestcontainerConfiguration {
    %%@Bean%%
    %%@ServiceConnection%%
    %%@RestartScope%%
    PostgreSQLContainer<?> postgreSQLContainer() {
        return new PostgreSQLContainer<>(
                DockerImageName.parse("postgres:latest"));
    }
}
\end{lstlisting}
\footnote{\javaname{impl-crud/src/test/karsch.lukas.PostgresTestcontainerConfiguration}}

The above code snippet starts a PostgreSQL container using the latest available image. \texttt{@ServiceConnection} makes sure the Spring application can connect to the container. This configuration can then be imported into the test class:

\begin{lstlisting}
%%@SpringBootTest%%
%%@Import%%(PostgresTestcontainerConfiguration.class)
public class CrudLecturesE2ETest extends AbstractLecturesE2ETest { }
\end{lstlisting}
\footnote{\javaname{impl-crud/src/test/karsch.lukas.e2e.lectures.CrudLecturesE2ETest}}

\subsection{CRUD implementation}

This chapter presents the relevant aspects of the CRUD implementation, mainly focusing on relational modeling using \acrshort{jpa} and the audit log implementation.

\subsubsection{Relational Modeling}

The CRUD implementation uses a \hyperref[sec:crud-architecture]{normalized database} in the Third Normal Form.

\begin{figure}[h]
    \includegraphics[width=\textwidth, inner]{../vault/Thesis/images/CRUD_ER_Diagram_3.png}
    \caption{Entity Relationship Diagram for the CRUD App}
    \label{fig:crud-er-diagram}
\end{figure}

Figure \ref{fig:crud-er-diagram} shows the Entity Relationship Diagram for the CRUD app. It includes nine entities and a value object for the app's relational database schema. Each box corresponds to an entity or value object, with the bold text being the name. Below the table's name, all attributes of the entity are listed with their type and name.

Arrows represent an association. The numbers at the end of the arrows convey the multiplicity. An arrow pointing in only one direction stands for a unidirectional association, while an arrow pointing in both directions conveys a bidirectional association. For example, an arrow pointing between entity \texttt{A} and entity \texttt{B} like so: \texttt{1} $\longleftrightarrow$ \texttt{0..1} shows that one \texttt{A} can be associated with any number of \texttt{B}'s, and a \texttt{B} is always associated with exactly one \texttt{A}. % TODO kinda unreadable 

Arrows with a filled diamond represent a composition. Compositions are used when an entity has a reference to a value object. This value object has no identity and is directly embedded into the entity. The only value object in figure \ref{fig:crud-er-diagram} is the \texttt{TimeSlotValueObject}.

In the app's ER diagram, the \texttt{LectureEntity} serves as core of the schema, having several key associations. The 0..* $\longrightarrow$ 1 association to \texttt{CourseEntity} shows that many lectures can be created from a course and a lecture is always associated with a course. The 0..* $\longrightarrow $ 1 association to \texttt{ProfessorEntity} shows that a professor can hold many lectures (or none), and that a lecture is always associated with a professor. From the lecture's side, these relationships are called "Many to One" relationships.

\texttt{LectureEntity} also has "One to Many" relationships to \texttt{LectureWaitlistEntryEntity}, \texttt{EnrollmentEntity} and \texttt{LectureAssessmentEntity}. \texttt{LectureWaitlistEntryEntity} is a table which stores students who are waitlisted for a lecture. It is effectively a join table (with one extra column to track when the student was waitlisted) and represents a Many to Many relationship between lectures and students. The same applies to \texttt{EnrollmentEntity} which is a table storing which students are enrolled to which lecture. \texttt{LectureAssessmentEntity} represents the fact that a lecture can have many assessments (which may be an exam, a paper or a project). Each assessment in turn has many \texttt{AssessmentGradeEntity}s associated with it. This table stores which student scored which grade on an assessment. % TODO fix spacing 

The \texttt{AuditLogEntry} is also visible on the ER diagram, however it has no relationships. This table and the entire audit log implementation will be laid out in the \hyperref[sec:audit-log]{following section}.

These entities are implemented using SpringBoot's \acrshort{jpa} integration. For example, an entity with a "One to Many" relationship can be implemented like this:

\begin{lstlisting}
%%@Entity%% 
class LectureEntity {
    %%@Id%% 
    private UUID id; 

    %%@OneToMany(fetch=FetchType.LAZY)%%
    private List<EnrollmentEntity> enrollments; 
}
\end{lstlisting}

The \javaname{@Entity} annotation informs \acrshort{jpa} that the class should be mapped to a database table. If the schema generation feature is enabled, \acrshort{jpa} automatically creates a table structure that mirrors the class definition. In production environments where this feature is typically disabled, developers must provide SQL scripts to manually define the expected structure. This is commonly achieved either by including a basic initialization script or by utilizing dedicated database migration tools such as Flyway or Liquibase to manage versioned schema changes.

Each entity must include a field annotated with \javaname{@Id}, which serves as the unique primary key for the corresponding database record.

The \javaname{@OneToMany} annotation defines a relational link between two entities. While the collection is accessed in Java as a standard list via \javaname{lecture.getEnrollments()}, \acrshort{jpa} manages this behind the scenes using a foreign key relationship. The \texttt{fetch} parameter determines when this data is retrieved: \texttt{LAZY} loading defers the database query until the collection is explicitly accessed in the code, whereas \texttt{EAGER} loading fetches the related entities immediately alongside the parent object.

\subsubsection{Audit Log implementation}
\label{sec:audit-log}

There are several strategies to implement an audit log, each with its own trade-offs:

\begin{enumerate}
    \item \textbf{Manual Logging}: Developers explicitly call a logging service in every service method that modifies data. While simple, this can lead to code duplication and is prone to human error, such as developers forgetting to add a log statement. A code example might look like this:

          \begin{lstlisting}
public void updatePhoneNumber(User user, int newNumber) {
    logChange(Date.now(), user, user.getPhoneNumber(), newNumber, "UserRequestedNumberChange");
    user.setPhoneNumber(newNumber);
}

void logChange(
    Date date, User user, Object oldValue, Object newValue, String context
) {
    LogEntry logEntry = new LogEntry(date, user, oldValue, newValue);
    logRepository.persist(logEntry);
}
\end{lstlisting}
    \item \textbf{Database Triggers}: Database-level triggers can capture changes automatically. This offers high performance and guarantees that no change is missed, even if made outside the application. However, this approach ties the logic to a specific database and makes business logic harder to maintain and test, as it resides outside the codebase (needs reference).
    \item  \textbf{Hibernate Envers}: A popular solution for JPA-based applications. It automatically versions entities. However, Envers typically creates a shadow table for every entity, which can be overkill if only specific high-level changes need to be tracked, and querying the history can be complex. (needs reference)
    \item \textbf{JPA Entity Listeners}: Using the JPA specification's lifecycle events (\texttt{@PrePersist}, \texttt{@PreUpdate}, etc.) to intercept changes. This approach is database-independent and keeps the logic within the Java application, allowing access to Spring's security context. The security context makes it easy to access the current user, making it possible to attach them to the new audit log entry. (needs reference) \label{item:jpa-entity-listener}
\end{enumerate}

\paragraph{Chosen Implementation: JPA Entity Listener}\mbox{}\\

The \acrshort{crud} implementation of the application utilizes approach \ref{item:jpa-entity-listener}, JPA entity listeners, which offer a good balance between automation and flexibility. This approach ensures that every change to an entity is captured without polluting the service layer with logging calls, while still allowing the application to enrich the log with application-level context. The current user is automatically added to the log entry by the entity listener, while service methods can attach additional context to an entity before it is persisted, if desired.

\subparagraph{Data Model}\mbox{}\\

The audit log is stored in a single database table, represented by the \texttt{AuditLogEntry} entity. This structure allows for easy querying of all system changes in chronological order. Table \ref{table:audit-log-entry} lists the fields contained by \texttt{AuditLogEntry}. This data model is adapted from \textcite{fowler_audit_2004}.

\begin{table}[htp!]
    \centering
    \begin{tabular}{|l|p{4cm}|p{6cm}|}
        \hline
        \textbf{Field} & \textbf{Type, Possible Values} & \textbf{Explanation}                                  \\ \hline
        entityName     & String                         & Identifies the type of the changed object.            \\ \hline
        entityId       & UUID                           & Unique identifier of the specific changed object.     \\ \hline
        timestamp      & LocalDateTime                  & The date and time when the change occurred.           \\ \hline
        operation      & CREATE, UPDATE, DELETE         & The type of action performed on the entity.           \\ \hline
        modifiedBy     & String                         & The user or system process that initiated the change. \\ \hline
        oldValueJson   & TEXT (JSON String)             & The serialized state of the entity before the change. \\ \hline
        newValueJson   & TEXT (JSON String)             & The serialized state of the entity after the change.  \\ \hline
        contextJson    & TEXT (JSON String)             & Captures additional business intent or metadata.      \\ \hline
    \end{tabular}
    \caption{Audit Log Entry Structure}
    \label{table:audit-log-entry}
\end{table}

\subparagraph{AuditableEntity and AuditEntityListener}\mbox{}\\

To enable auditing, entities extend an abstract base class, \texttt{AuditableEntity} \footnote{\javaname{impl-crud/src/main/karsch.lukas.audit.AuditableEntity}}. This class marks the entity with the \javaname{@EntityListeners(AuditEntityListener.class)} annotation and provides a transient field, \texttt{snapshotJson}, used to store the state of the entity when it is loaded from the database.

The core logic resides in \texttt{AuditEntityListener}\footnote{\javaname{impl-crud/src/main/karsch.lukas.audit.AuditEntityListener}}. It hooks into the JPA lifecycle:

\begin{itemize}
    \item \texttt{@PostLoad}: Immediately after an entity is fetched from the database, the listener serializes it to \acrshort{json} and stores the serialized \acrshort{json} string in the \texttt{snapshotJson} field. This serves as the "old value" for any subsequent updates.
    \item \texttt{@PrePersist}: Before a newly created entity is saved, a log entry is created with the operation \texttt{CREATE}. The \texttt{newValueJson} is the current state; \texttt{oldValueJson} is null.
    \item \texttt{@PreUpdate}: Before an existing entity is updated, the listener compares the current state with the \texttt{snapshotJson}. It creates an \texttt{UPDATE} entry using \texttt{snapshotJson} as the \texttt{oldValue} and the current state as the \texttt{newValue}.
    \item \texttt{@PreRemove}: Before deletion, a \texttt{DELETE} entry is created, preserving the last known state. \texttt{snapshotJson} is saved as the \texttt{oldValue}, \texttt{newValue} is \texttt{null}
\end{itemize}

\paragraph{Serialization and Relationships}\mbox{}\\

A major challenge in serializing JPA entities to \acrshort{json} for an audit log is handling relationships. A naive \acrshort{json} serialization would follow every relationship of the affected entity (e.g., \texttt{Lecture} $\rightarrow$ \texttt{Course}), potentially serializing huge entity graphs or even causing a \texttt{StackOverflowError} due to cyclic references. One example for such a problematic reference would be the self-reference of \texttt{CourseEntity}, which each course having a "One to Many" relationship to prerequisite courses.

A custom Jackson module, \texttt{IdSerializationModule}, was implemented to deal with the problem of relationship serialization. This module overrides the default serialization for related entities. Instead of serializing the full nested object, it only writes the related object's ID. This results in a flat, lightweight \acrshort{json} structure that is readable, keeps all references reconstructible and is safe to store. % TODO explain how this was implemented in detail 

\paragraph{Capturing Business Context}\mbox{}\\

One downside of standard automated auditing is that it captures \emph{what} changed (e.g., lecture lifecycle changed from \texttt{IN\_PROGRESS} to \texttt{FINISHED}) but not necessarily \emph{why} (e.g., Lecture cancelled by professor due to illness). To make the CRUD application's audit log compliant to the requirement of traceability, it supports an \texttt{AuditContext}. Services can attach metadata to the current thread/transaction, which the \texttt{AuditEntityListener} retrieves and stores in the \texttt{contextJson} field.

\subsection{ES/CQRS implementation}

\subsection{Infrastructure}

\subsection{Performance Tests}

\section{Results}

\section{Discussion}

\subsection{Analysis of results}

\subsection{Conclusion \& Further work}

Finally, I'm done!

\newpage
\printglossaries

\newpage
\printbibliography

\appendix

\section{Source Code}

The full source code for this thesis, including both apps, performance tests and markdown notes, is available at: \url{https://gitlab.mi.hdm-stuttgart.de/lk224/thesis}

\end{document} % This is the end of the document
