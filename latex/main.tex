%LTeX: enabled=false
\documentclass[11pt,a4paper]{report}
\usepackage{array}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[automake, acronym]{glossaries}
\usepackage{xcolor}
\usepackage[export]{adjustbox}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{float} 

\floatstyle{ruled}
\newfloat{listing}{tbp}{lol}
\floatname{listing}{Listing}

% TODO
% microtype is supposed to help with text breaking (overflow), but it gives an error:  pdfTeX error (font expansion): auto expansion is only possible with scalable fonts. <argument> ...shipout:D \box_use:N \l_shipout_box
% \usepackage{microtype}

\hfuzz=5.6pt 

\usepackage{color}

\definecolor{pblue}{rgb}{0.13,0.13,1}
\definecolor{pgreen}{rgb}{0,0.5,0}
\definecolor{pred}{rgb}{0.9,0,0}
\definecolor{pgrey}{rgb}{0.46,0.45,0.48}
\definecolor{porange}{rgb}{1,0.5,0.1} % Todo Change orange 

\usepackage{listings}
\lstset{language=Java,
  showspaces=false,
  showtabs=false,
  breaklines=true,
  showstringspaces=false,
  breakatwhitespace=true,
  columns=flexible,
  commentstyle=\color{pgrey},
  keywordstyle=\color{porange},
  stringstyle=\color{pgreen},
  basicstyle={\ttfamily\small},
  moredelim=[il][\textcolor{pgrey}]{\$\$},
  moredelim=[is][\textcolor{pgrey}]{\%\%}{\%\%}
}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{porange},
  ndkeywords={class, export, default, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{porange}\bfseries,
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{pgrey},
  stringstyle=\color{pgreen},
  morestring=[b]',
  morestring=[b]",
  morestring=[b]`
}

\usepackage{caption}
\captionsetup{font=small}

\DeclareCaptionFormat{listing}{
  \rule{\linewidth}{0.2pt}\par\vskip1pt #1#2#3
}
\captionsetup[lstlisting]{format=listing, justification=centering, singlelinecheck=false}

\usepackage[
  backend=biber,
  style=ieee, % or authoryear, numeric, ieee, apa, ...
]{biblatex}
\DefineBibliographyStrings{english}{
  and = {\&}
}

\usepackage{setspace}
\usepackage{etoolbox}
% 1.5 Zeilenabstand
\onehalfspacing
% außer für Listings 
\BeforeBeginEnvironment{lstlisting}{\begin{singlespace}}
\AfterEndEnvironment{lstlisting}{\end{singlespace}}

\usepackage{hyperref} % must be loaded last! 

% Adapted from Source - https://tex.stackexchange.com/a
% Posted by Don Hosek
% Retrieved 2026-01-08, License - CC BY-SA 4.0
\ExplSyntaxOn
\NewDocumentCommand{\javaname}{ m }
  {
    \group_begin:
    \tl_set:Nn \l_tmpa_tl { #1 }
    % Replace dots with dot + allowbreak
    \tl_replace_all:Nnn \l_tmpa_tl { . } { .\allowbreak }
    % Replace dashes with dash + allowbreak
    \tl_replace_all:Nnn \l_tmpa_tl { - } { -\allowbreak }
    
    \texttt{ \tl_use:N \l_tmpa_tl }
    \group_end:
  }
\ExplSyntaxOff


% adapted from https://tex.stackexchange.com/a/720461
% Posted by David Carlisle
% Retrieved 2026-01-23, License - CC BY-SA 4.0
\ExplSyntaxOn
% Create a boolean to track the first character
\bool_new:N \l__cschulz_first_char_bool
\NewDocumentCommand{\keyw}{m}
 {
  \texttt{ \cschulz_keyw:n { #1 } }
 }
\cs_new_protected:Nn \cschulz_keyw:n
 {
  % Initialize the flag to true for every new call
  \bool_set_true:N \l__cschulz_first_char_bool
  \tl_map_inline:nn { #1 }
   {
    % Check if uppercase
    \str_if_eq:eeT { ##1 } { \str_uppercase:n { ##1 } } 
     { 
      % ONLY add the hyphen if it's NOT the first character
      \bool_if:NF \l__cschulz_first_char_bool { \- }
     }
    % Print the character
    ##1
    % After the first character is printed, set flag to false
    \bool_set_false:N \l__cschulz_first_char_bool
   }
 }

\ExplSyntaxOff

\addbibresource{references.bib}

\makeglossaries

%LTeX: enabled=true
\newglossaryentry{api}
{
    name=API, 
    description={API stands for \emph{Application Programming Interface}. It describes the public interface of a module or service, often exposed over a network}
}

\newglossaryentry{rest}
{
    name=REST,
    description={REST stands for \emph{Representational State Transfer}. It is an architectural style for distributed hypermedia systems}
}

\newglossaryentry{http}
{
    name=HTTP,
    description={HTTP stands for \emph{Hypertext Transfer Protocol}. It is a protocol used in internet communication and was defined in RFC 2616 \parencite{rfc2616}}
}

\newglossaryentry{adm}
{
    name={Anemic Domain Model},
    description={The objects describing the domain only hold data, no logic}
}

\newglossaryentry{rdm}
{
    name={Rich Domain Model},
    description={Objects incorporate both data and the behavior or rules that govern that data}
}

\newglossaryentry{atomicity}
{
    name=Atomicity,
    description={Atomicity means that an action is either fully executed or not at all. Atomic operations make sure the application is not left in an invalid state \parencite[10]{bernstein_principles_2009}}
}

\newglossaryentry{cap}
{
    name={CAP Theorem},
    description={The CAP theorem by Eric Brewer states that a distributed data store can only display at most two of the following three guarantees at the same time: consistency, availability and partition tolerance \parencite{gilbert_brewers_2002}}
}

\newglossaryentry{contract-test}
{
    name={Contract Test},
    text={contract test},
    plural={contract tests},
    description={A contract test verifies that services implement a shared interface by testing their interactions against an explicitly defined contract}
}

\newglossaryentry{groovy}
{
    name=Groovy,
    description={A dynamic JVM language that extends Java with concise syntax and powerful features such as closures, making it well suited for scripting, DSLs, and test code \parencite{groovy-homepage}}
}

\newglossaryentry{gpath}
{
    name={GPath Expression},
    text={GPath expression},
    plural={GPath expressions},
    description={A Groovy-based path language for navigating and querying nested object graphs (such as JSON or XML) using concise, expressive selectors and closures  \parencite{groovy-gpath}}
}

\newglossaryentry{restassured}
{
    name={REST Assured},
    description={A library for testing HTTP servers}
}

\newglossaryentry{dockerfile}
{
    name=Dockerfile,
    description={A text document containing a series of instructions used to assemble a Docker image}
}

\newglossaryentry{docker}
{
    name=Docker,
    description={Open platform for developing, shipping and running containerized applications}
}

\newglossaryentry{testcontainer}
{
    name=Testcontainer,
    description={Testcontainers are a way to declare infrastructure dependencies as code using Docker \parencite{testcontainers-homepage}}
}

\newglossaryentry{percentile}
{
    name=Percentile,
    text=percentile,
    description={A statistical measure indicating the value below which a given percentage of observations in a group of data falls. For example, the $n$-th percentile is the threshold where $n$ percent of requests are faster than that specific value}
}

\newglossaryentry{median}
{
    name={Median (P50)},
    text=median,
    description={The middle value in a sorted list of response times, representing the "typical" delay experienced by users. It indicates that exactly 50\% of requests are served faster than this threshold; the other 50\% are slower}
}

\newglossaryentry{tail-latency}
{
    name={Tail Latency},
    text={tail latency},
    description={The response times observed at high percentiles (such as P95, P99, or P99.9), representing the slowest requests in a distribution. These "outliers" are critical to monitor because they often affect the most data-intensive operations or represent the worst-case user experience}
}

\newacronym{www}{WWW}{World Wide Web}

\newacronym{hateoas}{HATEOAS}{Hypermedia as the engine of application state}

\newacronym{html}{HTML}{HyperText Markup Language}

\newacronym{json}{JSON}{JavaScript Object Notation}

\newacronym{xml}{XML}{Extensible Markup Language}

\newacronym{dao}{DAO}{Data Access Object}

\newacronym{ddd}{DDD}{Domain Driven Design}

\newacronym[plural=URIs]{uri}{URI}{Uniform Resource Identifier}

\newacronym{crud}{CRUD}{Create Read Update Delete}

\newacronym{dto}{DTO}{Data Transfer Object}

\newacronym{acid}{ACID}{Atomicity, Consistency, Isolation, Durability}

\newacronym{cqrs}{CQRS}{Command Query Responsibility Segregation}

\newacronym{cqs}{CQS}{Command And Query Separation}

\newacronym{base}{BASE}{Basically Available, Soft State, Eventual Consistency}

\newacronym{es}{ES}{Event Sourcing}

\newacronym{dsl}{DSL}{Data Specific Language}

\newacronym{jpa}{JPA}{Jakarta Persistence API}

\newacronym{orm}{ORM}{Object-relational Mapper}

\newacronym{jpql}{JPQL}{Java Persistence Query Language}

\newacronym{pojo}{POJO}{Plain Old Java Object}

\newacronym{jmx}{JMX}{Java Management Extensions}

\newacronym{VU}{VU}{Virtual User}

\newacronym{rps}{RPS}{Requests Per Second}

\newacronym[plural=SLOs,firstplural={Service Level Objectives (SLOs)}]{slo}{SLO}{Service Level Objective}

\newacronym[plural=SLAs,firstplural={Service Level Agreements (SLAs)}]{sla}{SLA}{Service Level Agreement}

\newacronym{vm}{VM}{Virtual Machine}

\newacronym{ssh}{SSH}{Secure Shell}

\newacronym{scp}{SCP}{Secure Copy}

\begin{document}

\title{How does an Event Sourcing architecture compare to CRUD systems with an independent audit log, when it comes to scalability, performance and traceability?}
\author{Lukas Karsch}

\begin{titlepage}
    \centering

    \includegraphics[width=0.3\textwidth]{images/HdM_Logo.svg.png}
    \vspace{1cm}

    %LTeX: language=de-DE
    {\large Bachelorarbeit im Studiengang Medieninformatik}
    % {\large Bachelor's Thesis in Computer Science and Media}

    \vspace{1.5cm}

    %LTeX: language=en-US
    % Title
    {\LARGE\bfseries How does an Event Sourcing architecture compare to CRUD systems with an independent audit log, when it comes to scalability, performance and traceability?}
    %LTeX: language=de-DE

    \vspace{0.5cm}
    \rule{\linewidth}{0.5pt}
    \vspace{0.5cm}

    {\large\bfseries Vorgelegt von Lukas Karsch}

    % \vspace{0.3cm}

    % 45259

    \vspace{0.3cm}

    %LTeX: language=de-DE
    {\bfseries an der Hochschule der Medien Stuttgart am 02.03.2026}

    % \vspace{0.2cm}

    % to obtain the degree of Bachelor of Science
    zur Erlangung des akademischen Grades eines Bachelor of Science

    \vfill

    \begin{flushleft}
        \begin{tabular}{ll}
            \textbf{Erstprüfer:}  & Prof. Dr. Tobias Jordine \\[0.3cm]
            \textbf{Zweitprüfer:} & Felix Messner            \\[0.3cm] % TODO akademischer Grad? 
        \end{tabular}
    \end{flushleft}
    %LTeX: language=en-US

\end{titlepage}

\newpage

\pagenumbering{roman}
%LTeX: language=de-DE
\section*{Ehrenwörtliche Erklärung}

Hiermit versichere ich, Lukas Karsch, ehrenwörtlich, dass ich die vorliegende Bachelorarbeit mit dem Titel: "How does an Event Sourcing architecture compare to CRUD systems with an independent audit log, when it comes to scalability, performance and traceability?" selbstständig und ohne fremde Hilfe verfasst und keine anderen als die angegebenen Hilfsmittel benutzt habe. Die Stellen der Arbeit, die dem Wortlaut oder dem Sinn nach anderen Werken entnommen wurden, sind in jedem Fall unter Angabe der Quelle kenntlich gemacht. Ebenso sind alle Stellen, die mit Hilfe eines KI-basierten Schreibwerkzeugs erstellt oder überarbeitet wurden, kenntlich gemacht. Die Arbeit ist noch nicht veröffentlicht oder in anderer Form als Prüfungsleistung vorgelegt worden. Ich habe die Bedeutung der ehrenwörtlichen Versicherung und die prüfungsrechtlichen Folgen (§ 24 Abs. 2 Bachelor-SPO) einer unrichtigen oder unvollständigen ehrenwörtlichen Versicherung zur Kenntnis genommen.

\vspace{1.8cm} % Space for signature

\noindent\rule{7cm}{0.4pt} \\
\noindent \small{Lukas Karsch, 02.03.2026}

%LTeX: language=en-US

\newpage
\tableofcontents

\newpage
\listoffigures
\listoftables
\lstlistoflistings % TODO fix 

\newpage
\printglossaries

\newpage
\pagenumbering{arabic}

\chapter{Introduction}

\section{Motivation}

\section{Research questions}

This thesis provides a quantitative and qualitative comparison between Event Sourcing and traditional CRUD architectures. The primary research question is: \textbf{"How does an Event Sourcing architecture compare to CRUD systems with an independent audit log regarding scalability, performance, and traceability?"} % TODO probably wrong 

To provide a comprehensive answer, the following three sub-questions are addressed:

\begin{enumerate}
    \item \textbf{Performance and Scalability:} How do \acrshort{crud} and \acrshort{es}-\acrshort{cqrs} implementations perform under increasing load, and what are the resulting implications for system scalability and resource efficiency? \label{rs-performance-scalability}
    \item \textbf{Architectural Complexity and Maintainability:} What are the fundamental structural differences between the two approaches, and how do these impact the long-term flexibility and evolution of the codebase? \label{rs-architecture}
    \item \textbf{Historical Traceability:} To what extent can \acrshort{crud} and \acrshort{es}-\acrshort{cqrs} systems accurately and efficiently reconstruct historical states to satisfy business intent and compliance requirements? \label{rs-traceability}
\end{enumerate}

The individual findings from these sub-questions are combined in the conclusion to provide a holistic answer to the primary research question.

\section{Goals and non goals}

\section{Structure of the paper}

\chapter{Basics}

\section{WWW, Web APIs, REST}

The \acrfull{www} is a connected information network used to exchange data. Resources are can be accessed via \glspl{uri} which are transferred using formats like \acrshort{json} or \acrshort{html} via protocols like \gls{http}. \gls{http} is a stateless protocol based on a request-response structure. It supports standardized request types, such as \texttt{GET} and \texttt{POST}, which convey a semantic meaning \parencite{jacobs_architecture_2004}.

Web APIs are interfaces that enable applications to communicate. They use \gls{http} as a network-based API \parencite[138]{fielding_architectural_2000}. Modern APIs typically follow \gls{rest} principles. REST stands for "Representational State Transfer" and describes an architectural style for distributed hypermedia systems \parencite[76]{fielding_architectural_2000}.

REST APIs adhere to principles derived from a set of constraints imposed by the \gls{http} protocol, for example. One such constraint is "stateless communication": Communication between clients and the server must be \emph{stateless}, meaning the client must provide all the necessary information for the server to fully understand the request.

Furthermore, every resource in REST applications must be addressable via a unique ID, which can then be used to derive a \acrshort{uri} to access the resource. Below are some examples for resources and \glspl{uri} which could be derived from them:

\begin{itemize}
    \item Book; ID=1; URI=\texttt{http://example.com/books/1}
    \item Book; ID=2; URI=\texttt{http://example.com/books/2}
    \item Author; ID=100; URI=\texttt{http://example.com/authors/100}
\end{itemize}

The "\acrfull{hateoas}" principle states that resources should be linked to each other. Clients should be able to control the application by following a series of links provided by the server \parencite{tilkov_brief_2007}.

Every resource must support the same interface, usually \gls{http} methods (GET, POST, PUT, etc.) where operations on the resource correspond to one method of the interface. For example, a POST operation on a customer might map to the \texttt{createCustomer()} operation on a service.

Resources are decoupled from their representations. Clients can request different representations of a resource, depending on their needs \parencite{tilkov_brief_2007}: a web browser might request \acrshort{html}, while another server or application might request \acrshort{xml} or \acrshort{json}.

%s TODO explain CRUD here or somewhere else? 

\section{Layered Architecture Foundations}
\label{sec:layered}

Layered Architecture is the most common architecture pattern in enterprise applications. Applications following a layered architecture are divided into \emph{horizontal layers}, with each layer performing a specific role. A standard implementation consists of the following layers:

\begin{itemize}
    \item Presentation: Handles requests and displays data in a user interface or by turning it into representations (e.g. \acrshort{json}).
    \item Business: Encapsulates business logic.
    \item Persistence: Persists data by interacting with the underlying persistence technologies (e.g. SQL databases).
    \item Database: Manages the physical storage, retrieval, and integrity of the application's data records.
\end{itemize}

A key concept in this design is layers of isolation, where layers are "closed", meaning a request must pass through the layer directly below it to reach the next, ensuring that changes in one layer do not affect others.

In a layered application, data flows downwards during request handling and upwards during the response: a request arrives in the presentation layer, which delegates to the business layer. The business layer fetches data from the persistence layer which holds logic to retrieve data, e.g. by encapsulating SQL statements.

The database responds with raw data, which is turned into a \acrlong{dao} (\acrshort{dao}) by the persistence layer. The business layer uses this data to execute rules and make decisions. The result will be returned to the presentation layer which can then wrap the response and return it to the caller. \parencite{richards_software_2015}

The data in layered applications is often times modeled in an \emph{anemic} way. In an \gls{adm}, business entities are treated as only data. They are objects which contain no business logic, only getters and setters. Business logic is entirely contained in the business (or "service") layer. \textcite{anemic-fowler-2003} describes this as an object-oriented \emph{antipattern}. % TODO explain why (this is procedural style, not actually object oriented)

% TODO figure

\section{Domain Driven Design}
\label{sec:ddd}

\acrfull{ddd} is a different architectural approach for applications.  It differs from layered architecture primarily in the way the domain is modelled and the responsibilities of application services.

The core idea of \acrshort{ddd} is that the primary focus of a software project should not be the underlying technologies, but the domain. The domain is the topic with which a software concerns itself. The software design should be based on a model that closely matches the domain and reflects a deep understanding of business requirements. \parencite[8, 12]{evans_domain-driven_2004}

This domain model is built from a \emph{ubiquitous language} which is a language shared between domain experts and software experts. This ubiquitous language is built directly from the real domain and must be used in all communications regarding the software. \parencite[24-26]{evans_domain-driven_2004}

%s TODO here, talk about model driven design -> way from the language to the code 

The software must always reflect the way that the domain is talked about. Changes to the domain and the ubiquitous language must result in an immediate change to the domain model.

When modeling the domain model, the aim should not be to create a perfect replica of the real world. While it should carefully be chosen, the domain model is artificial and forms a selective abstraction which should be chosen for its utility. \parencite[12, 13]{evans_domain-driven_2004}

While \hyperref[sec:layered]{Layered Architecture} organizes code into technical tiers and is typically built on \glspl{adm}, often resulting in the \emph{big ball of mud} antipattern \parencite[V]{richards_software_2015}, \acrshort{ddd} demands a \gls{rdm} where objects incorporate both data and the behavior or rules that govern that data. The code is structured semantically into bounded context and modules which are chosen to tell the "story" of a system rather than its technicalities. \parencite[80]{evans_domain-driven_2004}

Entities (also known as reference objects) are domain elements fundamentally defined by a thread of continuity and identity rather than their specific attributes. Entities must be distinguishable from other entities, even if they share the same characteristics. To ensure consistency and identity, a unique identifier is assigned to entities. This identifier is immutable throughout the object's life. \parencite[65-69]{evans_domain-driven_2004}

Value Objects are elements that describe the nature or state of something and have no conceptual identity of their own. They are interesting only for their characteristics. While two entities with the same characteristics are considered as different from each other, the system does not care about "identity" of a value object, since only its characteristics are relevant. Value objects should be used to encapsulate concepts, such as using an "Address" object instead of distinct "Street" and "City" attributes. Value objects should be immutable. They are never modified, instead they are replaced entirely when a new value is required. \parencite[70-72]{evans_domain-driven_2004}

Using a \gls{rdm} does not mean that there should be no layers, the opposite is the case. \textcite{evans_domain-driven_2004} advocates for using layers in domain driven designs. He proposes the following layers: \parencite[53]{evans_domain-driven_2004}

\begin{itemize}
    \item Presentation: Presents information and handles commands
    \item Application Layer: Coordinates app activity. Does not hold business logic, but delegate tasks and hold information about their progress
    \item Domain Layer: Holds information about the domain. Stateful objects (rich domain model) that hold business logic and rules
    \item Infrastructure layer: Supports other layers. Handles concerns like communication and persistence
\end{itemize}

\textcite[75]{evans_domain-driven_2004} points out that in some cases, operations in the domain can not be mapped to one object. For example, transferring money does conceptually not belong to one bank account. In those cases, where operations are important domain concepts, domain services can be introduced as part of model-driven design. To keep the domain model rich and not fall back into procedural style programming like with an \gls{adm}, it is important to use services only when necessary. Services are not allowed to strip the entities and value objects in the domain of behavior. According to Evans, a good domain service has the following characteristics:

\begin{itemize}
    \item The operation relates to a domain concept which would be misplaced on an entity or a value object
    \item The operation performed refers to other objects in the domain
    \item The operation is stateless
\end{itemize}

\section{CRUD architecture}
\label{sec:crud-architecture}

% TODO refine 

\hyperref[sec:layered]{Layered architectures} are the standard for data-oriented enterprise applications. These applications mostly follow a \acrshort{crud} architecture. \acrshort{crud} is an acronym coined by \textcite{martin_managing_1983} that stands for "Create, Read, Update, Delete". These four actions can be applied to any record of data.

The state of domain objects in a \acrshort{crud} architecture is often mapped to normalized tables on a relational database, though other storage mechanisms maybe used. The application acts on the current state of the data, with all actions (reads and writes) acting on the same data. % TODO cite 

\acrshort{acid} (\acrlong{acid}) are an important feature of \acrshort{crud} applications. They can be guaranteed using transactions, ensuring that data stays consistent and operations are \glslink{atomicity}{atomic}. \parencite[10,11]{bernstein_principles_2009} % TODO more explanation: what is a transaction, commit, rollback, etc 

Databases in CRUD systems are typically normalized. Normalization is a process of organizing data into separate tables, removing redundancies and creating relationships through "foreign keys". It is the best practice for relational databases. There are several normal forms that can be achieved, each form building on the previous one: to achieve the second normal form, the first normal form has to be achieved first. \parencite[203]{martin_managing_1983}

\begin{itemize}
    \item 1NF (First Normal Form): Each table cell contains a single (atomic) value, every record is unique
    \item 2NF (Second Normal Form): Remove partial dependencies by requiring that all \emph{non-key} columns are fully dependent on the primary key
    \item 3NF (Third Normal Form): Removes transitive dependencies by requiring that non-key columns depend \emph{only} on the primary key
    \item Further Normal Forms (4NF, 5NF): Require a table can not be broken down into smaller tables without losing data
\end{itemize}

\section{CQRS Architecture}
\label{sec:cqrs}

\acrfull{cqrs} is an architectural pattern based on the fundamental idea that the models used to update information should be separate from the models used to read information. This approach originated as an extension of Bertrand Meyer’s \acrfull{cqs} principle, which states that a method should either perform an action (a command) or return data (a query), but never both. \parencite[148]{meyer_standard_2006}

\acrshort{cqrs} is different from \acrshort{cqs} in the fact that in \acrshort{cqrs}, objects are split into two objects, one containing commands, one containing queries. \parencite[17]{young_cqrs_2010}

\acrshort{cqrs} applications are typically structured by splitting the application into two paths:

\begin{itemize}
    \item Command Side: Deals with data changes and captures user intent. Commands tell the system what needs to be done rather than overwriting previous state. Commands are validated by the system before execution and can be rejected. \parencite[11,12]{young_cqrs_2010}
    \item Read Side: Strictly for reading data. The read side is not allowed to modify anything in the primary data store. The read side typically stores \glspl{dto} in its own data store that can directly be returned to the presentation layer. \parencite[20]{young_cqrs_2010}
\end{itemize}

In a CQRS architecture, the read side typically updates its data asynchronously by consuming notifications or events generated by the write side. Because the models for updating and reading information are strictly separated, a synchronization mechanism is required to ensure the read store eventually reflects the changes made by commands. This usually leads to stale data on the read side.

Each read service independently updates its model by consuming notifications or events published by the write side, allowing the read model to store optimized, denormalized views on the data. \parencite[23]{young_cqrs_2010}

%s TODO figure 

\section{(Eventual) Consistency}

% TODO have to talk about "read your writes"? 
% TODO maybe move above CRUD and CQRS 

\textcite{gray_dangers_1996} explain that large-scale systems become unstable if they are held consistent at all times according to \acrshort{acid} principles. This is mostly due to the large amount of communication necessary to handle atomic transactions in distributed systems. To address these issues, modern distributed systems often adopt the \acrshort{base} (\acrlong{base}) model which explicitly trades off isolation and strong consistency for availability. Eventually consistent systems are allowed to exist in a so-called "soft state" which eventually converges through the use of synchronization mechanisms over time rather than being strongly consistent at all times. \parencite{braun_tackling_2021, vogels_eventually_2009} This creates an inconsistency window in which data is not consistent across the system. During this window, stale data may be read. \parencite{vogels_eventually_2009}

\section{Event Sourcing and event-driven architectures}
\label{sec:event-sourcing}

Event driven architecture is a design paradigm where systems communicate via the production and consumption of events. Events are records of changes in the system's domain. \parencite{michelson_event-driven_2006} This approach allows for a high degree of loose coupling, as the system publishing an event does not need to know about the recipient(s) or how they will react. These architectures offer excellent horizontal scalability and resilience, as individual system components can fail or be updated without bringing down the entire network. \parencite{fowler_event_2005}

Event Sourcing is an architectural pattern within the landscape of event driven architectures. Event-sourced systems ensure that all changes to a system's state are captured and stored as an ordered sequence of domain events. Unlike traditional persistence models that overwrite data and store only the most recent state, event sourcing maintains an immutable record of every action taken over time. These events are persisted in an append-only event store, which serves as the principal source of truth from which the current system state can be derived. \parencite[457,458]{kleppmann_designing_2017}

The current state of any entity in such a system can be rebuilt by replaying the history of events from the log, starting from an initial blank state. \parencite{fowler_event_2005} To address the performance costs of replaying thousands of events for every request, developers implement projections or materialized views, which are read-only, often denormalized versions of the data optimized for specific queries. \parencites{malyi_developing_2024}[461,462]{kleppmann_designing_2017} This separation of concerns is frequently managed by pairing event sourcing with the \hyperref[sec:cqrs]{\acrlong{cqrs} (\acrshort{cqrs})} pattern, which physically divides the data structures used for reading from those used for writing state changes. \parencite[50]{young_cqrs_2010} % TODO Snapshots erklären; Projection = "ephemeral", kann leicht neu gebaut werden 

\section{Traceability and auditing in IT systems}

Traceability and auditing are legal requirements across various sectors, as they are derived from federal laws and regulations intended to protect the integrity and confidentiality of sensitive data. Organizations implement these mechanisms to stay compliant with mandates that require a verifiable, time-sequenced history of system activities to support oversight and forensic reviews. In the U.S. financial sector, for example, 17 CFR § 242.613 requires the establishment of a consolidated audit trail to track the complete lifecycle of securities orders, documenting every stage from origination and routing to final execution. \parencite{us_securities_and_exchange_commission_17_2012}

\subsection{Audit Logs}
\label{sec:audit-log}

An audit log (often called audit trail) is a chronological record which provides evidence of a sequence of activities on an entity. \parencite{committee_on_national_security_systems_national_2010} In information security, the audit log stores a record of system activities, enabling the reconstruction of events. \parencite{atis_committee_atis_2013} A trustworthy audit log in a system can guarantee the principle of traceability which states that actions can be tracked and traced back to the entity who is responsible for them. \parencite[266]{joint_task_force_interagency_working_group_security_2020}

\textcite{fowler_audit_2004} describes an audit log as simple and effective way of storing temporal information. Changes are tracked by writing a record indicating \emph{what} changed \emph{when}. A basic implementation of an audit log can have many forms, for example a text file, database tables or \acrshort{xml} documents. Fowler also mentions that while the audit log is easy to write, it is harder to read and process. While occasional reads can be done by eye, complex processing and reconstruction of historical state can be resource-intensive.

\subsection{Event Streams as a Basis for Traceability}

While \hyperref[sec:audit-log]{traditional audit logs} are often implemented as secondary systems that capture state changes, event-driven architectures, such as those utilizing Event Sourcing, turn an event stream into the primary source of truth. In this context, an event stream is not just a diagnostic tool but an exact, chronological sequence of intent-driven records.

As established in \autoref{sec:event-sourcing}, every state change is captured as a discrete event. Because these events are immutable and append-only, they provide a natural foundation for the principle of traceability. Unlike traditional "state-based" auditing, where the system might only record that a value changed from $A$ to $B$, an event stream captures the specific domain context. Tthe \emph{intent} behind a change is semantically conveyed through the event type. For example, while a traditional audit log might simply record a status update to \texttt{CLOSED}, an event-sourced system distinguishes between an \texttt{AccountDeletedByUser} event and an \texttt{AccountTerminatedForInactivity} event. This inherent metadata provides an exhaustive audit trail without the need for additional logging logic.

\textcite{fowler_event_2005} notes that because the event log is complete, the system can perform \emph{Temporal Queries}, effectively "time-traveling" to reconstruct the exact state of the system at any historical checkpoint. This makes event streams particularly robust for forensic reviews and regulatory compliance, as they eliminate the "information loss" associated with traditional database overwrites. In the context of the legal requirements discussed in the previous section, the event stream serves as a sequence of actions that satisfies the need for a verifiable, time-sequenced history.

\subsection{Rebuilding state from an audit log and an event stream}
% TODO remove this section?
\section{Scalability of systems}

\chapter{Related Work}

\chapter{Proposed Method}

This thesis aims to provide a fair, quantitative comparison of \acrshort{crud} and \acrshort{cqrs} / \acrshort{es} architectures. To achieve this, the architectures should be applied not only to the same domain, but to the exact same requirements. The implementations can then be tested against the same \glspl{contract-test}.

This chapter will first present the requirements and the domain for the actual application, then outline metrics and comparison methods.

\section{Project requirements}

The applications will implement a course enrollment and grading system which might for example be used in universities. Core features include:

\begin{itemize}
    \item Professors can create courses and lectures
    \item Students can enroll and disenroll from lectures
    \item Professors can enter grades
    \item Students can view their current and past lectures
    \item Students can view their credits
\end{itemize}

\subsection{Entities}
\label{sec:entities}

Two types of users exist in the domain: professors and students. Their personal information is not relevant for this thesis, which is why only their first and last name are stored for presentation reasons. The student additionally has a semester.

Professors can create courses. Courses have a name, a description, an amount of credits they yield, a minimum amount of credits required to enroll and can have a set of courses as prerequisites.

Courses are the "blueprints" for lectures. Lectures are the "implementation" of a course for a semester. Each lecture created from a course yields the course's amount of credits and has the requirements specified by the course. Lectures have a lifecycle: they can be in draft state, open for enrollment, in progress, finished or archived. A lecture has a list of time slots and a maximum amount of students that can enroll.

A lecture can have several assessments. Each assessment has a type. The professor can enter grades for a student and an assessment. Grades are integers in the range of 0 to 100. Credits are awarded to a student as soon as they completed all assessments for a lecture with a passing grade (grade higher than 50).

\subsection{Business rules}

Relationships and business rules in this system are deliberately chosen complex, involving many relationships between \hyperref[sec:entities]{entities} and intricate validation rules. This approach was adopted in order to be able to make realistic assumptions about the research question by evaluating a project that closely resembles complex, real-world scenarios.

The following list presents a selection of business rules which were implemented.

\begin{itemize}
    \item Existence checks: any requests including references to entities will fail if the references entities do not exist.
    \item Requests leading to conflicts, for example creating a lecture with overlapping time slots, will fail.
    \item When a student tries enrolling to a lecture which is already full, they will be put on a waitlist.
    \item When a student disenrolls from a lecture, the next eligible student (higher semesters are preferred) will be enrolled.
    \item Actions on a lecture can only be performed during the appropriate lifecycle state (enrolling only when the lifecycle is "open for enrollment", grades can only be assigned when the lecture is "finished").
\end{itemize}

\subsection{Contract Tests}
\label{sec:contract-tests}

To ensure both implementations adhere to the business rules, an extensive test suite was set up. While the internals of the implementations are vastly different architecturally and conceptually, they both have the same public \gls{api}. This makes it possible to run the same test suite on both apps by sending \gls{http} requests and verifying their responses. The test suite includes integration tests for all \gls{api} endpoint covering both regular and edge-case (error) scenarios to ensure that the \acrshort{crud} and \acrshort{es}-\acrshort{cqrs} application exhibit identical state transitions and error behaviors. \hyperref[sec:contract-test-implementation]{Section \ref*{sec:contract-test-implementation}} outlines the implementation of those tests in detail.

\section{Performance}

\hyperref[rs-performance-scalability]{Research question 1} evaluates and compares the performance characteristics of both architectural patterns. To achieve this, load tests are executed on selected endpoints. The theoretical foundations of load testing, the environmental constraints, and the data collection methodology are described in this section.

\subsection{Theoretical Load Testing Foundations}
\label{sec:load-test-theory}

To provide accurate performance metrics, load testing is performed on both implementations based on the methodology defined by \textcite[10-17]{kleppmann_designing_2017}. Load is characterized using "load parameters," which vary depending on the system's nature. For this study, the primary load parameter is the number of concurrent requests to the web server. \parencite[11]{kleppmann_designing_2017}

The tests measure \textbf{\acrfull{rps}} and client-side response times. \textcite[15,16]{kleppmann_designing_2017} emphasizes the importance of client-side measurement to account for "queueing delays." As server-side processing is limited by hardware resources (e.g., CPU cores), requests may be stalled before processing begins. Server-side metrics often exclude this wait time, leading to an overly optimistic view of performance. Consequently, the load-generating client must utilize an "open model," sending requests without waiting for previous ones to complete, to simulate realistic concurrent user behavior.

This thesis adopts the approach of keeping system resources constant while measuring performance fluctuations under varying load intensities. \parencite[13]{kleppmann_designing_2017}

To evaluate results, the arithmetic mean is avoided as it obscures the experience of typical users and the impact of outliers. Instead, this thesis uses \glspl{percentile}. The \gls{median} (P50) serves as the metric for "typical" response time. However, to capture the experience of users facing significant delays—often caused by data-intensive operations—it is critical to measure "\glspl{tail-latency}" via the P95 percentile. High percentiles identify the performance of outliers which, despite being a numerical minority, often represent the most valuable or complex operations. \parencite[14-16]{kleppmann_designing_2017}

\subsection{Service Level Objectives}
\label{sec:slo}

While \glspl{sla} are agreements with users regarding uptime and performance, \glspl{slo} are the technical targets used by engineers to meet those requirements. \parencite{beyer_site_2016} This thesis attempts to define realistic \acrshortpl{slo} to establish a "breaking point" for each architecture.

Following \textcite[135]{nielsen_usability_1993}, a response time of 100ms is the threshold for human perception of "instant" feedback. This serves as the baseline for the following targets:

\begin{itemize}
    \item \textbf{Latency \acrshort{slo}}: All endpoints must maintain a client-side P95 latency of $\le$100ms to ensure the system feels "instant" for 95\% of requests. \label{slo-latency}
    \item \textbf{Freshness \acrshort{slo}}: In the Event Sourcing implementation, the asynchronous nature of projections introduces a lag. All writes must be reflected in the PostgreSQL read-model within $\le$100ms to ensure eventual consistency remains imperceptible. This \acrshort{slo} applies only to the ES-CQRS implementation. \label{slo-freshness}
    \item \textbf{Reliability \acrshort{slo}}: Both implementations must maintain a failure rate of <0.1\% under stress. \label{slo-reliability}
\end{itemize}

\subsection{Comparability and Environment}

The test environment and scenarios are defined as code to ensure reproducibility. Tests are executed in an isolated environment with fixed hardware allocations as specified in \autoref{table:hardware-specs}.

\begin{table}[htp!]
    \small
    \centering
    \begin{tabularx}{\linewidth}{lX}
        \toprule
        \textbf{Component} & \textbf{Specification}                                                      \\ \midrule
        CPU                & 13th Gen Intel(R) Core(TM) i7-13700H. 14 Cores, 20 total threads. Max. 5GHz \\
        RAM                & 32GB DDR4 (2x16GB), 3200 MT/s                                               \\
        Hard Drive         & SanDisk Plus SSD 1TB 2.5" SATA 6GB/s                                        \\
        \bottomrule
    \end{tabularx}
    \caption{Hardware specifications for the performance evaluation machine}
    \label{table:hardware-specs}
\end{table}

The physical host provisions two \glspl{vm}: the "client VM" for load generation and the "server VM" for the application and its dependencies (PostgreSQL and Axon Server). While hosting both on one physical machine makes network latency negligible, the "queueing delay" remains measurable at the client level, allowing for the identification of request queues building up on the server, indicating bottlenecks.

\subsection{Test Execution}
\label{sec:test-execution-method}

The tool \hyperref[sec:k6]{k6} is used to generate load. The client follows an "open model" via k6's arrival-rate executors to decouple request rate from response times, as discussed in \autoref{sec:load-test-theory}.

Each test follows a specific pattern:
\begin{itemize}
    \item \textbf{Ramp-up}: Linear increase from 0 to the target \acrshort{rps} over 20 seconds.
    \item \textbf{Steady State}: Constant target \acrshort{rps} for 80 seconds.
    \item \textbf{Ramp-down}: Linear decrease to 0 \acrshort{rps} over 20 seconds.
\end{itemize}

A \emph{test configuration} (the combination of a script and target \acrshort{rps}) is executed at least 25 times. Between runs, the application and its dependencies (PostgreSQL/Axon Server) are restarted to ensure independent results. Client-side metrics are captured in \acrshort{json} format, while server-side resource consumption (CPU \& RAM usage) is collected via \hyperref[sec:actuator]{Spring Boot Actuator}.

\subsection{Collected Metrics}
\label{sec:collected-metrics}

All metrics collected during load testing are listed in \autoref{table:collected-metrics}. The column "Metric" shows the name that will be used to refer to this metric from now on. "Location" describes where the metric is being recorded.

As described in \autoref{sec:load-test-theory}, not just the average latency is measured, but \glspl{percentile} are used to accurately report the number of users experiencing the respective latency.

While internal metrics, like CPU usage, RAM usage, the number of database connections ($hikari\_connections$) and the number of Tomcat worker threads are not measures for user-perceived performance, they can give an indicator about bottlenecks inside the applications.

Even though latencies are measured on both the client and the server, the client latency will be used primarily when visualizing results, as it is the latency users perceive when interacting with the applications. The server latency is recorded because it can help identify queueing delays by exposing differences in server and client latencies which exceed a factor of 1.1x.

$axon\_storage\_size$ is only recorded for the ES-CQRS application. $postgres\_size$ can be recorded for both applications. The CRUD implementation stores its entities and audit log in PostgreSQL, while the ES-CQRS implementation uses PostgreSQL as a secondary data store where denormalized projections and lookup tables live. These metrics are relevant when assessing a system's long-term performance, scalability and maintainability.

\begin{table}[htp!]
    \small
    \centering
    \begin{tabularx}{\linewidth}{lXl}
        \toprule
        \textbf{Metric}       & \textbf{Description}                                 & \textbf{Location} \\ \midrule
        $latency\_avg$        & Average (arithmetic mean) latency                    & Server, Client    \\
        \addlinespace
        $latency\_p50$        & 50th \gls{percentile} Latency (\gls{median})         & Server, Client    \\
        \addlinespace
        $latency\_p95$        & 95th \gls{percentile} Latency                        & Server, Client    \\
        \addlinespace
        $latency\_p99$        & 99th \gls{percentile} Latency                        & Server, Client    \\
        \addlinespace
        $cpu\_usage$          & CPU usage of the Server process                      & Server            \\
        \addlinespace
        $ram\_usage\_heap$    & Usage of Heap memory                                 & Server            \\
        \addlinespace
        $ram\_usage\_total$   & Usage of total memory                                & Server            \\
        \addlinespace
        $hikari\_connections$ & Number of Data Source connections                    & Server            \\
        \addlinespace
        $tomcat\_threads$     & Number of Tomcat worker threads                      & Server            \\
        \addlinespace
        $postgres\_size$      & Size of PostgreSQL database                          & Server            \\
        \addlinespace
        axon\_storage\_size   & Size of Axon storage, incl. Event and Snapshot store & Axon Server       \\
        \bottomrule
    \end{tabularx}
    \caption{All metrics collected during load testing}
    \label{table:collected-metrics}
\end{table}

\subsection{Visualizing Results}

Data is visualized using box plots and line graphs. Box plots are used to show the distribution of latencies, while line graphs illustrate performance changes as \acrshort{rps} increases. Per scientific standards, error bars are used to represent the variability of the measurements across the test runs.

\subsection{Performance Implications (TODO)}

The results collected during load testing show how the applications perform under varying load and different situations. Writes and reads are both tested using endpoints which require a varying degree of invariant checking or entity JOINS.

This section will describe the implications of the collected results on the predicted scalability of systems. To do so, literature is analyzed which describes the correlation between performance, resource usage and scalability. We analyze how quickly a system reaches its bottlenecks and in which cases which system is better scalable.

Scalability also depends on the architecture of a software. For example, an application designed to separate reads and writes allows to separate the application into microservices more easily than a monolith. This view on scalability will be described further when presenting the results of architectural metrics.

\subsection{Load Testing Scenarios (TODO)}

This subsection lists every load testing scenario and explains why its results are relevant to the research question.

\begin{itemize}
    \item Create Courses Simple
    \item Create Courses with Prerequisites
    \item Get lectures
    \item Enrollment
    \item Create lecture and read (time to consistency)
    \item More?
\end{itemize}

\section{Flexibility - Architectural Metrics (TODO)}

This section describes various architectural metrics which are established in literature and used to assess the flexibility and quality of a software architecture.

\section{Traceability}

\section{Technologies}
\label{sec:technologies}

This section describes all technologies used for the implementation and evaluation of the two applications. % TODO dependency matrix in appendix 

\subsection{SpringBoot}

SpringBoot \footnote{\href{https://spring.io/projects/spring-boot}{SpringBoot}} is an open-source, opinionated framework for developing enterprise Java applications. It is based on Spring Framework \footnote{\href{https://spring.io/projects/spring-framework}{Spring Framework}}, which is a platform aiming to make Java development "quicker, easier, and safer for everybody" \parencite{broadcom_inc_why_2026}. At Spring Framework's core is the Inversion of Control (IoC) container. The objects managed by this container are referred to as \textit{Beans}. While the term originates from the JavaBeans specification, a standard for creating reusable software components, Spring extends this concept by taking full responsibility for the lifecycle and configuration of these objects \parencite[Chapter~1.1]{walls_spring_2016}. Instead of a developer manually instantiating classes using the \texttt{new} operator, the container "injects" required dependencies at runtime. This process is known as Dependency Injection. \parencite[Chapter~1]{deinum_spring_2023}. Spring offers support for several programming paradigms: reactive, event-driven, microservices and serverless. \parencite{broadcom_inc_why_2026}

SpringBoot builds on top of the Spring platform by applying a "convention-over-configuration" approach, intended to minimize the need for configuration. In a 2023 survey by JetBrains, SpringBoot was the most popular choice of web framework. \parencite{jetbrains_java_2023}

Spring Boot starters are specialized dependency descriptors designed to simplify dependency management by aggregating commonly used libraries into feature-defined packages. Rather than requiring developers to manually identify and maintain a list of individual group IDs, artifact IDs, and compatible version numbers for every necessary library, starters use transitive dependency resolution to pull in all required components under a single entry. To quickly bootstrap a web application, a developer can simply add the \javaname{spring-boot-starter-web} dependency to their Maven or Gradle build file. By requesting this specific functionality, Spring Boot automatically includes essential dependencies such as Spring MVC, Jackson for JSON processing, and an embedded Tomcat server, ensuring that all included libraries have been tested together for compatibility. This approach shifts the developer's focus from managing individual JAR files to simply defining the high-level capabilities the application requires, minimizing configuration overhead and reducing risk of version mismatches. \parencite[Chapter~1.1.2]{walls_spring_2016}
% TODO code example for IoC container / dependency injection 

\subsection{JPA}
\label{sec:jpa}

\acrfull{jpa} \footnote{\href{https://jakarta.ee/specifications/persistence/}{JPA}}, formerly Java Persistence \gls{api} is a Java specification which provides a mechanism for managing persistence and object-relational mapping (\acrshort{orm}). \glspl{orm} act as a bridge between the relational world of SQL databases and the object-oriented world of Java.

Instead of writing SQL to create the database schema, entities can be described using special Java classes (defined by annotations or \acrshort{xml} configurations) which can be mapped to an SQL schema. \acrshort{jpa} allows querying the database for these entities in a type-safe way by providing a range of helpful query methods on JPA repositories, for example \texttt{findAll()} or \texttt{findById(UUID id)}. This removes the need to write "low-level", database-specific SQL for basic \acrshort{crud} operations. Complex data retrieval is also possible with \acrshort{jpa} using the \acrfull{jpql}, which is an object-oriented, database-agnostic query language.

When using \acrshort{jpa} with SpringBoot by including the \javaname{spring-boot-starter-data-jpa} dependency, \emph{Hibernate} \footnote{\href{https://hibernate.org/orm/}{Hibernate}} is used as implementation of the \acrshort{jpa} standard. \parencite[Chapter~1]{bauer_java_2016}

\subsection{PostgreSQL}
\label{sec:postgresql}

PostgreSQL \footnote{\href{https://www.postgresql.org/}{PostgreSQL}} is an open-source relational database system which has been in active development for over 35 years. Thanks to its reliability, robustness and performance, it has a strong earned reputation. \parencite{postgresql_global_development_group_postgresql_2026} PostgreSQL is designed for a wide range of workloads and can handle many tasks thanks to its extensibility and large suite of extensions, such as the popular PostGIS extension for storing and querying geospatial data. \parencite{postgis_psc_postgis_2023}

\subsection{Jackson}

Jackson \footnote{\href{https://github.com/FasterXML/jackson}{Jackson}} is a high-performance, feature-rich \acrshort{json} processing library for Java. It is the default \acrshort{json} library used within the Spring Boot ecosystem. Its primary purpose is to provide a seamless bridge between Java objects and JSON data through three main processing models: the Streaming API for incremental parsing, the Tree Model for a flexible node-based representation, and the most commonly used Data Binding module. This data binding capability allows developers to automatically convert (\emph{marshal}) Java \glspl{pojo} into JSON and vice versa (\emph{unmarshal}) with minimal configuration. Beyond its speed and efficiency, Jackson is highly extensible, offering modules to handle complex Java types like Java 8 Date/Time and Optional classes. Jackson also supports various other data formats such as XML, YAML and CSV. \parencite{oracle_jackson_nodate, fasterxml_jackson_2025}

\subsection{Axon}
\label{sec:axon}

Axon Framework \footnote{\href{https://www.axoniq.io/framework}{Axon Framework}} is an open-source Java framework for building event-driven applications. Following the \acrshort{cqrs} and event-sourcing pattern, Commands, Events and Queries are the three core message types any Axon application is centered around. Commands are used to describe an intent to change the application's state. Events communicate a change that happened in the application. Queries are used to request information from the application.

Axon also supports \acrlong{ddd} by providing tools to manage entities and domain logic. \parencite{axoniq_introduction_2025,axoniq_messaging_2025}

Axon Server \footnote{\href{https://www.axoniq.io/server}{Axon Server}} is a platform designed specifically for event-driven systems. It functions as both a high-performance Event Store and a dedicated Message Router for commands, queries, and events. By bundling these responsibilities into a single service, Axon Server replaces the need for separate infrastructures such as a relational database for events and a message broker like Kafka or RabbitMQ for communication. Axon Server is designed to seamlessly integrate with Axon Framework. When using the Axon Server Connector, the application automatically finds and connects to the Axon Server. It is then possible to use the Axon server without further configuration. \parencite{axoniq_introduction_2025-1,axoniq_axon_2025} % TODO book source 

\subsubsection*{Command dispatching}

Command dispatching is the starting point for handling a command message in Axon. Axon handles commands by routing them to the appropriate command handler. The command dispatching infrastructure can be interacted with using the low-level \keyw{CommandBus} and a more convenient \keyw{CommandGateway} which is a wrapper around the \keyw{CommandBus}.

\keyw{CommandBus} is the infrastructure mechanism responsible for finding and invoking the correct command handler. At most one handler is invoked for each command; if no handler is found, an exception is thrown.

Using \keyw{CommandGateway} simplifies command dispatching by hiding the manual creation of \keyw{CommandMessages}. The gateway offers two main methods for synchronous and asynchronous patterns. The \keyw{send} method returns a \keyw{CompletableFuture}, which is an asynchronous mechanism in Java. If the thread needs to wait for the command result, the \keyw{sendAndWait} method can be used.

In general, a handled command returns \keyw{null}, if handling was successful. Otherwise, a \keyw{CommandExecutionException} is propagated to the caller. While returning values from a command handler is not forbidden, it is used sparsely as it contradicts with CQRS semantics. One exception: command handlers which \emph{create} an aggregate typically return the aggregate identifier. \parencite{axoniq_command_2025,axoniq_infrastructure_2025}

\subsubsection*{Query Handling}
Before a query is handled, Axon dispatches it through its messaging infrastructure. Just like the command infrastructure, Axon offers a low-level \keyw{QueryBus} which requires manual query message creation and a more high-level \keyw{QueryGateway}.

In contrast to command handling, multiple query handlers can be invoked for a given query. When dispatching a query, callers can decide whether they want a single result or results from all handlers. When no query handler is found, an exception is thrown.

The \keyw{QueryGateway} includes different dispatching methods. For regular "point-to-point" queries, the \keyw{query} method can be used. Subscription queries are queries where callers expect an initial result and continuous updates as data changes. These queries work well with reactive programming. For large result sets, streaming queries should be used. The response returned by the query handler is split into chunks and streamed back to the caller.
All query methods are asynchronous by nature and return Java's \keyw{CompletableFuture}. \parencite{axoniq_query_2025}

\subsubsection*{Aggregates}
\label{sec:aggregates}

An aggregate is a core concept of \acrfull{ddd}. In Axon, an aggregate defines a consistency boundary around domain state and encapsulates business logic. Aggregates are the primary place where domain invariants are enforced and where commands that intend to change domain state are handled.

Aggregates define command handlers using methods or constructors annotated with \keyw{@CommandHandler}. These handlers receive commands and decide whether they are valid according to domain rules. If a command is accepted, the aggregate emits one or more domain events describing \emph{what} happened. Command handlers are responsible only for decision-making; they must not directly mutate the aggregate’s state. Instead, all state changes must occur as a result of applying events.

Every aggregate is typically annotated with \keyw{@Aggregate} and must declare exactly one field annotated with \keyw{@AggregateIdentifier}. This identifier uniquely identifies the aggregate instance. Axon uses it to route incoming commands to the correct aggregate and to load the corresponding event stream when rebuilding aggregate state.

By default, Axon uses event-sourced aggregates. This means that aggregates are not persisted as a snapshot of their fields. Instead, their current state is reconstructed by replaying all previously stored events. Methods annotated with \keyw{@EventSourcingHandler} are called by Axon during this replay process to update the aggregate’s internal state based on event data. Since events represent facts that already occurred, event sourcing handlers must not contain business logic or make decisions.

Axon also supports multi-entity aggregates. In this model, an aggregate may contain child entities that participate in command handling. Such entities are registered using \keyw{@AggregateMember}, and each entity must define a unique identifier annotated with \keyw{@EntityId}. Based on this identifier, Axon is able to route commands to the correct entity instance within the aggregate. \parencite{axoniq_multi-entity_2025}

\subsubsection*{External Command Handlers}
\label{sec:external-command-handlers}

Often, command handling functions are placed directly inside the aggregate. However, this is not required and in some cases it may not be desirable or possible to directly route a command to an aggregate. Thus, any object can be used as a command handler by including methods annotated with \keyw{@CommandHandler}. One instance of this command handling object will be responsible for handling \emph{all} commands of the command types it declares in its methods.

In these external command handlers, aggregates can be loaded manually from Axon's repositories using the aggregate's ID. Afterward, the \keyw{execute} function can be used to execute commands on the loaded aggregate. \parencite{axoniq_command_2025-1}

\subsubsection*{Set-based validation}

TO DO $\rightarrow$  move section from implementation to here

\subsubsection*{Events}
\label{sec:axon-events}

Event handlers are methods annotated with \keyw{@EventHandler} which react to occurrences within the app by handling Axon's event messages. Each event handler specifies the types of events it is interested in. When no handler for a given event type exists in the application, the event is ignored. \parencite{axoniq_event_2025}

Axon's \keyw{@EventBus} is the infrastructure mechanism dispatching events to the subscribed event handlers. Event stores offer these functionalities and additionally persist and retrieve published events. \parencite{axoniq_event_2025-1}

Event processors take care of the technical part aspects of event processing. Axon's \keyw{EventBus} implementations support both subscribing and tracking event processors. \parencite{axoniq_event_2025-1} Subscribing event processors subscribe to a message source, which delivers (pushes) events to the processor. The event is then processed in the same thread that published the event. This makes subscribing event processors suitable for real-time updates of models. However, they can only be used to receive current events and do not support event replay. Additionally, as they run on the same thread, they can not be parallelized. \parencite{axoniq_subscribing_2025}

Tracking event processors, which a type of streaming event processors, read (pull) events to be processed from an event source. They run decoupled from the publishing thread, making them parallelizable. These event processors use tracking tokens track their position in the event stream. Tracking tokens can be reset and events can be replayed and reprocessed. Tracking event processors are the default in Axon and recommended for most ES-CQRS use cases. \parencite{axoniq_streaming_2025}

Subscribing event processors can be configured using SpringBoot's \javaname{application.properties} file or through Java configuration classes.

\subsubsection*{Sagas}
\label{sec:sagas}

In Axon, Sagas are long-running, stateful event handlers which not just react to events, but instead manage and coordinate business transactions. For each transaction being managed, one instance of a Saga exists. A Saga, which is a class annotated with \keyw{@Saga} has a lifecycle that is started by a specific event when a method annotated with \keyw{@StartSaga} is executed. The lifecycle may be ended when a method annotated with \keyw{@EndSaga} is executed; or conditionally using \keyw{SagaLifecycle.end()}. A Saga usually has a clear starting point, but may have many different ways for it to end. Each event handling method in a Saga must additionally have the \keyw{@SagaEventHandler} annotation. \parencite{axoniq_saga_2025}

The way Sagas manage business transactions is by sending commands upon receiving events. They can be used when workflows across several aggregates should be implemented; or to handle long-running processes that may span over any amount of time. \parencite{axoniq_saga_2025} For example, the lifecycle of an order, from being processed, to being shipped and paid, is a process that usually takes multiple days. A use case like this is typically implemented using Sagas.

A Saga is associated with one or more association values, which are key-value pairs used to route events to the correct Saga instance. A \keyw{@StartSaga} method together with the \keyw{@SagaEventHandler(associationProperty="aggregateId")} automatically associates the Saga with that identifier. Additional associations can be made programmatically, by calling \keyw{SagaLifecycle.associateWith()}. Any matching events are then routed to the Saga. \parencite{axoniq_saga_2025-1}

For example, a Saga managing an order's lifecycle may be started by an \keyw{@OrderPlaced} event and associated with the \keyw{orderId}. It can then issue a \keyw{CreateInvoiceCommand} using an \keyw{invoiceId} generated inside the event handler. The Saga then associates itself with this ID to be notified of further events regarding this invoice, such as an \keyw{InvoicePaidEvent}.

% TODO Show command and query gateway and illustrate example flow through an Axon application. 

\subsection{Testing}
\label{sec:testing}

To ensure functionality of the applications, unit and integration tests were implemented using various testing libraries like JUnit as the testing platform, \gls{restassured} for making and asserting \gls{http} calls, Mockito for unit testing and ArchUnit for architecture tests. This section describes all mentioned technologies.

JUnit \footnote{\href{https://docs.junit.org/5.11.0/user-guide/index.html}{JUnit 5}} is an open-source testing framework for Java. It offers a structured way of writing tests, driven by lifecycle methods like \texttt{beforeEach} or \texttt{afterAll}. Tests are annotated with \texttt{@Test}. They can also be parametrized and run repeatedly. Results can be asserted using assertion methods like \texttt{assertTrue()}. \parencite{noauthor_junit_nodate}

REST Assured \footnote{\href{https://rest-assured.io/}{REST Assured}} is a Java library that provides a highly fluent \acrshort{dsl} for testing and validating REST APIs in a readable, chainable style. It allows complex assertions to be written inline using \gls{groovy} expressions, making it easy to deeply verify JSON responses beyond simple field checks. \parencite{restassured-documentation}

The below code example shows how one might use a \gls{groovy} expression to find and validate a path in the returned JSON object:

\begin{lstlisting}[caption={Validating JSON path using Rest Assured},captionpos=b]
RestAssured.when()
    // omitted request 
    .then()
    .body(
        "data.grades.find { it.combinedGrade == 0 }.credits", 
        equalTo(0)
    );
\end{lstlisting}

Here, the path \texttt{data.grades} of the returned JSON object is expected to be an array. The array is filtered using a \gls{gpath} with a closure to find the first entry where \texttt{combinedGrade} equals 0. Then, this entry's \texttt{credits} field is extracted and validated using the \texttt{equalTo(0)} matcher.

% TODO Mockito, ArchUnit 

\subsection{SpringBoot Actuator}
\label{sec:actuator}

Spring Boot Actuator \footnote{\href{https://docs.spring.io/spring-boot/reference/actuator/index.html}{SpringBoot Actuator}} is a tool designed to help monitor and manage Spring Boot applications running in a production environment. It provides several built-in features that allow developers to check the status of the application, gather performance data, and track \gls{http} requests. These features can be accessed using either \gls{http} or \acrshort{jmx} (\acrlong{jmx}), which is a standard Java management technology. By using Actuator, developers can quickly see if an application is running correctly without the need to write custom monitoring code.

The most common way to use Actuator is through its "endpoints", which are specific web addresses that provide different types of information. For example, the health endpoint shows whether the application and its connected services, like databases, are functioning correctly, while the metrics endpoint displays detailed data on memory and CPU usage. Beyond the standard options, developers can also create their own custom endpoints or connect the data to external monitoring software to visualize how an application is performing over time.

Actuator can be enabled in a Spring Boot project by including the \javaname{spring-boot-starter-actuator} dependency. \parencite{broadcom_inc_production-ready_2026}

\subsection{Prometheus}

Prometheus \footnote{\href{https://prometheus.io/docs/introduction/overview/}{Prometheus}} is an open-source systems monitoring toolkit that was originally developed at SoundCloud and is now a project of the Cloud Native Computing Foundation. It is primarily used for collecting and storing multidimensional metrics as time-series data, meaning information is recorded with a timestamp and optional key-value pairs called labels. The system is designed for reliability and is capable of scraping data from instrumented jobs and web servers, storing it in a local time-series database, and triggering alerts based on predefined rules when specific thresholds are met. Through its powerful functional query language, PromQL, developers can aggregate and visualize performance data. \parencite{prometheus_authors_prometheus_2026,prometheus-overview-2026}

To collect and export \hyperref[sec:actuator]{Actuator} metrics specifically for Prometheus, the \javaname{micrometer-registry-prometheus} dependency must be included in the classpath. \parencite{vmware_inc_micrometer_nodate} Access to the metrics is granted by including "prometheus" in the list of exposed web endpoints within the application's configuration properties. Once these components are in place, the metrics are automatically formatted for consumption and can be scraped by a Prometheus server. \parencite{broadcom_inc_metrics_2026}

\subsection{Docker}

\gls{docker} \footnote{\href{https://docs.docker.com/}{Docker}} is a platform used for developing and deploying applications. It is designed to separate software from the underlying infrastructure, allowing for faster delivery and consistent environments.

\gls{docker}'s capabilities are centered around the use of containers, which are lightweight and isolated environments. Each container is packaged with all necessary dependencies required for an application to run, ensuring it operates independently of the host system. These workloads can be executed across different environments, such as local computers, data centers, or cloud providers, ensuring high portability. \parencite{what-is-docker}

A \gls{dockerfile} is a text-based document containing a series of instructions for assembling a Docker image. Each command in this file results in the creation of a layer in the image, making the final template efficient and fast to rebuild. These images serve as read-only blueprints from which runnable instances, or containers, are created. \parencite{writing-a-dockerfile}

Docker Compose is a tool used to define and manage applications consisting of multiple containers. A single configuration file is used to specify the services, networks, and volumes required for the entire application stack. The lifecycle of complex applications can be managed with this tool, enabling all associated services to be started, stopped, and coordinated with a single command. \parencite{what-is-docker-compose}

\subsection{k6}
\label{sec:k6}

Grafana k6 \footnote{\href{https://grafana.com/docs/k6/latest/}{Grafana k6}} is an open-source performance testing tool designed to evaluate the reliability and performance of a system. It simulates various traffic patterns, such as constant load, sudden stress spikes, and long-term soak tests, to identify slow response times and system failures during development and continuous integration. Metrics are collected during execution and can be visualized through platforms like Grafana or exported to various data backends for detailed reporting. \parencite{k6-overview}

k6 allows tests to be written in JavaScript, making it accessible and easy to integrate into existing codebases. Every k6 test follows a common structure. The main component is a function that contains the core logic of the test. This function should be the default export of the JavaScript file. It is executed concurrently for each \acrlong{VU} (\acrshort{VU}), which act as independent execution threads to repeatedly apply the test logic. The tests can be enhanced using k6's lifecycle functions, such as a setup function, which is executed only once and may be utilized to insert seed data into the system. The test execution can be configured using an "options" object, where VUs, test duration and performance thresholds can be set. \parencite{k6-write-your-first-test}

\chapter{Implementation}

\section{Contract Test Implementation}
\label{sec:contract-test-implementation}

The \hyperref[sec:contract-tests]{contract tests} are implemented in a separate maven module called \texttt{test-{\allowbreak}suite}\footnote{\javaname{test-suite/src/test/java/karsch.lukas}}. The test classes use the \texttt{JUnit 5} testing framework and \texttt{REST Assured} to send and assert \gls{http} requests. A basic test might look like this:

\begin{lstlisting}
%%@DisplayName%%("GET /lectures should return 200 and include 2 dates")
%%@Test%%
void getLectureDetails_shouldReturn200_returnTwoDates() {
    // First, create seed data
    var lectureSeedData = createLectureSeedData();

    RestAssured.given()
            .when() 
            .get("/lectures/{lectureId}", lectureSeedData.lectureId())
            .then()
            .statusCode(200)
            .body("data.dates", hasSize(2));
}
\end{lstlisting}
\captionof{lstlisting}[Contract test example]{Contract test example; adapted from \javaname{test-suite/src/test/karsch.lukas.lectures.AbstractLecturesE2ETest}}
\label{lst:e2e-test}

All contract tests follow a consistent pattern as shown in \autoref{lst:e2e-test}. First, a test method is annotated with \javaname{@DisplayName} to provide a descriptive, human-readable name. The test method itself is precisely named after the behavior it asserts. In the example above, the test verifies that the response status code is \javaname{200} and that the response body contains a field called \javaname{dates} consisting of an array of size two.

Before making these assertions, each test creates "seed data". Seed data is prerequisite data that must exist on the system under test for the execution to be valid. For instance, a professor, a course, and a lecture must be created before the endpoint to \javaname{GET} that specific lecture can be tested. Tests that assert invariants, such as the business rule preventing lecture from having overlapping timeslots, typically set the system time via a Spring Boot Actuator endpoint first.

Once the prerequisites are met, the request is executed and assertions are made using \gls{restassured}. The \texttt{given()} block sets up the request requirements like headers, parameters, or body content; the \texttt{when()} block defines the action, such as the \gls{http} method (GET, POST) and the endpoint URL. Finally, the \texttt{then()} block is used to verify the response, allowing the developer to assert status codes and validate the data returned in the response body.

The test classes in \texttt{test-suite} are all \texttt{abstract}, meaning they can not be run directly. Instead, they are intended to be subclassed by the modules implementing the concrete applications (\texttt{impl-crud} \& \texttt{impl-es-cqrs}). The subclasses must implement a set of abstract methods which are implementation specific, for example a method to reset the database in between each test, a method to set the application's time and methods to create seed data for tests.

Necessary infrastructure is spun up by the subclasses using \glspl{testcontainer}. \glspl{testcontainer} is a way to declare infrastructure dependencies as code and is an open-source library available for many programming languages. \parencite{testcontainers-homepage}

\begin{lstlisting}
%%@TestConfiguration%%
public class PostgresTestcontainerConfiguration {
    %%@Bean%%
    %%@ServiceConnection%%
    %%@RestartScope%%
    PostgreSQLContainer<?> postgreSQLContainer() {
        return new PostgreSQLContainer<>(
                DockerImageName.parse("postgres:latest"));
    }
}
\end{lstlisting}
\captionof{lstlisting}[\keyw{PostgresTestcontainerConfiguration}]{\javaname{impl-crud/src/test/karsch.lukas.}\keyw{PostgresTestcontainerConfiguration}}
\label{lst:testcontainer-configuration}

\autoref{lst:testcontainer-configuration} starts a \hyperref[sec:postgresql]{PostgreSQL} container using the latest available image. \texttt{@ServiceConnection} makes sure the Spring application can connect to the container. This configuration can then be imported as shown in \autoref{lst:import-testcontainer}.

\begin{lstlisting}
%%@SpringBootTest%%
%%@Import%%(PostgresTestcontainerConfiguration.class)
public class CrudLecturesE2ETest extends AbstractLecturesE2ETest { }
\end{lstlisting}
\captionof{lstlisting}[\keyw{CrudLecturesE2ETest}]{\javaname{impl-crud/src/test/karsch.lukas.e2e.lectures.CrudLecturesE2ETest}}
\label{lst:import-testcontainer}

\section{CRUD implementation}

% TODO explain architecture / layout 

This section presents the relevant aspects of the CRUD implementation\footnote{\javaname{impl-crud/src/main/java/karsch.lukas}}, mainly focusing on relational modeling using \hyperref[sec:jpa]{\acrshort{jpa}} and the audit log implementation.

\subsection{Relational Modeling}

The CRUD implementation uses a \hyperref[sec:crud-architecture]{normalized database} in the Third Normal Form.

\begin{figure}[h]
    \includegraphics[width=\textwidth, inner]{../vault/Thesis/images/CRUD_ER_Diagram_3.png}
    \caption{Entity Relationship Diagram for the CRUD App}
    \label{fig:crud-er-diagram}
\end{figure}

Figure \ref{fig:crud-er-diagram} shows the Entity Relationship Diagram for the CRUD app. It includes nine entities and a value object for the app's relational database schema. Each box corresponds to an entity or value object, with the bold text being the name. Below the table's name, all attributes of the entity are listed with their type and name.

Arrows represent an association. The numbers at the end of the arrows convey the multiplicity. An arrow pointing in only one direction stands for a unidirectional association, while an arrow pointing in both directions conveys a bidirectional association. For example, an arrow pointing between entity \texttt{A} and entity \texttt{B} like so: \texttt{1} $\longleftrightarrow$ \texttt{0..1} shows that one \texttt{A} can be associated with any number of \texttt{B}'s, and a \texttt{B} is always associated with exactly one \texttt{A}. % TODO kinda unreadable 

Arrows with a filled diamond represent a composition. Compositions are used when an entity has a reference to a value object. This value object has no identity and is directly embedded into the entity. The only value object in figure \ref{fig:crud-er-diagram} is the \keyw{TimeSlotValueObject}.

In the app's ER diagram, the \keyw{LectureEntity} serves as core of the schema, having several key associations. The 0..* $\longrightarrow$ 1 association to \keyw{CourseEntity} shows that many lectures can be created from a course and a lecture is always associated with a course. The 0..* $\longrightarrow $ 1 association to \keyw{ProfessorEntity} shows that a professor can hold many lectures (or none), and that a lecture is always associated with a professor. From the lecture's side, these relationships are called "Many to One" relationships.

\keyw{LectureEntity} also has "One to Many" relationships to \keyw{LectureWaitlistEntryEntity}, \keyw{EnrollmentEntity} and \keyw{LectureAssessmentEntity}. \keyw{LectureWaitlistEntryEntity} is a table which stores students who are waitlisted for a lecture. It is effectively a join table (with one extra column to track when the student was waitlisted) and represents a Many to Many relationship between lectures and students. The same applies to \keyw{EnrollmentEntity} which is a table storing which students are enrolled to which lecture. \keyw{LectureAssessmentEntity} represents the fact that a lecture can have many assessments (which may be an exam, a paper or a project). Each assessment in turn has many \keyw{AssessmentGradeEntity}s associated with it. This table stores which student scored which grade on an assessment. % TODO fix spacing 

The \keyw{AuditLogEntry} is also visible on the ER diagram, however it has no relationships. This table and the entire audit log implementation will be laid out in the \hyperref[sec:audit-log-implementation]{following section}.

These entities are implemented using SpringBoot's \acrshort{jpa} integration. For example, an entity with a "One to Many" relationship can be implemented like this:

\begin{lstlisting}[caption={Simple JPA entity with a "One to Many" relationship},captionpos=b]
%%@Entity%% 
class LectureEntity {
    %%@Id%% 
    private UUID id; 

    %%@OneToMany(fetch=FetchType.LAZY)%%
    private List<EnrollmentEntity> enrollments; 
}
\end{lstlisting}

The \javaname{@Entity} annotation informs \acrshort{jpa} that the class should be mapped to a database table. If the schema generation feature is enabled, \acrshort{jpa} automatically creates a table structure that mirrors the class definition. In production environments where this feature is typically disabled, developers must provide SQL scripts to manually define the expected structure. This is commonly achieved either by including a basic initialization script or by utilizing dedicated database migration tools such as Flyway or Liquibase to manage versioned schema changes.

Each entity must include a field annotated with \javaname{@Id}, which serves as the unique primary key for the corresponding database record.

The \keyw{@OneToMany} annotation defines a relational link between two entities. While the collection is accessed in Java as a standard list via \keyw{lecture.getEnrollments()}, \acrshort{jpa} manages this behind the scenes using a foreign key relationship. The \texttt{fetch} parameter determines when this data is retrieved: \texttt{LAZY} loading defers the database query until the collection is explicitly accessed in the code, whereas \texttt{EAGER} loading fetches the related entities immediately alongside the parent object.

\subsection{Audit Log implementation}
\label{sec:audit-log-implementation}

There are several strategies to implement an audit log, each with its own trade-offs:

\begin{enumerate}
    \item \textbf{Manual Logging}: Developers explicitly call a logging service in every service method that modifies data. While simple, this can lead to code duplication and is prone to human error, such as developers forgetting to add a log statement. A code example might look like this:

          \begin{lstlisting}[caption={Code example for manual audit logging},captionpos=b]
public void updatePhoneNumber(User user, int newNumber) {
    logChange(Date.now(), user, user.getPhoneNumber(), newNumber, "UserRequestedNumberChange");
    user.setPhoneNumber(newNumber);
}

void logChange(
    Date date, User user, Object oldValue, Object newValue, String context
) {
    LogEntry logEntry = new LogEntry(date, user, oldValue, newValue);
    logRepository.persist(logEntry);
}
\end{lstlisting}
    \item \textbf{Database Triggers or Stored Procedures} can capture changes automatically and directly on the database. This guarantees that no change is missed, even if made outside the application. \textcite[515]{ingram_design_2009} mentions that database triggers run on a "per-record" basis, meaning the logic is run for each changed record individually. This may lead to degraded performance during batch operations, which is why stored procedures should be preferred over triggers for auditing concerns. It is also worth noting that this approach ties the auditing logic to a specific database, making it less portable.
    \item  \textbf{Hibernate Envers} is an auditing solution for JPA-based applications which automatically versions entities by using the concept of revisions. Envers creates an auditing table for each entity which stores historical data, whenever a transaction is committed. \parencite{hibernate_envers_nodate}
    \item \textbf{JPA Entity Listeners}: JPA's lifecycle events (\texttt{@PrePersist}, \texttt{@PreUpdate}, etc.) can be used to intercept changes. This approach is database-independent and keeps the logic within the Java application, allowing access to application internals like beans and Spring's security context. In full-grade applications built using Spring Security, the security context lets developers access the current user, making it possible to attach them to the new audit log entry. (needs reference) \label{item:jpa-entity-listener}
\end{enumerate}

\subsection{Chosen Implementation: JPA Entity Listener}

The \acrshort{crud} implementation of the application utilizes approach \ref{item:jpa-entity-listener}, JPA entity listeners, which offer a good balance between automation and flexibility. This approach ensures that every change to an entity is captured without polluting the service layer with logging calls, while still allowing the application to enrich the log with application-level context. The current user is automatically added to the log entry by the entity listener, while service methods can attach additional context to an entity before it is persisted, if desired.

\subsubsection*{Data Model}

The audit log is stored in a single database table, represented by the \keyw{AuditLogEntry} entity. This structure allows for easy querying of all system changes in chronological order. Table \ref{table:audit-log-entry} lists the fields contained by \keyw{AuditLogEntry}. This data model is adapted from \textcite{fowler_audit_2004}.

\begin{table}[htp!]
    \small
    \centering
    \begin{tabularx}{\linewidth}{lp{4cm}X}
        \toprule
        \textbf{Field} & \textbf{Type, Possible Values} & \textbf{Explanation}                                  \\ \midrule
        entityName     & String                         & Identifies the type of the changed object.            \\ \addlinespace
        entityId       & UUID                           & Unique identifier of the specific changed object.     \\ \addlinespace
        timestamp      & LocalDateTime                  & The date and time when the change occurred.           \\ \addlinespace
        operation      & CREATE, UPDATE, DELETE         & The type of action performed on the entity.           \\ \addlinespace
        modifiedBy     & String                         & The user or system process that initiated the change. \\ \addlinespace
        oldValueJson   & TEXT (JSON String)             & The serialized state of the entity before the change. \\ \addlinespace
        newValueJson   & TEXT (JSON String)             & The serialized state of the entity after the change.  \\ \addlinespace
        contextJson    & TEXT (JSON String)             & Captures additional business intent or metadata.      \\
    \end{tabularx}
    \caption{Audit Log Entry Structure}
    \label{table:audit-log-entry}
\end{table}

\subsubsection*{AuditableEntity and AuditEntityListener}

To enable auditing, entities extend an abstract base class, \texttt{AuditableEntity} \footnote{\javaname{impl-crud/src/main/java/karsch.lukas.audit.AuditableEntity}}. This class marks the entity with the \javaname{@EntityListeners(AuditEntityListener.class)} annotation and provides a transient field, \texttt{snapshotJson}, used to store the state of the entity when it is loaded from the database.

The core logic resides in \texttt{AuditEntityListener}\footnote{\javaname{impl-crud/src/main/java/karsch.lukas.audit.AuditEntityListener}}. It hooks into the JPA lifecycle:

\begin{itemize}
    \item \texttt{@PostLoad}: Immediately after an entity is fetched from the database, the listener serializes it to \acrshort{json} and stores the serialized \acrshort{json} string in the \texttt{snapshotJson} field. This serves as the "old value" for any subsequent updates.
    \item \texttt{@PrePersist}: Before a newly created entity is saved, a log entry is created with the operation \texttt{CREATE}. The \texttt{newValueJson} is the current state; \texttt{oldValueJson} is null.
    \item \texttt{@PreUpdate}: Before an existing entity is updated, the listener compares the current state with the \texttt{snapshotJson}. It creates an \texttt{UPDATE} entry using \texttt{snapshotJson} as the \texttt{oldValue} and the current state as the \texttt{newValue}.
    \item \texttt{@PreRemove}: Before deletion, a \texttt{DELETE} entry is created, preserving the last known state. \texttt{snapshotJson} is saved as the \texttt{oldValue}, \texttt{newValue} is \texttt{null}
\end{itemize}

\subsubsection{Serialization and Relationships}

A major challenge in serializing JPA entities to \acrshort{json} for an audit log is handling relationships. A naive \acrshort{json} serialization would follow every relationship of the affected entity (e.g., \texttt{Lecture} $\rightarrow$ \texttt{Course}), potentially serializing huge entity graphs or even causing a \texttt{StackOverflowError} due to cyclic references. One example for such a problematic reference would be the self-reference of \texttt{CourseEntity}, which each course having a "One to Many" relationship to prerequisite courses.

A custom Jackson module, \texttt{IdSerializationModule}\footnote{\javaname{impl-crud/src/main/java/karsch.lukas.audit.IdSerializationModule}}, was implemented to deal with the problem of relationship serialization. This module overrides the default serialization for related entities. Instead of serializing the full nested object, it only writes the related object's ID. This results in a flat, lightweight \acrshort{json} structure that is readable, keeps all references reconstructible and is safe to store. % TODO explain how this was implemented in detail 

\subsubsection{Capturing Business Context}

One downside of standard automated auditing is that it captures \emph{what} changed (e.g., lecture lifecycle changed from \texttt{IN\_PROGRESS} to \texttt{FINISHED}) but not necessarily \emph{why} (e.g., Lecture cancelled by professor due to illness). To make the CRUD application's audit log compliant to the requirement of traceability, it supports an \texttt{AuditContext}. Services can attach metadata to the current thread/transaction, which the \texttt{AuditEntityListener} retrieves and stores in the \texttt{contextJson} field.

\section{ES/CQRS implementation}
\label{sec:es-cqrs-implementation}

\subsection{Architecture Overview}
\label{sec:architecture-overview}

The architecture of the \texttt{impl-es-cqrs} application \footnote{\javaname{impl-es-cqrs/src/main/java/karsch.lukas}} differs from the traditional layered architecture seen in the \texttt{impl-crud} application. While the CRUD implementation also has some vertical slicing, the ES-CQRS implementation is much more explicit about it. The code is organized into "features", each representing a vertical slice of the application's functionality (e.g., \texttt{course}, \texttt{enrollment}, \texttt{lectures}). Each feature is self-contained and includes its own command handlers, event sourcing handlers, query handlers, and its own web controller, if needed.

A "feature slice" architecture is descriptive and able to communicate the features of a project at a glance. As clean architecture is not in the scope of this thesis, the separation into features with clear naming conventions for command and query components is sufficient, however introducing completely separate modules for the command and read sides would have increased the project structure's readability even more by clearly showing how command and read side have no access to each other. % TODO reference for "feature slicing" 

\subsection{The API Layer}
\label{sec:the-api-layer}

The \texttt{api} package in each feature slice is shared between web controllers, command side and read side, containing the public interface of the application. It defines the Commands, Events, and Queries that are dispatched and handled by the \javaname{impl-es-cqrs} application. Keeping the public API in a separate package ensures that the internal implementation details of the \javaname{impl-es-cqrs} application are not exposed to its clients.

\subsection{Command Side}
\label{sec:command-side}

The command side is responsible for handling state changes in the application. It is implemented using Axon's Aggregates, Command Handlers, and Sagas. This section goes in detail about the implementation aspects, using the courses feature as an example.

\subsubsection{Aggregates and Set-Based Validation}
\label{sec:aggregates-and-set-based-validation}

Aggregates are the core components of the command side. They represent a consistency boundary for state changes. In this implementation, an example of an aggregate is the \keyw{CourseAggregate} \footnote{\javaname{impl-es-cqrs/src/main/java/karsch.lukas.features.course.commands.CourseAggregate}}. It handles the \keyw{CreateCourseCommand}, validates it, and if successful, emits a \keyw{CourseCreatedEvent}.

A core aspect of the validation within the \keyw{CourseAggregate} is set-based validation. Before creating a course, the system must verify that all the specified prerequisite courses actually exist. This is handled by the \keyw{ICourseValidator}\footnote{\javaname{impl-es-cqrs/src/main/java/karsch.lukas.features.course.commands.ICourseValidator}}, which is injected into the aggregate's command handler. The validator uses an SQL lookup table, implemented using a \acrshort{jpa} entity and repository, that is maintained by the \keyw{CourseLookupProjector}. This ensures that the command is validated against a consistent view of the system's state. It is important to note that while projectors typically belong to the read side, "lookup projectors" in this application belong to the command side. They are independent to the read side and maintain their own view of the system. This view is not eventually consistent, but strongly consistent, making it suitable for command validation. A strongly consistent lookup table is achieved through the use of \hyperref[sec:axon-events]{subscribing event listeners}, which are executed immediately after an event has been applied.

\subsubsection{External Command Handlers}
\label{sec:impl-external-command-handlers}

Not all commands can be handled by a single aggregate. For instance, assigning a grade to a student for a specific lecture involves the \texttt{EnrollmentAggregate} and the \texttt{LectureAggregate}. In such cases, a dedicated command handler, \texttt{Enrollment{\allowbreak}Command{\allowbreak}Handler}, is used. This handler coordinates the interaction between the aggregates. It loads the \texttt{EnrollmentAggregate} from the event sourcing repository, validates the command (e.g., checking if the professor is allowed to assign a grade for the lecture), and then executes the command on the aggregate.

\subsubsection{Sagas for Process Management}
\label{sec:sagas-for-process-management}

Sagas are used to manage long-running business processes that span multiple aggregates. The \texttt{AwardCreditsSaga} is a prime example. It is initiated when an \keyw{EnrollmentCreatedEvent} occurs. The saga then waits for a \keyw{LectureLifecycleAdvancedEvent} with the status \texttt{FINISHED}. Once this event is received, the saga sends an \keyw{AwardCreditsCommand} to the \keyw{EnrollmentAggregate}. The saga ends when it receives a \keyw{CreditsAwardedEvent}. This ensures that credits are only awarded after a lecture is finished and all assessments have been graded. It is interesting to note that while the CRUD application calculates awarded credits based on the current state of a lecture, in the ES-CQRS implementation, the fact that credits are awarded after finishing a lecture is explicit. Even when changing the Saga later on, credits which have already been awarded will not be revoked, unless additional, explicit logic is implemented (e.g. by applying a \keyw{CreditsRevokedEvent}). % TODO keep elaborating on traceability here, OR move it to the end / Fazit.

\subsection{Read Side}
\label{sec:read-side}

The read side listens to events asynchronously and builds read models, called "projections", which are views of the system. A component that listens for events and maintains projections is called a "projector". Projections are designed to answer specific questions about the system: each projector saves exactly the necessary information. This is achieved by using denormalized data models, a contrast to typical CRUD systems that follow normalization rules.

When the system is queried, the queries are routed to the read side. The read side can efficiently fetch data from the projections, usually without \texttt{JOINs}. This makes reads fast. It is important to keep in mind that projections are built asynchronously, meaning they are eventually consistent and may not always reflect the latest changes applied by the command side.

In the context of the ES-CQRS implementation, a good example of a projector that stores denormalized data for efficient querying is the \keyw{LectureProjector}. It demonstrates the fact that each projector maintains its own view of the system. Projectors must not query the system using Axon's \keyw{QueryGateway} to get access to any data needed for the projection. One reason for that is the fact that when \emph{rebuilding} projections, a common use case in event sourcing, the projectors should be able to run in parallel. If projectors depend on each other, this can result in one projection attempting to query data from another projection that is not yet up to date. This is why the \keyw{LectureProjector} not only maintains a view of lectures, but also of courses, professors and students, which are then used when building the lecture's projection.

The projector also illustrates how the projection's database entities are designed: they are built in the same way as the DTO which is returned from the query handler. Arrays and associated objects are not stored via foreign keys but are instead serialized to \acrshort{json}. This allows the retrieval of all the necessary data to respond to a query with a simple \texttt{SELECT} statement. The same concepts apply to all other projectors in the ES-CQRS implementation.

\subsection{Synchronous Responses with Subscription Queries}
\label{sec:synchronous-responses-with-subscription-queries}

A common challenge in \acrshort{cqrs} and event-driven architectures is providing synchronous feedback to users. For example, when a student enrolls in a lecture, they expect an immediate response indicating whether they were successfully enrolled or placed on a waitlist. However, commands are usually handled asynchronously. In \acrshort{cqrs}, commands are also not intended to return data.

To solve this, the \keyw{LecturesController} uses Axon's subscription queries. When an enrollment request is received, it sends the \keyw{EnrollStudentCommand} and simultaneously opens a subscription query (\keyw{EnrollmentStatusQuery}). This query waits for an \keyw{EnrollmentStatusUpdate} event. The read-side projector responsible for processing enrollments publishes this update after processing the respective \keyw{StudentEnrolledEvent} or \keyw{StudentWaitlistedEvent}. The controller blocks for a short period, waiting for this update to be published, and then returns the result to the user. This approach makes the user interface synchronous, while not contradicting with the asynchronous nature of \acrshort{cqrs} systems, as the command handling process is unchanged. While this approach provides the desired synchronous user experience, it has the downside of coupling the client to the event processing flow. In a typical scenario, one might use WebSockets or other client-side notification mechanisms to inform the user about the result of their action. However, for the context of this thesis, where the focus is on the backend implementation and contract testing, this solution is a pragmatic compromise. % TODO improve this section

\subsection{Encapsulation and API Boundaries}
\label{sec:encapsulation-and-api-boundaries}

To enforce the separation of concerns and maintain a clean architecture, the internal components of the command and read sides are package-private. For example, the \keyw{CourseAggregate} and \keyw{CourseProjector} are not accessible from outside their respective feature packages. The public API of the application is exposed through the controllers, which only interact with the \keyw{CommandGateway} and \keyw{QueryGateway}. This ensures that all interactions with the system go through the proper channels and that internal implementations can be changed without affecting the clients.

\subsection{Tracing Request Flow}
\label{sec:tracing-request-flow}

This section illustrates the flow of commands and queries through the system. Axon's \keyw{CommandGateway} and \keyw{QueryGateway} are used in controllers to decouple them from the internals of the application. The gateways create location transparency: a controller does not need to know where its commands and queries are being routed to. % TODO reference 

\subsubsection{Command Request: CreateCourseCommand}
\label{sec:command-request-createcoursecommand}

Figure \ref{fig:es-cqrs-command-flow} illustrates the flow of a command through the system using the example of the \texttt{POST} \texttt{/courses} endpoint. Upon receiving a request, the controller constructs a \keyw{CreateCourseCommand} containing the request data and dispatches it through the \keyw{CommandGateway}. This gateway is responsible for routing the command to the appropriate destination, which in this case is the constructor of the \keyw{CourseAggregate}. This constructor is annotated with \keyw{@CommandHandler}. The command handler verifies that the command is allowed to be executed by performing validation logic. When creating courses, it has to be made sure that all prerequisite courses actually exist. This check is done using set-based validation. If the validation is successful, the aggregate triggers a state change by applying a \keyw{CourseCreatedEvent} via the \keyw{AggregateLifecycle.apply()} method. This action notifies the system of the change and persists the event by recording it in the event store.

After being applied, Axon routes the event to all subscribed handlers. The \keyw{CourseAggregate}'s \keyw{@EventSourcingHandler} is executed, changing the aggregate's internal state. What is worth noting here is that in the case of \keyw{CourseAggregate}, only the \texttt{id} of the course is set as other properties of the event, like name or description of the newly created course, are not relevant to the command side. Any read-side projectors with \keyw{@EventHandlers} for the \keyw{CourseCreatedEvent} are also executed after the event is applied.

\begin{figure}[H]
    \includegraphics[width=\textwidth, inner]{images/es-cqrs-command-flow.png}
    \caption{Sequence Diagram: Command Flow inside the ES-CQRS application}
    \label{fig:es-cqrs-command-flow}
\end{figure}

\subsubsection{Query Request: FindAllCoursesQuery}
\label{sec:query-request-findallcoursesquery}

\hyperref[fig:es-cqrs-query-flow]{Figure \ref*{fig:es-cqrs-query-flow}} illustrates the flow of a query through the application using the \texttt{GET} \texttt{/courses} request as an example. The request is received by \keyw{CoursesController}. It creates a \keyw{FindAllCoursesQuery} instance and sends it to Axon's \keyw{QueryGateway}, which routes the query to the appropriate \keyw{@QueryHandler} method responsible for \keyw{FindAllCoursesQuery}. The query handler method then accesses its JPA repository to get all courses, maps them to a list of \keyw{CourseDTOs} and returns this list. The \keyw{QueryGateway} hands this result over to the web controller which reads the data and sends it back to the client.

\begin{figure}[H]
    \includegraphics[width=\textwidth, inner]{images/es-cqrs-query-flow.png}
    \caption{Sequence Diagram: Query Flow inside the ES-CQRS application}
    \label{fig:es-cqrs-query-flow}
\end{figure}

\section{Infrastructure}
\label{sec:infrastructure}

The project's infrastructure is designed for consistency and reproducibility across development and testing environments. It is composed of a containerized environment for running the applications and their dependencies, an automated \gls{vm} provisioning setup for performance testing, as well as an integration testing strategy using Testcontainers, described in \autoref{sec:contract-test-implementation}.

\subsection{Containerized Services}
\label{sec:containerized-services}

The core of the infrastructure is defined in a \gls{docker} compose file at the root of the project, which orchestrates the deployment of the two primary applications and their external dependencies: a \hyperref[sec:postgresql]{PostgreSQL} database, used by both applications, and an \hyperref[sec:axon]{Axon Server} instance, used by the ES-CQRS application.

A \keyw{postgres:18-alpine} container provides the relational database used by both applications. The database schema, user, and credentials are configured through environment variables. A volume is used to persist data across container restarts.

An \keyw{axoniq/axonserver} container provides the necessary infrastructure for the Event Sourcing and CQRS implementation, handling event storage and message routing. It is configured to run in development mode.

The CRUD and ES-CQRS applications are containerized using \keyw{Dockerfile}s. Both use \keyw{amazoncorretto:25} as the base image, and the compiled Java application (\keyw{.jar} file) is copied into the container and executed.

Configuration details, such as database connection strings and server hostnames, are externalized from the \keyw{application.properties} files. They are injected into the application containers at runtime as environment variables via the \keyw{docker-compose.yml} file, allowing for flexible configuration without modifying the application code.

\subsection{Local Development and Integration Testing}
\label{sec:local-development-and-integration-testing}

For local development and integration testing, the project uses the \glspl{testcontainer} library. This approach allows developers to programmatically define and manage the lifecycle of throwaway Docker containers for dependencies like PostgreSQL and Axon Server directly from the test code. (TODO duplicate?)

By integrating with Spring Boot's \glspl{testcontainer} support, running the application or its tests automatically starts the required containers. This eliminates the need to manually install and manage these services on their local machines, ensuring a consistent and isolated testing environment. The configuration for this is found in the test resources, where a special JDBC URL prefix signals Spring Boot to manage the database container.

\subsection{VM Provisioning for Performance Testing}
\label{sec:vm-provisioning}

To ensure a stable and isolated environment for performance benchmarks, a dedicated VM setup is used. The process of creating and provisioning these \acrshortpl{vm} on a Proxmox host is fully automated.

A shell script, \keyw{create-vm.sh}, orchestrates the creation of a VM template from an Ubuntu 24.04 cloud image. Cloud images are pre-configured, lightweight variants of operating systems. This script works in conjunction with a \keyw{cloud-init.yml} configuration file that handles the provisioning of the VM upon its first boot.

During the provisioning process, a number of steps are executed. First, it is made sure that the system is up-to-date by installing any available software updates. Next, a `thesis` user is created for which the environment is configured. Afterward, the script installs all necessary software, including \gls{docker}, git, Conda, Python, k6, Maven, and Java 25. Once all necessary software is installed, the project's git repository is cloned and a Maven build is triggered. Finally, the \gls{docker} images are built. After these steps are completed, the provisioned \acrshort{vm} is ready to run the applications and load tests.

Instead of starting the \acrshort{vm} directly, the script shuts the \acrshort{vm} down and converts it into a Proxmox template, which can be re-created efficiently. This template is used to create the client and server \glspl{vm}.

\section{Load Tests}

This section describes the implementation of load tests.

\subsection{k6 Scripts}
\label{sec:k6-implementation}

The core of the load testing suite are the load-generating scripts developed using \hyperref[sec:k6]{k6}. \autoref{lst:create-course-k6-script} illustrates the implementation of a typical k6 script using the creation of courses with prerequisites as an example. \footnote{\javaname{performance-tests/k6/writes/create-course-prerequisites/create-course-prerequisites.js}}

After defining necessary imports, the test script extracts execution parameters from the \keyw{\_\_ENV} object which is injected by the k6 test runner. Most k6 scripts written for this project rely on \acrshort{rps}, representing the target iteration rate, and \texttt{TARGET\_HOST}, which is the URL the application under test is reachable at.

The value of \acrshort{rps} is used to define test options. Namely, a scenario, optional thresholds and the statistics to collect are defined. A test may have several scenarios, however in the k6 scripts used in this project, only one scenario per test is defined. Each scenario has a specific executor. In this case, the "ramping-arrival-rate" executor is used, as opposed to the "ramping-vus" executor. While the "ramping-vus" executor defines the number of virtual users interacting with the application (closed model), "ramping-arrival-rate" executors define the number of iterations per second (open model). This important distinction is described in more detail in \autoref{sec:load-test-theory}. Stages in a scenario define the "timeline" of \acrshort{rps}. In the given example, \acrshort{rps} are increased from 0 to the target \acrshort{rps} over a duration of 20 seconds. This \acrshort{rps} is then held for a duration of 80 seconds, before decreasing \acrshort{rps} back to 0 over a span of 20 seconds.

After defining test options, an optional setup function is implemented. It is executed once by k6, before running the load-generating "export default" function. In the setup function, seed data can be created. The given code example uses the setup function to create 10 prerequisite courses. Their IDs are returned from the setup function.

Data returned from the setup function can be passed to the "export default" function, which is the core of any load test. This is the function that is executed repeatedly to generate load. The implementation of this function in the given example is rather simple. One POST request is sent to the server. This request includes a payload which references a random number of prerequisite courses, as well as other required parameters for course creation.

\begin{lstlisting}[language=JavaScript]
// Imports omitted
const {TARGET_HOST, RPS} = __ENV;

export const options = {
    scenarios: {
        createCourses: {
            executor: "ramping-arrival-rate",
            timeUnit: "1s",
            preAllocatedVUs: RPS,
            stages: [
                {target: RPS, duration: "20s"},
                {target: RPS, duration: "80s"},
                {target: 0, duration: "20s"}
            ]
        }
    },
    thresholds: {
        'http_req_failed': ['rate<0.01'], // Error rate must be <1%
    },
    summaryTrendStats: ["med", "p(99)", "p(95)", "avg"],
};

export function setup() {
    const prerequisiteIds = createPrerequisites(10);
    return { prerequisiteIds };
}

export default function (data) {
    const {prerequisiteIds} = data; 

    const url = `${TARGET_HOST}/courses`;
    const prerequisiteCourseIds = selectRandomPrerequisiteIds();
    const payload = createPayload(prerequisiteCourseIds);
    const res = http.post(url, payload);
    checkResponseIs201(res);
}
\end{lstlisting}
\captionof{lstlisting}[k6 script, simplified code example]{Simplified code example of a k6 script to test course creation. Adapted from \javaname{performance-tests/k6/writes/create-course-prerequisites/create-course-prerequisites.js}}
\label{lst:create-course-k6-script}

\subsection{Load Test Lifecycle}

The k6 scripts alone are not enough to execute a large, repeated load test. While they can generate load on a running application and are capable of collecting client-side metrics, external lifecycle management is needed to control the infrastructure and ensure a clean environment in between each test run.

The lifecycle of repeated load tests is managed using python scripts. The core scripts are \keyw{perf\_runner.py} \footnote{\keyw{performance-tests/perf\_runner.py}} and \keyw{many\_runs.py} \footnote{\keyw{performance-tests/many\_runs.py}}. These scripts instrument the entire lifecycle of the application and k6 runs. They are responsible for starting the application using \gls{docker}, collecting server-side metrics using Prometheus and post-processing results.

The core logic within \keyw{perf\_runner.py} follows a defined flow for every single test run. It begins by determining the execution context. If a remote configuration is provided, it establishes a \gls{docker} Remote Context via \gls{ssh} to interact with the target \acrshort{vm}. It then deploys the application using \texttt{docker compose up}. Before directing any traffic towards the application, the Actuator's health endpoint is polled to ensure the application is running properly.

Once the application is healthy, the script sets up Prometheus for server-side monitoring. After dynamically generating a \javaname{prometheus.yml} configuration file, a Prometheus container is started, targeted to scrape the application under test. To ensure short-term spikes in latency or resource consumption can be captured, the configuration defines a polling interval of 2 seconds.

With the environment and monitoring active, the script invokes k6. Configuration parameters for the test run are expected to be defined in \javaname{metric.json}, which is a file placed alongside a test script. It includes metadata and parameters such as the number of \acrshortpl{VU} and the target host URL. These parameters are passed directly to the k6 engine via environment variables. Inside the k6 scripts, the VUs environment variable is used to define the arrival rate within the ramping-arrival-rate executor rather than a fixed number of concurrent users. Because k6 is configured to trigger a specific number of iterations per second, this parameter effectively acts as a control for Requests Per Second (RPS), ensuring the load remains consistent regardless of how long the individual HTTP calls take to complete. % TODO unverständlicher satz wahrscheinlich

After k6 completed its load generation, the script enters a data-extraction phase. It queries the Prometheus API to retrieve system-level metrics. Next, it parses the k6-summary.json file, which is a file generated by k6 that includes all metrics recorded during the run. The collected data is processed and merged into standardized CSV files (client\_metrics.csv and server\_metrics.csv).

Once all data is extracted, the system is ready for the next run. To prepare the environment, all containers need to be stopped first. That is done by running \javaname{docker compose down -v} inside the \gls{docker} remote context, with the \texttt{-v} argument explicitly removing all docker volumes. This makes sure PostgreSQL's and Axon Server's data stores are emptied out before the next test iteration.

While \javaname{perf\_runner.py} manages the lifecycle of a single test, \javaname{many\_runs.py} acts as a high-level orchestrator, designed to automate large-scale comparative benchmarks by executing multiple iterations across both implementations by running a single command. The script can be configured to run an arbitrary number of tests, which will be executed for both applications. The script accepts the metric configuration files and passes them on to \javaname{perf\_runner.py}.

\subsection{Post Processing Test Results}

After extracting data from the k6 output and Prometheus, it is consolidated into a unified CSV format. This is necessary because the two systems use differing naming conventions and units: while k6 might report the 95th percentile latency as $p(95)$ in milliseconds, Prometheus might expose it through a complex PromQL query resulting in a label like $latency_p95$, measured in seconds. Precisely, k6's $med$, $avg$ and percentile latency metrics are mapped to the Prometheus equivalent. Performing this normalization step immediately after the test run means the collected data can easily be compared and visualized later.

\subsection{Testing "Freshness": Time to Consistency}

To assess the eventual consistency of the ES-CQRS architecture, a specialized test for the \hyperref[slo-freshness]{Freshness \acrshort{slo}} was developed.\footnote{\keyw{performance-tests/k6/time-to-consistency/create-lecture/create-lecture.js}} Unlike standard performance scripts, which measure the speed of isolated requests, this script is specifically designed to measure the synchronization delay between the command and query sides of the application. This delay, called eventual consistency, occurs because the write-side (Command) and read-side (Query) are strictly separated in \acrshort{cqrs}.

The primary difference from a typical k6 test lies in the execution flow within the default function. Rather than executing a single \acrshort{http} call, this test executes two calls to the application. First, it performs a POST request to create a lecture and captures the resulting ID. After creating the lecture, the script performs sleeps for exactly 0.1 seconds, the threshold defined in the Freshness \acrshort{slo}. After this threshold, the application is expected to have synchronized the write- and read-side. Once the script wakes up from its sleep, it performs a GET request, attempting to fetch the newly created lecture.

To track the success rate of this request, the script introduces a custom Rate metric named $read\_visible\_rate$. By manually adding true or false to this metric based on whether the lecture was found, indicated by a response status of 200, the script generates a percentage of "fresh" requests inside the required threshold of 100ms. This provides a clear statistical view of how reliably the ES-CQRS system maintains its "fresh" data under varying levels of load.

\chapter{Results}

\input{tables/latency_get-lectures_500VU}

\chapter{Discussion}

\section{Analysis of results}

\section{Conclusion \& Further work}

Finally, I'm done!

\newpage
\printbibliography

\appendix

\chapter{Source Code}

The full source code for this thesis, including both apps, performance tests and markdown notes, is available at: \url{https://gitlab.mi.hdm-stuttgart.de/lk224/thesis}


\end{document} % This is the end of the document
